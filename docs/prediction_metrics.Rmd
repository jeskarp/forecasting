---
title: "Prediction Metrics"
author: "Janetta E. Skarp"
date: "13 April 2018"
output: pdf_document
fonsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r packages}
# Install the most up-to-date versions of each package
drat::addRepo("reconhub")
install.packages(c("outbreaks", "incidence", "epicontacts", "epitrix", "earlyR", "EpiEstim"))
```

# Real data

Assume that I have daily case data for an outbreak. The outbreak of choice is a measles outbreak in Hagelloch in 1861 (outbreaks). There is data for 87 days, but I will pretend that I have data only for various time periods. I assume that the data is complete and devoid of reporting delays. I am aware that this is an unrealistic expectation for real data but this assumption works for my example.

```{r data}
# Look at what the full dataset contains
library(outbreaks)
measles_hagelloch_1861

# Sort by date of prodrome
measles_hagelloch_sorted <- dplyr::arrange(measles_hagelloch_1861, date_of_prodrome)

# Make a new case ID so that each case has its own number according to date_of_prodrome
cbind(prodrome_case_ID = seq(1, nrow(measles_hagelloch_sorted), 1), measles_hagelloch_sorted)
```

Cori et al. (2013) suggest that calculating $R$ from fewer than 12 cases and for a time period that is less than 1 average serial interval can be misleading. According to the contact data of this measles dataset, the mean serial interval can be approximated to be 11 (10.89) days.

Manual splitting of the dataset according to Cori et al.'s (2013) limitations gives the following dates for this measles outbreak:

* 30/10/1861 to 17/11/1861 (19 days, 12 cases)
* 18/11/1861 to 28/11/1861 (11 days, 60 cases)
* 29/11/1861 to 09/12/1861 (11 days, 104 cases)
* 10/12/1861 to 24/01/1862 (46 days, 12 cases)

This split means that the first and last subsets of the dataset span for a long period of time to reach the minimum number of 12 cases. The middle subsets on the other hand span the minimum number of dates (the mean serial interval) and contain most of the cases. There is one case that is far later than the rest (January 24th 1862), outliers like that are better ignored. Due to previous time windows' forces of infection affecting the current time window, it is better to take cutoff dates rather than windows of data.

Therefore, the cutoff dates are:
* 17/11/1861
* 28/11/1861
* 09/12/1861
* 21/12/1861
* 02/01/1862

Let's have a look at the incidence curve for the outbreak.

```{r incidence_curve}
library(incidence)

# Plot the incidence for all the data we have
measles_hagelloch_incidence <- incidence(measles_hagelloch_1861$date_of_prodrome, interval = 1)
plot(measles_hagelloch_incidence)

# Split incidence data into the 4 subgroups
measles_hagelloch_incidence_1 <- measles_hagelloch_incidence[1:19, ]
measles_hagelloch_incidence_2 <- measles_hagelloch_incidence[1:30, ]
measles_hagelloch_incidence_3 <- measles_hagelloch_incidence[1:42, ]
measles_hagelloch_incidence_4 <- measles_hagelloch_incidence[1:65, ]

plot(measles_hagelloch_incidence_4)
```

\newpage
# Estimating R

Next, I'll generate a forecast so that I can assess its performance later. 

The datasets of the EpiEstim package provide an estimate of the serial interval Gamma distribution already so I don't need to calculate it in these cases. Just take the mean and standard deviation and run with it.

For some outbreaks, such as this measles outbreak, it is necessary to estimate the serial interval yourself. In order to do so, you need to find a dataset relevant to your disease that includes contact data and approximate the serial interval from there. One can also approximate a serial interval distribution given a mean and variance of a serial interval. In this case we have contact data so we will just use the data that comes with this dataset for illustrative purposes. 

```{r serial_interval_calculation}
library(earlyR)
library(epicontacts)
library(outbreaks)
library(epitrix)

# The dataset
measles_hagelloch_1861
# 1st column says the case ID
# 2nd column marks the infector of the case
# 3rd column marks the date of prodrome

# Make a directed list of contacts
contacts <- make_epicontacts(linelist = measles_hagelloch_1861, contacts = measles_hagelloch_1861, 
                             id = 1L, from = 2L, to = 1L, directed = TRUE)

# Make a serial interval distribution
directed_contacts <- get_pairwise(contacts, "date_of_prodrome")
hist(directed_contacts)

# Discretise the distribution
si_fit <- fit_disc_gamma(directed_contacts)
```

I can now estimate R:
```{r R_estimate}
library(earlyR)

# The R estimation - won't work because not much data at the end of the outbreak
R <- get_R(measles_hagelloch_incidence, si_mean = si_fit$mu, si_sd = si_fit$sd, 
           max_R = 10)
print(R)
# plot(R)

# R estimation for different groups
R1 <- get_R(measles_hagelloch_incidence_1, si_mean = si_fit$mu, si_sd = si_fit$sd, 
            max_R = 10)
R2 <- get_R(measles_hagelloch_incidence_2, si_mean = si_fit$mu, si_sd = si_fit$sd, 
            max_R = 10)
R3 <- get_R(measles_hagelloch_incidence_3, si_mean = si_fit$mu, si_sd = si_fit$sd, 
            max_R = 10)
R4 <- get_R(measles_hagelloch_incidence_4, si_mean = si_fit$mu, si_sd = si_fit$sd, 
            max_R = 10)
print(c(R1$R_ml, R2$R_ml, R3$R_ml, R4$R_ml))

# Visualising R
plot(R_test)
plot(R_test, "lambdas")
abline(v = as.Date(max(measles_hagelloch_incidence[1:25, ]$dates)), lty = 2)
```

The dotted lines in the global force of infection plot show the current date (i.e. last date observed). There is still a possibility of a new transmission event occurring. 

I can also estimate R with the EpiEstim package:
```{r epiestim_R_estimate}
library(EpiEstim)

# Specifying the input for the estimate
# mu <- 2.6 # serial interval mean
# sigma <- 1.5 # serial interval SD
i <- c(measles_hagelloch_incidence$counts) # make an incidence vector

# T.Start and T.End should be vectors of of time periods for which R should be estimated
# We care about the posterior coefficient of variation for R
epiestim_R <- EstimateR(i, T.Start = c(2, 9, 15, 22), T.End = c(8, 14, 21, 32), method = c("ParametricSI"), Mean.SI = si_fit$mu, Std.SI = si_fit$sd, plot = TRUE)
```

\newpage
# Incidence forecasting

Now that I know what's happening with the effective reproduction number, I can forecast into the future to predict what will happen next.

The *projections* package will not work unless you feed incidence to it as an incidence object (using the package *incidence*), so you'll need to re-make your daily incidence with the as.incidence function from the package if you haven't done so already. You can then continue on to forecasting.

```{r forecast}
#devtools::install_github("reconhub/projections")
library(projections)
library(ggplot2)
library(magrittr)
library(incidence)

# number of days that I hid data for
measles_hagelloch_total_incidence <- incidence(measles_hagelloch_1861$date_of_prodrome)
# measles_hagelloch_hidden <- subset(measles_hagelloch_1861, date_of_prodrome > cutoff_date)
# measles_hagelloch_hidden_incidence <- incidence(measles_hagelloch_hidden$date_of_prodrome, interval = 1)

# Number of trajectories for the projection
n_traj <- 10000

# Serial interval distribution already provided with the EpiEstim data
proj_1 <- project(measles_hagelloch_incidence_1, R = sample_R(R1, 1000), si = si_fit$distribution, 
                n_sim = n_traj, n_days = 20, R_fix_within = TRUE)
proj_2 <- project(measles_hagelloch_incidence_2, R = sample_R(R2, 1000), si = si_fit$distribution, 
                n_sim = n_traj, n_days = 20, R_fix_within = TRUE)
proj_3 <- project(measles_hagelloch_incidence_3, R = sample_R(R3, 1000), si = si_fit$distribution, 
                n_sim = n_traj, n_days = 20, R_fix_within = TRUE)
proj_4 <- project(measles_hagelloch_incidence_4, R = sample_R(R4, 1000), si = si_fit$distribution, 
                n_sim = n_traj, n_days = 20, R_fix_within = TRUE)

# Look at prediction quantiles
# apply(proj, 1, summary)

# Look at how the mean develops
# apply(proj, 1, function(x) mean(x > 0))

# Plot the projection
plot(measles_hagelloch_incidence) %>% add_projections(proj_1[1:10, ], boxplots = FALSE)
plot(measles_hagelloch_incidence) %>% add_projections(proj_2[1:10, ], boxplots = FALSE)
plot(measles_hagelloch_incidence) %>% add_projections(proj_3[1:20, ], boxplots = FALSE)
plot(measles_hagelloch_incidence) %>% add_projections(proj_4[1:20, ], boxplots = FALSE)
```

Below I test different cutoff points and lengths of projection and then use prediction metrics on these test projections.

```{r testing}
library(earlyR)
library(projections)
library(ggplot2)
library(magrittr)
library(incidence)

# Testing different cutoff points
cutoff_point <- 32
proj_days <- 7 # number of days projected into future

R_test <- get_R(measles_hagelloch_incidence[1:cutoff_point, ], si_mean = si_fit$mu, si_sd = si_fit$sd, 
            max_R = 10)

proj_test <- project(measles_hagelloch_incidence[1:cutoff_point, ], R = sample_R(R_test, 1000), si = si_fit$distribution, 
                n_sim = n_traj, n_days = proj_days, R_fix_within = TRUE)

plot(measles_hagelloch_incidence) %>% add_projections(proj_test, boxplots = FALSE)
```

According to the predictions, it looks like the outbreak incidence will be increasing over the next 18 days. The uncertainty around the prediction estimates increases as the forecast moves further into the future. Compared to the true incidence (black histogram), the forecast stays near the prediction for around 3 days, after which the prediction starts to grossly overestimate incidence.

\newpage
# Assessing prediction performance

## Calibration (Funk et al.)
Calibration relates to the model's ability to correctly identify prediction uncertainty. Calibration needs to be assessed before sharpness.

If the model is perfectly calibrated, each time point's data looks as if it comes from the predictive probability distribution (the distribution of possible unobserved values conditional on the observed values) for that time point:

$$ u_{t} = F_{t}(x_{t}) $$
Here $x_{t}$ is the datapoint for time $t$. $F_{t}$ is the cumulative probability distribution at time $t$. $u_{t}$ is the probability.

In my case the predictions should follow a Poisson-distribution with some lambda, $\lambda$. I then use my hidden data as the true data points, $x_{t}$.

The Anderson-Darling test (a statistical test for seeing if the data is drawn from a given probability distribution) of uniformity to $u_{t}$ is applied. The model is calibrated if p > 0.1.

```{r calibration}
# library("goftest")
library(incidence)

# The true datapoints: x_t
measles_hagelloch_hidden_incidence <- measles_hagelloch_incidence[(cutoff_point+1):(cutoff_point+proj_days), ]
x_t <- measles_hagelloch_incidence[(cutoff_point+1):(cutoff_point+proj_days), ]$counts

# Number of days 
hidden_days <- as.vector(measles_hagelloch_hidden_incidence$timespan)

# The cumulative probability distribution for time t: F_t
cumulative_poisson <- function(data, pred){
  ppois(data, mean(pred))
}

# Calculating calibration for each prediction day
calibration <- function(data, pred){
  calibration <- array(NA, dim = c(hidden_days))
  for (i in 1:hidden_days){
    calibration[i] <- cumulative_poisson(data[i], pred[i, ])
  }
  return(calibration)
}

# anderson-darling <- function(data, pred){
#   anderson-darling <- array(NA, dim = c(hidden_days))
#   for (i in 1:hidden_days){
#   ad.test()
#   }
#   return(anderson-darling)
# }

model_calibration <- calibration(x_t, proj_test)
```

```{r plot_calibration, echo = FALSE}
calibration_days <- seq(1, hidden_days, 1)
calibration_data <- data.frame(model_calibration)
calibration_data$days <- calibration_days

plot_calibration <- ggplot(data = calibration_data, aes(x = days, 
                                                        y = model_calibration)) +
                           geom_point(size = 2, shape = 16) 
plot_calibration
```

The model is kind of calibrated for the first few days (also intuitively seen in the previous plot), but after that calibration drops.

\newpage
## Sharpness (Funk et al.)
Model's ability to make predictions within a narrow range of possible outcomes. 

Sharpness is calculated as follows:

$$ S_{t}(F_{t}) = 1 - \frac{MADM(y)}{m(y)} $$
where y is the prediction variable. MADM stands for the normalised median absolute deviation around the median (m(y)) of y:

$$ MADM(y) = m(|y - m(y)|) $$
The closer to 1 S is, the sharper the forecast.

```{r sharpness, cache = TRUE}
# Function for calculating forecasted incidence median
forecast_median <- function(x){
  return(median(x))
}

# Function for calculating the standard deviations
# Want it to return MADM(y)
MADM <- function(x){
  traj_diff <- array(NA, dim = c(n_traj)) # array for storing SDs
  # For each trajectory for this day, calculate the SD
  for (i in 1:n_traj){
    # save the absolute difference for each trajectory
    traj_diff[i] <-  abs(x[i] - forecast_median(x))
  }
  MADM <- median(as.vector(traj_diff))
  return(MADM)
}

# Function for calculating the normalised median absolute deviation
# Want it to return St(Ft)
sharpness <- function(x){
  # array for storing daily sharpness
  sharpness <- array(NA, dim = c(hidden_days))
  for (i in 1:hidden_days){
    sharpness[i] <- 1 - (MADM(as.vector(x[i, ])) / forecast_median(as.vector(x[i, ]))) 
  }
  return(sharpness)
}

model_sharpness <- sharpness(proj_test)
```

```{r plot_sharpness, echo = FALSE}
sharpness_days <- seq(1, as.vector(hidden_days), 1)
sharpness_data <- data.frame(model_sharpness)
sharpness_data$days <- sharpness_days

plot_sharpness <- ggplot(data = sharpness_data, aes(x = days, 
                                                    y = model_sharpness)) +
                           geom_point(size = 2, shape = 16) 
plot_sharpness
```

Model sharpness decreases as you forecast further. Remember, S = 1 is perfect sharpness.

\newpage
## Bias (Funk et al.)
Does model systematically over- or underpredict data?

Bias can be calculated as:

$$ B_{t}(F_{t}, x_{t}) = 2(E_{F_{t}}[H(X - x_{t})] - 0.5) $$

Where $E_{F_{t}}$ refers to the expectation with respect to the predictive cumulative probability distribution $F_{t}$, and $X$ are independent realisations of a variable with distribution $F_{t}$

If unbiased, half of forecasts would be above estimate and half would be below the estimate, thus $B_{t}$ would be 0. A completely biased model would either be completely above the prediction line ($B_{t} = 1$) or completely under the prediction line ($B_{t} = -1$).

The Heaviside function takes a value of 0 for a negative number and 1 for a positive number. A Heaviside function with the half-maximum convention is 0.5 for H(0)

```{r bias}
# Calculate the difference between datapoints
# Feed in the true datapoint and predictions for a given time t
difference <- function(data, pred){
    # Calculate difference between data point and sample
  difference <- array(NA, dim = c(n_traj))
  for (i in 1:n_traj){
    difference[i] <- pred[i] - data
  }
  return(difference)
}

# Do the Heaviside function with half-maximum convention
# Feed in the differences between the predictions and true values for a given time t
heaviside <- function(difference){
  heaviside <- array(NA, dim = c(n_traj))
  for (i in 1:n_traj){
    if (difference[i] > 0){
      heaviside[i] <- 1
    } else if (difference[i] < 0){
      heaviside[i] <- 0
    } else{
      heaviside[i] <- 0.5
    }
  }
  return(heaviside)
}

# Calculate the expected prediction value for a given time t
expected <- function(h_values, pred){
  mean_heaviside <- mean(h_values)
  return(mean_heaviside)
}

# Calculate bias
bias <- function(data, pred){
  bias <- array(NA, dim = c(hidden_days))
  for (i in 1:hidden_days){
  bias[i] <- 2 * (expected(heaviside(difference(data[i], as.vector(pred[i, ]))), as.vector(pred)) - 0.5)
  }
  return(bias)
}

# Get single bias score
bias_score <- function(data, pred){
  daily_bias <- bias(data, pred)
  bias_score <- sum(daily_bias) / hidden_days
  return(bias_score)
}

model_bias <- bias(x_t, proj_test)

print(paste("The bias score is", bias_score(x_t, proj_test)))
```

```{r plot_bias, echo = FALSE}
bias_days <- seq(1, hidden_days, 1)
bias_data <- data.frame(model_bias)
bias_data$days <- bias_days

plot_bias <- ggplot(data = bias_data, aes(x = days, 
                                          y = model_bias)) +
                           geom_point(size = 2, shape = 16) 
plot_bias
```

The model tends to over-predict the number of cases (positive bias?).

\newpage
## Root mean square error (Viboud et al. 2017)
The sample standard deviation of the differences between predicted and observed values.

```{r RMSE}
# Calculate squared difference between true data and prediction for a given timepoint
squared_diff <- function(data, pred){
  squared_diff <- array(NA, dim = c(length(pred)))
  for (i in 1:length(pred)){
  squared_diff[i] <- (data - pred[i])^2
  }
  return(squared_diff)
}

# Take the mean of the squared differences
mean_squared_diff <- function(data, pred){
  mean_squared_diff <- sum(squared_diff(data, pred)) / length(pred)
  return(mean_squared_diff)
}

# Calculates RMSE for each forecasted timepoint
rmse <- function(data, pred){
  rmse <- array(NA, dim = c(length(data)))
  for (i in 1:length(data)){
    rmse[i] <- sqrt(mean_squared_diff(data[i], as.vector(pred[i, ])))
  }
  return(rmse)
}

model_rmse <- rmse(x_t, proj_test)
```

```{r plot_rmse, echo = FALSE}
rmse_days <- seq(1, hidden_days, 1)
rmse_data <- data.frame(model_rmse)
rmse_data$days <- rmse_days

plot_rmse <- ggplot(data = rmse_data, aes(x = days, 
                                          y = model_rmse)) +
                           geom_point(size = 2, shape = 16) 
plot_rmse
```

\newpage
## Continuous ranked probability score (Funk et al.)
An evaluation method that combines calibration and sharpness. Assesses whether the predictive distribution and the data-generating distribution are the same. In a perfect scenario, CRPS is 0.

Not sure if this equation for calculating CRPS only applies to deterministic forecasts:

$$ CRPS(F_{t}, x_{t}) = E_{F_{t}} | X - x_{t} | - \frac{1}{2} E_{F_{t}} | X - X' | $$

where $X$ is a sample of the predicted number of cases for a given timepoint from predictive distribution $F_{t}$, $x_{t}$ is the observed number of cases for that timepoint, and $X'$ is also a sample of the predicted number of cases, but independent of $X$. 

There's a package called *scoringRules* for evaluating forecasts?

```{r CRPS, cache = TRUE}
absolute_diff <- function(single_x, vector_x){
  absolute_diff <- array(NA, dim = c(length(vector_x)))
  for (i in 1:length(vector_x)){
  absolute_diff[i] <- abs(single_x - vector_x[i])
  }
  return(absolute_diff)
}

expected_diff <- function(single_x, vector_x){
  difference <- absolute_diff(single_x, vector_x)
  expectation <- mean(difference)
  return(expectation)
}

expected_diff_pred <- function(vector_x){
  # absolute_diff <- abs(apply(combn(vector_x, 2), 2, diff)) # much slower
  absolute_diff <- as.numeric(dist(vector_x))
  expectation <- mean(absolute_diff)
  return(expectation)
}

crps <- function(data, pred){
  crps <- array(NA, dim = c(length(data)))
  for (i in 1:length(data)){
  crps[i] <- expected_diff(data[i], pred[i, ]) - 0.5 * expected_diff_pred(pred[i, ])
  }
  return(crps)
}

model_crps <- crps(x_t, proj)
```

```{r plot_crps, echo = FALSE}
crps_days <- seq(1, hidden_days, 1)
crps_data <- data.frame(model_crps)
crps_data$days <- crps_days

plot_crps <- ggplot(data = crps_data, aes(x = days, 
                                          y = model_crps)) +
                           geom_point(size = 2, shape = 16) 
plot_crps
```

CRPS increases as forecasts are further away from the current date. There's a clear jump after the third forecast day, this coincides with the true data not being within the scope of the confidence intervals of the prediction seen in the prediction plot.

\newpage
## Mean absolute error (Viboud et al. 2017)
```{r MAE}
# Calculate the absolute difference
absolute_diff <- function(data, pred){
  absolute_diff <- array(NA, dim = c(length(pred)))
  for (i in 1:length(pred)){
  absolute_diff[i] <- abs(data - pred[i])
  }
  return(absolute_diff)
}

# Take the mean of the absolute differences
mean_absolute_diff <- function(data, pred){
  mean_absolute_diff <- sum(absolute_diff(data, pred)) / length(pred)
  return(mean_absolute_diff)
}

# Calculate MAE for each forecasted timepoint
mae <- function(data, pred){
  mae <- array(NA, dim = c(length(data)))
  for (i in 1:length(data)){
    mae[i] <- mean_absolute_diff(data[i], pred[i, ])
  }
  return(mae)
}

model_mae <- mae(x_t, proj)
```

```{r plot_mae, echo = FALSE}
mae_days <- seq(1, hidden_days, 1)
mae_data <- data.frame(model_mae)
mae_data$days <- mae_days

plot_mae <- ggplot(data = mae_data, aes(x = days, 
                                        y = model_mae)) +
                           geom_point(size = 2, shape = 16) 
plot_mae
```

\newpage
## Relative mean square error (Viboud et al. 2017)
Not quite sure what this is and how it differs from the other errors

## R squared (Viboud et al. 2017)

* Based on y = ax
* Plot predicted vs. observed and see how much variation is explained by linear model.
* Wouldn't I need just one estimate for the prediction here?
* But should I use an adjusted R squared instead of just the normal one?

## Pearson's correlation between predicted and observed incidence (Viboud et al. 2017)

* Another measure of linear correlation 
* Total linear correlation is indicated by +1 (positive) and -1 (negative).

## Bias (Viboud et al. 2017)

* Fitting a linear regression to predicted and observed incidence, y = ax + b
* Then again, I already have a method for measuring bias. 