%%%%%%%%%%%%%%%%%%%%%
%% Document set-up %%
%%%%%%%%%%%%%%%%%%%%%

% Requirements:
  % Double-spaced
  % minimum 11pt font
  % Arial or Verdana font
  % 2 cm margins

\documentclass[a4paper, 12pt]{article} % sets document shape and font size

\usepackage[margin=2.0cm]{geometry} % set margins to 2cm
% \usepackage[document]{ragged2e} % make text left-aligned

\usepackage{setspace, caption}
\captionsetup{font=doublespacing} %double-spaced float captions
\doublespacing %double-spaced document
\setlength{\parindent}{2em} % 5 space indent

% change font to Arial
\renewcommand{\rmdefault}{phv} % Arial
\renewcommand{\sfdefault}{phv} % Arial

\renewcommand*\contentsname{} % removes Table of Contents' title

\usepackage{amsmath} % Needed for maths equations
\usepackage{graphicx}
\graphicspath{ {/home/evelina/Development/forecasting/figs/} } % Where the images will be found 

\usepackage[numbers]{natbib}

\usepackage{multirow} % for combining rows in tables

\usepackage{float} % for forcing figure placement

\usepackage{fontspec}

\usepackage[section]{placeins}

\usepackage{tabularx} % make table page-wide

\usepackage{etoc} % to make Supplementary Info its own ToC

%%%%%%%%%%%%%%%%%%%%%%%
%% Start of document %%
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \SweaveOpts{concordance=TRUE}
\setmainfont[Ligatures=TeX]{Verdana}

%%%%%%%%%%%%%%%%%%%%%%%%
%% Functions and data %%
%%%%%%%%%%%%%%%%%%%%%%%%

<<functions_data, echo = FALSE, results = "hide", message = FALSE>>=
# Function for making multi-panel plots
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

library(data.table)
library(dplyr)
# Combo tables for the diseases
# Ebola
# list all files in directory named full_proj_metrics.csv
output_files <- list.files("/home/evelina/Development/forecasting/simulations/ebola_8si_norm/", pattern = "full_proj_metrics.csv", 
                           full.names = TRUE, recursive = TRUE)
# read and row bind all data sets
ebola_table <- rbindlist(lapply(output_files, fread))
ebola_table$cali_proj_ratio <- round(ebola_table$cali_window_size / ebola_table$proj_window_no, 1)
ebola_table$pred_type[ebola_table$pred_type == 1] <- 0
ebola_table$pred_type[ebola_table$pred_type == 3] <- 1
# SARS
# list all files in directory named full_proj_metrics.csv
output_files <- list.files("/home/evelina/Development/forecasting/simulations/sars_8si_norm/", pattern = "full_proj_metrics.csv", 
                           full.names = TRUE, recursive = TRUE)
# read and row bind all data sets
sars_table <- rbindlist(lapply(output_files, fread))
sars_table$cali_proj_ratio <- round(sars_table$cali_window_size / sars_table$proj_window_no, 1)
sars_table$pred_type[sars_table$pred_type == 1] <- 0
sars_table$pred_type[sars_table$pred_type == 3] <- 1
# Influenza
# list all files in directory named full_proj_metrics.csv
output_files <- list.files("/home/evelina/Development/forecasting/simulations/influenza_8si_norm/", pattern = "full_proj_metrics.csv", 
                           full.names = TRUE, recursive = TRUE)
# read and row bind all data sets
influenza_table <- rbindlist(lapply(output_files, fread))
influenza_table$cali_proj_ratio <- round(influenza_table$cali_window_size / influenza_table$proj_window_no, 1)
influenza_table$pred_type[influenza_table$pred_type == 1] <- 0
influenza_table$pred_type[influenza_table$pred_type == 3] <- 1

# Combine all diseases into one table
total_table <- bind_rows(ebola_table, sars_table, influenza_table)
@

<<real_outbreaks, echo = FALSE, results = "hide", message = FALSE, warning = FALSE>>=
real_ebola <- read.csv("/home/evelina/Development/forecasting/simulations/real_outbreaks/ebola/full_proj_metrics.csv")
# real_ebola$cali_window_size <- real_ebola$cali_window_size / real_ebola$proj_window_size
real_ebola$cali_proj_ratio <- round(real_ebola$cali_window_size / real_ebola$proj_window_no, 1)
real_ebola$pred_type[real_ebola$pred_type == 1] <- 0
real_ebola$pred_type[real_ebola$pred_type == 3] <- 1
real_sars <- read.csv("/home/evelina/Development/forecasting/simulations/real_outbreaks/sars/full_proj_metrics.csv") 
# real_sars$cali_window_size <- real_sars$cali_window_size / real_sars$proj_window_size
real_sars$cali_proj_ratio <- round(real_sars$cali_window_size / real_sars$proj_window_no, 1)
real_sars$pred_type[real_sars$pred_type == 1] <- 0
real_sars$pred_type[real_sars$pred_type == 3] <- 1
real_influenza <- read.csv("/home/evelina/Development/forecasting/simulations/real_outbreaks/influenza/full_proj_metrics.csv")
# real_influenza$cali_window_size <- real_influenza$cali_window_size / real_influenza$proj_window_size
real_influenza$cali_proj_ratio <- round(real_influenza$cali_window_size / real_influenza$proj_window_no, 1)
real_influenza$pred_type[real_influenza$pred_type == 1] <- 0
real_influenza$pred_type[real_influenza$pred_type == 3] <- 1
  
real_total_table <- bind_rows(real_ebola, real_sars, real_influenza)
@

<<global_options, echo = FALSE>>=
knitr::opts_chunk$set(fig.pos = 'H')
@

<<points_for_tibo, echo = FALSE, results = "hide", message = FALSE, warning = FALSE>>=
# Average number of cases in each calibration window by disease
library(dplyr)
case_table <- total_table %>% group_by(disease, cali_window_size) %>% summarise(mean_cases = mean(no_cali_cases), std_cases = sd(no_cali_cases))
@

%%%%%%%%%%%
%% Title %%
%%%%%%%%%%%

\begin{titlepage}
    \begin{center}
        \vspace*{2.5cm}
        
        \textbf{Evaluating incidence forecasting for informing outbreak response}
        
        \vspace{0.5cm}
        Project 2
        
        MRes Biomedical Research 
        
        Epidemiology, Evolution, and Control of Infectious Diseases Stream 
        
        \vspace{0.5cm}
        
        \textbf{Janetta E. Skarp}
        
        \vspace{2.5cm}
        
        \includegraphics[width=0.4\textwidth]{ICL_crest}
        % \includegraphics{ICL_crest.png}
        
        \vspace{2.5cm}
        
        Supervisors: Thibaut Jombart, Anne Cori\\ 
        Submitted: August 2018\\
        Department of Surgery and Cancer, Imperial College London
        
    \end{center}
    
\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Statement of Originality %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Statement of Originality}

I certify that this thesis, and the research to which it refers, are the product of my own work, conducted during the current year of the MRes in Biomedical Research at Imperial College London. Any ideas or quotations from the work of other people, published or otherwise, or from my own previous work are fully acknowledged in accordance with the standard referencing practices of the discipline.

The methods used for forecasting disease incidence were developed by members of the R Epidemics Consortium. The empirical outbreak linelists were openly accessible from the Sierra Leone government (Ebola), and the \textit{EpiEstim} (SARS) and \textit{outbreaks} (influenza) R packages.

Additionally, I would like to thank Thibaut Jombart and Anne Cori for providing me with advice throughout the course of my project and giving me feedback on my thesis. 

\textcolor{red}{The second paragraph needs to essentially state what I did not do myself but rather built up on: "The statement should clearly and specifically acknowledge any work of other researchers which you have used, and clearly state which parts of the research were performed by you."}


\textcolor{red}{
Things I need to take into account:
\begin{itemize}
  \item I used packages developed by other people to make my projections
  \item The empirical outbreak data came from openly accessible sources
  \item I did the metrics
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%
%% Table of contents %%
%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{Table of Contents}
\textcolor{red}{We don't have to have a Table of Contents. This will be deleted when done to save pages}
% \addcontentsline{toc}{section}{Table of Contents}
\vspace{-4em}
% \tableofcontents
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}
\tableofcontents

%%%%%%%%%%%%%%
%% Abstract %%
%%%%%%%%%%%%%%
\newpage
\section*{Abstract}

Branching process models can be used for forecasting disease incidence in real-time to aid outbreak response. The performance of branching process models for early outbreak analysis purposes has not yet been evaluated. Three empirical outbreaks, Ebola, influenza, and SARS, and 90 simulations based on the $R_{0}$ and serial interval distribution of the real outbreaks were created. A branching process model was used to predict disease incidence. The performance of the projected daily incidence was explored through four metrics: the average residual, mean-square error, sharpness, and bias. The branching process model tends to overestimate the forecasted daily incidence. Predictive branching process model works best on SARS. Average residual and MSE get better with increasing proportion of calibration window size to projection window number. Sharpness and bias vary by prediction type. It is not a good idea to project too far from the calibration window especially if you have a short calibration window.

%%%%%%%%%%%%%%%%%%
%% Introduction %%
%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\renewcommand{\thesection}{\arabic{section}}
\newpage
\section{Introduction}

%%%%%%%%%%

% \textcolor{red}{Thibaut's suggested structure:
% 1) transmissibility at the core of epi-modelling 2) used for informing response 3) models 4) limited evaluation of performances 5) aim of my project.}
% \textcolor{red}{Subsections will be removed once done.}

% \subsection{Transmissibility at the core of epi-modelling}
Infectious diseases have been estimated to be the cause of approximately XX deaths in 2016 \textcolor{red}{CITE}. While the top of the list is dominated by diseases that are in constant transmission such as malaria and HIV/AIDS, outbreaks of other diseases such as haemorrhagic fevers can be devastating if they spread. Often interventions can be implemented to control outbreak spread, but knowing how the outbreak is developing may be useful when deciding what intervention methods to use and figuring out if the outbreak will worsen. Real-time modelling can be used to make an educated guess as to whether the outbreak is dying out or if one may expect it to grow further.

Global burden of disease study.

When modelling infectious disease outbreaks, the disease's effective reproduction number, often denoted as $R$, is of particular interest. $R$ refers to the number of susceptible individuals that an infectious individual infects on average \citep{Vynnycky2010}. In other words, it is a measure of the disease's transmissibility. Due to this, $R$ can be used as an indicator for whether the disease's transmissibility is increasing or decreasing at a given timepoint \citep{Vynnycky2010}. 

Disease incidence is impacted by $R$, defined as the number of new cases for a given time period. If $R$ is above 1, each infected individual infects on average more than one susceptible individual and thus the number of incident cases is increasing \citep{Vynnycky2010}. If $R$ is below 1, each infected individual infects fewer than one person on average, implying that the number of incident cases is on the decline \citep{Vynnycky2010}.

% \subsection{Epi-modelling informing response}

It is important to have a grasp of the current $R$ and numbers of future incidence in order to inform outbreak response and scaling. Numerous past outbreaks have utilised forecasting to answer questions pertaining to the future of the outbreak \citep{Baguelin2010, Lessler2014, Kraemer2017, Barry2018}. 

For instance, during the 2009 pandemic H1N1 influenza in England, the peak of the epidemic as well as the cost-effectiveness of various vaccination strategies for the autumn of 2009 were predicted as the pandemic progressed in real-time \citep{Baguelin2010}.

The Middle-Eastern Respiratory Syndrome (MERS) outbreak in the Kingdom of Saudi Arabia in March to June 2014 raised concerns regarding the possible spread of infection during the Hajj that October, prompting a request for predicting disease incidence. Disease incidences from different pre-October time windows were used to forecast incidence in October under pessimistic and optimistic scenarios \citep{Lessler2014}. This modelling study found that an outbreak amongst Hajj pilgrims was not likely but recommended that some preparations for an outbreak should be made \citep{Lessler2014}. 
\textcolor{red}{This one feels a bit less relevant than the rest, can remove if better that way.}

Another example is the 2015-2016 yellow fever outbreak in Angola and the Democratic Republic of Congo, during which modellers were attempting to predict where to yellow fever would spread next \citep{Kraemer2017}. This was done by utilising data sets on consisting of the information such as what areas the mosquito vectors were viable in and human mobility in addition to basic information on daily incidence \citep{Kraemer2017}.  

More recently, incidence forecasting was used during the Ebola outbreak of 2018 in the Democratic Republic of Congo, where data from April 30th to May 24th was available \citep{Barry2018}. Here it was determined that even in the worst case scenario, the affected areas would have been capable of isolating all required individuals as long as the outbreak did not expand \citep{Barry2018}. 

One of the earliest cases of incidence forecasting was applied by William Farr on the smallpox epidemic of 1837 to 1839. He forecasted the reduction in smallpox cases with surprising accuracy by assuming that the ratio between his estimates of quarterly reproduction numbers (calculated as the ratio of cases in a given quarter to those in the preceding quarter) remained constant \citep{Neuberger2013, Santillana2018}. Since Farr's initial attempts, many methods have been developed for inferring $R$ from incidence and estimating the number of future disease cases.
% This led to the birth of Farr's law stating that an epidemic's shape can be approximated by a normal curve, which has since faced some criticism regarding the conditions under which it can be applied \citep{Neuberger2013, Artzrouni1990}.

% \subsection{Models}

% \subsubsection{Other models}
% \textcolor{red}{Do not go into detail on how the other models work (provide references instead?). Talk about usage instead?}

For instance, $R$ can be estimated and used for forecasting incidence using a exponential growth rate model where a growth rate $r$ is fit to the cumulative incidence, which can then be used to estimate $R$ \citep{Chowell2007, Wallinga2007}. This relies on the assumption that the early growth of the epidemic is exponential \citep{Chowell2007}. A compartmental model, characterised by it having compartments for every disease category of interest and rates describing the movement in and out of the compartments, is another method that can be used to forecast daily incidence \citep{Anderson1991, Funk2018}. This is achieved by fitting the model to existing incidence data through estimating ranges of values that the model parameters can take by and then using the fitted model to obtain daily incidence forecasts \citep{Funk2018}.

% Compartmental models have compartments for every disease category of interest, in addition to rates describing the movement in and out of the compartments \citep{Anderson1991}. A common example of this type of model is the SIR model, where there are compartments for susceptible (S), infected (I), and recovered (R) individuals, and rates of infection and recovery linking S to I and I to R respectively \citep{Anderson1991}. A compartmental model can be fit to existing incidence data through estimating ranges of values that the model parameters can take by for example using particle Markov Chain Monte Carlo \citep{Funk2018}. This fitted model can then be used to obtain daily incidence forecasts \citep{Funk2018}.

% For such a model, $R$ can be estimated from the growth rate $r$ if the generation interval's duration is assumed to be constant \citep{Wallinga2007}.

%  ($\beta$ and $\gamma$ respectively) ($S$):$R = \frac{\beta \times S}{\gamma}$

% \subsubsection{Branching process model}
% \textcolor{red}{Explain what a branching process model is and maybe give examples of applications}

Branching process models are another method for estimating transmissibility and forecasting incidence in real-time during infectious disease outbreaks. A branching process can be used to model the number offspring produced by a given generation of individuals in a stochastic manner. The simplest example of a branching process is one where every individual in a given generation has a random number of offspring, which could result in a tree such as the one shown in Figure \ref{branching_process_ex}.

\begin{figure}[h]
\centering
\includegraphics[scale = 0.25]{branching_process_example}
\caption{A visualisation of what a branching process where each individual in a generation has a random number of offspring could look like.}
\label{branching_process_ex}
\end{figure}

This theory can be applied to inferring $R$ and forecasting disease incidence. Given an observation window containing the daily incidence of symptom onset for a number of days and a serial interval distribution, a branching process model can be used to infer $R$ by estimating the case-specific $R$ for each observed case. This is achieved through considering the relative likelihood of a given case infecting any other case in the observation window i.e. how likely is it that another case was the offspring that given case \citep{Wallinga2004, Cori2013}. These case-specific $R$s can then be used to obtain a $R$ likelihood distribution for the whole observation window \citep{Cori2013}. This in turn is used to forecast the incidence in the next timestep, as $R$ states how many susceptibles a case infects on average \citep{Nouvellet2017}. These forecasted cases are incorporated into the calculation of a new $R$ using the same method for the following timestep that is forecasted. 

Compared to the previously discussed modelling methods, branching process models have a few distinctive features. These features include their ability to consider the full serial interval distribution when forecasting incidence \citep{Wallinga2004, Cori2013}. Additionally, unlike compartmental models, they assume an infinite pool of susceptibles \citep{Cori2013}. 

% \textcolor{red}{I don't need to find a paper that right out states this, it's enough to see this from the lack of a group in the modelling process?}.

% \textcolor{red}{Thibaut suggested talking about how $R$ can be estimated from the growth rate somewhere in this section - essentially Wallinga \& Lipsitch 2007 - but I felt like if I would put that bit somewhere, it would be in the compartmental model paragraph and that could get confusing.}

% \subsection{Limited evaluation of performance}

In addition to the models discussed here, other incidence forecasting methods exist. These methods do not perform equally well when forecasting incidence. In a forecasting challenge organised by the RAPIDD (Research and Policy for Infectious Disease Dynamics) program, different modelling groups were issued with the task of forecasting the incidence of simulated Ebola epidemics and nine different models were compared for the occasion \citep{Viboud2017}. A group utilising the branching process modelling method for its forecasts showed the most promising results, outperforming groups using methods such as compartmental models, logistic growth regression, and other agent-based models \citep{Viboud2017, Nouvellet2017}.

During the Ebola epidemic of 2013-2016, a lack of a ready-to-use tool for which there was a widespread consensus on how it should be used for forecasting epidemics was noted \citep{Cori2017}. 
%\textcolor{red}{I know this from talking to people who were involved with analysing the linelists etc., I tried to find a fitting paper to back this statement up and the closest thing I found was your paper from 2017. It doesn't fully make this point but rather says that there were disagreements on how forecasts should be used. I can change or delete this sentence if misleading.} 
This led to the development of new tools and accompanying suggestions  for data analysis protocols by groups such as the R Epidemics Consortium (RECON) \citep{RECON2018}. RECON's R package \textit{projections}, for instance, is a forecasting tool that uses a branching process model to predict the number of cases during each of the forecasted days \citep{Jombart2018}. However, branching process model forecasting tools' ability to accurately predict the course of an epidemic in real time has not been evaluated.

% \subsection{Aim}
In this study, I aim to decipher the conditions under which a branching process model can or cannot be reliably used to forecast daily incidence. To accomplish this, I have chosen metrics for evaluating the performance of the forecasting branching process model provided by RECON with a particular focus being placed on the methods' performance during the early stages of outbreaks. I have then used the branching process model to forecast daily incidence on Ebola-like, influenza-like, and severe acute respiratory syndrome(SARS)-like simulated outbreaks. This was followed by an assessment of the performance of the model with the performance metrics. I also illustrate the application of this analysis on three empirical outbreaks. 

% Takeouts from the Introduction

% Parametric methods include approaches such as branching process and compartmental models, while the usage of epidemic trees represents a non-parametric approach \citep{Wallinga2004, Ferrari2005, Haydon2003}. These inferential methods would be unlikely to result in exactly identical estimated values of $R$ when given the same data. One study compared the $R$ estimates of multiple different models, an exponential growth rate model, two types of compartmental SEIR model, and a stochastic compartmental SIR model on the same Spanish flu outbreak data and found that while there were differences in the $R$ estimates, all of them fell within an acceptable range \citep{Chowell2007}. Altering one's assumptions while keeping the model otherwise unchanged can also result in differing estimates. For example, assumptions regarding contact patterns between individuals affected the R estimate for a model of a H1N1 influenza outbreak \citep{Ajelli2014}.

% One study compared the $R$ estimates of multiple different models, an exponential growth rate model, two types of compartmental SEIR model, and a stochastic compartmental SIR model on the same Spanish flu outbreak data and found that while there were differences in the $R$ estimates, all of them fell within an acceptable range \citep{Chowell2007}. Altering one's assumptions while keeping the model otherwise unchanged can also result in differing estimates. For example, assumptions regarding contact patterns between individuals affected the R estimate for a model of a H1N1 influenza outbreak, though all estimates were found to be within an acceptable range \citep{Ajelli2014}.

% \begin{align*}
% R = \frac{\beta \times S}{\gamma}
% \end{align*}

% (from project description). 
% It is possible, for instance, that they perform better on specific types of outbreak, such as outbreaks with an exponential growth phase, or with larger outbreaks.

% (The impact of factors such as under-reporting, reporting delays, or super-spreading may also be considered.)

% Transmissibility is estimated by calculating the likelihood of each susceptible individual being infected by a given infected individual, normalising for the likelihood that that susceptible was infected by another infected individual \citep{Wallinga2004}. From this estimate of $R$, incidence can be forecasted by using a branching process model again \citep{Nouvellet2017}.

% The branching process modelling approach could be used to model outbreaks in real-time. Here $R_{t}$, the effective reproduction number for a time window, can be used as the subject of estimation \citep{Wallinga2004, Cori2013}. Knowledge of the $R_{t}$ of the most recent timepoint can be used for forecasting incidence in the following time period \citep{Nouvellet2017}. Disease incidence forecasts can be used to aid decision-making in outbreak situations. Forecasts suggesting a major increase in the number of cases in the following weeks could for example highlight a need for additional hospital staff in the near future. 


%%%%%%%%%%%%%
%% Methods %%
%%%%%%%%%%%%%

\newpage
\section{Methods}
% 5-10 pages

All data simulation and analyses presented in this report was conducted on R, a statistical computing language \citep{RCoreTeam2018}.

%%%%%%%%%%

\subsection{Outbreaks and predictive branching process model}

Empirical daily incidence data for three diseases, Ebola, influenza, and SARS, were chosen to assess the performance of the branching process model \citep{Team2014, Cori2018, Campbell2018}. Table \ref{outbreak_table} shows the published $R_{0}$, and the mean and standard deviation (in days) of the serial interval for these empirical outbreaks. 

For each given disease, 80 simulations were run based on the published $R_{0}$, and mean and standard deviation (in days) of the serial interval for each outbreak (Table \ref{outbreak_table}). The simulations were conducted using the \textit{simOutbreak} function from the package \textit{outbreaker}, and each simulation was run for eight times the mean serial interval for a population size of 1 million \citep{Jombart2014}. The length of the simulation was therefore 96 days of Ebola, 24 days for influenza, and 72 days for SARS. For projection purposes, a section of this data would later be used to calibrate the predictive branching process model while other sections would be hidden to test the performance of said model.
\textcolor{red}{REMOVE OUTBREAK DATA SOURCE}
\textcolor{white}{\citep{Team2014, Leone2014, Cori2018, Fraser2011, Campbell2018, Jombart2018a}}
% Source for ebola data: http://opendatasl.gov.sl/dataset/ebola-virus-data

<<outbreak_table, echo = FALSE, results = "asis", message = FALSE>>=
library(xtable)

outbreak_table <- array(NA, dim =c(3, 5))

# Disease names
outbreak_table[1, 1] <- "Ebola"
outbreak_table[2, 1] <- "Influenza"
outbreak_table[3, 1] <- "SARS"

# Disease R0
outbreak_table[1, 2] <- 2.02
outbreak_table[2, 2] <- 2.7
outbreak_table[3, 2] <- 1.77

# Disease SI mean
outbreak_table[1, 3] <- 11.6
outbreak_table[2, 3] <- 8.7
outbreak_table[3, 3] <- 2.6

# Disease SI SD
outbreak_table[1, 4] <- 5.6
outbreak_table[2, 4] <- 3.6
outbreak_table[3, 4] <- 1.5

# Source
outbreak_table[1, 5] <- "19, 20" # "WHO 2014, SL data"
outbreak_table[2, 5] <- "21, 22" # "EpiEstim, Fraser (2011)"
outbreak_table[3, 5] <- "23, 24" # "Campbell et al. (2018)"

colnames(outbreak_table) <- c("Disease", "$R_{0}$", "SI mean (days)", "SI SD (days)", "Source")
rownames(outbreak_table) <- c("Ebola", "Influenza", "SARS")

tab <- xtable(outbreak_table, digits = 2, caption = "$R_{0}$, and serial interval (SI) mean and standard deviation (SD) in days used for simulating outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza.", label = "outbreak_table")
align(tab) <- "lXXXXX"
print(tab, hline.after=c(-1, 0, 3), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity,  sanitize.colnames.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@

To predict incidence for the upcoming days, a branching process forecasting method based on Cori et al.'s method for estimating the reproduction number was used \citep{Cori2013, Jombart2018}. The full details on how the branching process model is used for forecasting can be found in Supplementary Information section 1.1. The information required for forecasting are data on observed daily incidence and a Gamma-distributed serial interval for the disease in question.

The future incidence for each real and simulated outbreak was forecasted for a given number of time windows by first estimating the reproduction number ($R$) for the outbreak so far using the \textit{get\_R} function in the \textit{earlyR}-package (Fig. \ref{projection_ex})\citep{Jombart2017}. Future dates' incidence could then be projected based on the likelihood distribution of this $R$ using the \textit{project} function of the \textit{projections}-package \citep{Jombart2018}.

\textcolor{red}{more in-depth explanation} Forecasts for the daily incidence of each empirical and simulated outbreak were obtained as follows. First a distribution of likely values of $R$ were obtained based on the observed incidence in the calibration window and a discretised serial interval \textcolor{red}{get\_R}. This distribution is sampled from in the projection process to obtain likely values of $R$ for the first forecast day \textcolor{red}{sample\_R}. For the following forecast days, the projected daily incidence of the previous forecast days is taken into account when obtaining a forecast. $R$ is resampled for each day. Each projection is composed of 10,000 individual projection trajectories, from which the projection median and projection intervals are calculated. The full details on how the branching process model is used for forecasting can be found in the Supplementary Information section 1.1.

A single time window was defined as one mean serial interval, in days. A single projection window was the size of this time window, while calibration windows were multiples of the time window, with all calibration windows starting from day 0. The differing calibration window lengths are due to the oubreak's force of infection and thus $R$ for a given time window being affected by the cases from previous time windows. The incidence data was hidden for dates that were not within the calibration window.

The calibration windows and projection windows were combined so that the outbreak was either projected for maximum four time windows, or so that the sum of time windows (calibration or projection) was eight time windows. This would mean for instance that a calibration window the size of one time window for an outbreak would be used to predict four time windows ahead, while a calibration window consisting of seven time windows would only be used to predict one time window ahead. This limitation in the total number of time windows used was done in an attempt to keep the focus on the performance of the branching process model during early outbreak analysis and avoid the simulated outbreaks reaching a point where the outbreak has burned through the susceptibles.  

\begin{figure}[h]
<<branching_process_ex, echo = FALSE, warning = FALSE, fig.width = 6, fig.height = 4, fig.align = "center">>=
library(epitrix)
library(distcrete)
library(incidence)
library(earlyR)
library(projections)
library(EpiEstim)
library(outbreaks)
library(ggplot2)
library(grid)
library(gridBase)
library(magrittr)

# Influenza
data("Flu1918")
flu_1918 <- Flu1918
flu_i <- as.incidence(flu_1918$incidence, interval = 1)

delta <- 3

set.seed(1)

# Get serial interval and R calculation, do projection
# cv = sigma / mean 
flu_sim_si <- gamma_mucv2shapescale(2.6, (1.5/2.6))
flu_si <- distcrete("gamma", shape = flu_sim_si$shape, scale = flu_sim_si$shape, w = 0, interval = 1)
flu_R3 <- get_R(flu_i[1:(delta * 2), ], si = flu_si, max_R = 10)
flu_proj3 <- project(flu_i[1:(delta * 2), ], R = sample_R(flu_R3, 1000), si = flu_si, 
                     n_sim = 10000, n_days = (delta * 4 + 1), R_fix_within = TRUE)

# Plot R distribution
R_dataframe <- data.frame(grid = flu_R3$R_grid,
                          ml = flu_R3$R_ml,
                          ll = flu_R3$R_like)

R_plot <- ggplot(R_dataframe, aes(x = grid, y = ll)) +
                 geom_line(color = "dodgerblue") +
                 geom_area(fill = "dodgerblue", alpha = 0.5) +
                 geom_vline(xintercept = R_dataframe$ml, linetype = "dashed") +
                 labs(x = "Reproduction number", y = "Likelihood", size = 5) +
                 annotate("text", label = paste("R =", round(R_dataframe$ml, 2), sep = " "), 
                          x = R_dataframe$ml + 1, y = max(R_dataframe$ll), size = 4) +
                 coord_cartesian(xlim = c(0.0, 10.0)) +
                 theme(axis.text.y = element_blank(),
                       axis.ticks.y = element_blank())


# Plot influenza incidence
influenza_incidence <- plot(flu_i[1:19, ]) %>% add_projections(flu_proj3, quantiles = c(0.05, 0.5)) # the base projection plot
projection_plot <- influenza_incidence + 
                   geom_vline(xintercept = 7, linetype = "dashed") +
                   labs(x = "Day") +
                   coord_cartesian(xlim = c(1, 18)) +
                   annotation_custom(ggplotGrob(R_plot), xmin = 0.2, xmax = 6.9, ymin = 100, ymax = 200) +
                   geom_segment(aes(x = 1, xend = 6.9, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 6.9, xend = 1, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 7.1, xend = 18.8, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 18.8, xend = 7.1, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 7.1, xend = 9.9, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 9.9, xend = 7.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 10.1, xend = 12.9, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 12.9, xend = 10.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 13.1, xend = 15.9, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 15.9, xend = 13.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 16.1, xend = 18.8, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 18.8, xend = 16.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   annotate("text", label = "Calibration window", x = 4, y = 77) +
                   annotate("text", label = "Full projection", x = 13, y = 77) +
                   annotate("text", label = "1", x = 8.6, y = 55) +
                   annotate("text", label = "2", x = 11.6, y = 55) +
                   annotate("text", label = "3", x = 14.6, y = 55) +
                   annotate("text", label = "4", x = 17.6, y = 55) +
                   theme(legend.position = "none")
# Print plot
projection_plot
@
% \centerline{\includegraphics[width=0.8\textwidth]{projection_example}}
\caption{A visualisation of a calibration window, utilised for estimating the reproduction number ($R$), and full projection for daily incidence forecasts along with its split to individual projection windows later analysed by performance metrics (labelled 1-4). As the projection progresses, the forecasted daily incidence of previous projections is taken into account when estimating the projected $R$.}
\label{projection_ex}
\end{figure}

\FloatBarrier
\subsection{Performance metrics and their analysis}

The performance of different calibration windows with varying projection windows was quantified by observing the average residual, mean-square error, sharpness, and bias of the projections for each projection window of each simulated or real outbreak.

The average residual, measuring whether the model is over- or under-predicting values while taking into account the magnitude of the difference, was calculated for a given projection day such that the mean of differences between the true incidence $x$ and projected incidence $p$ for projection trajectory $i$ for all $I$ trajectories:
\begin{equation}
\epsilon = \frac{\sum_{i}^{I}{(x - p_{i})}}{I}
\end{equation}
Here a negative residual implies that the model is overpredicting the daily incidence, and a positive residual implies that the model is underpredicting the daily incidence. A perfect prediction would thus have a residual of 0.

The MSE, the sample standard deviation of the differences between predicted and observed values, was calculated. Much like the mean residual, this measures whether the model is over- or under-predicting values, but it is more successful at emphasising outliers as the errors are squared. 

For a given prediction day the MSE is calculated as:
\begin{equation}
MSE = \frac{\sum_{i}^{I}{(x - p_{i})^{2}}}{I \times (x + 1)}
\end{equation}
where the mean of the squared difference between the true incidence $x$ and projected incidence $p$ for trajectory $i$ is taken for all trajectories $I$. This is then divided by $x + 1$ to make the measure scale-independent in order to make the metric comparable between diseases and to avoid division by zero.  

Sharpness ($S$), a measure of how narrow the range of predictions provided by the model are, was also calculated following Funk et al.'s approach \citep{Funk2018}. Here the median absolute difference around the median for the collection of projections ($p$) for a given timepoint $t$ is calculated as:
\begin{equation}
S_{t}(F_{t}) = 1 - \frac{median(|p - median(p)|)}{median(p)}
\end{equation}
Sharpness ranges from 0 to 1, where 1 is perfect sharpness.

Bias, showing systematic over- or under-prediction of daily incidence for a prediction window, was also calculated by following Funk et al.'s approach where bias $B$ for a particular projection for a given projection day $t$ is:
\begin{equation}
B_{t}(F_{t}, x_{t}) = 2(E_{F_{t}}[H(X - x_{t})] - 0.5)
\end{equation}
where $E_{F_{t}}$ is the expectation with respect to the predictive cumulative probability distribution $F_{t}$, and $X$ are independent realisations of a variable with distribution $F_{t}$ \citep{Funk2018}. An unbiased model would have a $B$ of 0, while a constantly overestimating and underestimating models would have a $B$ of 1 and -1, respectively.

To provide a single score for each projection window a , the mean of the daily values for sharpness, bias, mean residual, and RMSE were calculated. The performance of the predictions was analysed in the same manner for both the simulated and empirical outbreaks. 

% Reliability, the predictive model's ability to assess uncertainty, was assessed by identifying how likely it was that the true incidence for a given day would have come from the distribution of predictions for that given timepoint. This was calculated by using Funk et al.'s approach where the uniformity of predictions' cumulative distribution functions is tested with an Anderson-Darling test, with a modification allowing for the discrete Poisson distribution to be perceived as continuous. EXPLAIN HOW I DID THIS.
%The RMSE and bias for the branching process model's prediction windows was compared against a null model where the predicted incidence for the projection windows was the mean of the incidence of the calibration window.while for reliability, the p-value for the Anderson-Darling test on the data for the given prediction window was calculated.

In order to determine which aspects of the outbreaks affect the accuracy of the branching process model's predictions as measured by the metrics, a linear regression analysis was undertaken for each metric of the simulated outbreaks. The explanatory variables taken into consideration were disease ($d$), the how many serial intervals the calibration window was composed of ($c$), the number of cases observed within said calibration window ($n$), the distance in serial intervals between the end of the calibration window and the end of the projection window ($p$), and whether the projection window contained projections that predicted only zero incidence, some zero incidence, or no zero incidence ($z$). Thus, the equation for the linear regression consisting of all these explanatory variables for a metric $y$ would be:
\begin{equation}
  y = b_{0}d + b_{1}c + b_{2}n + b_{3}p + b_{4}z + \epsilon
\end{equation}
where $b$ represents the regression slope for each variable and $\epsilon$ is the intercept. 

The most parsimonius model with the fewest parameters was selected stepwise out of a starting model consisting of all the above explanatory variables by using Akaike Information Criterion (AIC) using the \textit{stepAIC} function in the R package \textit{MASS} \citep{Venables2002}. This model was then treated as the "basic" model. After this, the additional benefit to the quality of the model gained by adding either an additive interaction between calibration window size and distance of projection window, the ratio of calibration window size to prediction window number, or both interaction and ratio to the basic model was assessed by comparing the AICs of each model. \textcolor{red}{Here Thibaut had left a note in my thesis - "These are other response variables". I find this a bit cryptic as we didn't discuss this in our meeting and I only saw this later}.

\textcolor{red}{Let me know if you think that I should have just put everything (interactions and all) into the "basic" model and then just found the most parsimonius model using stepwise AIC and not have bothered with comparing the "basic" model with additions.}

The correlation between the performance metrics was also explored through observing the correlation coefficients for every combination of metric pairs.

%%%%%%%%%%%%%
%% Results %%
%%%%%%%%%%%%%

\FloatBarrier
\newpage
\section{Results}
\textcolor{red}{Thibaut's comments regarding text were mostly focused on Introduction and Methods. For the Results he mostly focused on what figures I should but I have essentially re-written everything since the previous draft.}

\subsection{Simulated outbreaks}

% The real and simulated outbreaks were fitted with multiple calibration windows and projection windows, as exemplified with the real outbreak datasets in Fig. \ref{projection_plot}. As is seen in the figure, some time windows are projected for more than once, as often multiple projection windows at different distances from the calibration window are predicted based on a single calibration window. The predicted daily incidence for the real Ebola and influenza outbreaks is overestimated especially when forecasts are based on early outbreak disease incidence data and incidence is forecasted for many projection windows
% \subsubsection{Prediction metrics}

The simulated outbreaks varied in total outbreak size both within and between diseases. The length of simulated outbreak also varied between diseases as each simulation was run for eight times the mean serial interval of a given disease. Consquently, the Ebola-like simulations experienced the largest outbreaks of up to 600 cases, whilst the influenza outbreaks barely reached 50 cases (SI Fig. \ref{sim_dist}). This wide variety in outbreak size resulted in forecasts of irregular quality. Figure \ref{good_bad_plot} exemplifies this with three representative forecasts for simulated Ebola-like outbreaks. While some forecasts were near the true incidence and with a relatively narrow range of projections, others had either a wide range of projections, a range that did more often not include the true daily incidence, or both.  

\begin{figure}[h]
<<good_bad_plot, cache = TRUE, echo = FALSE, fig.width = 8, fig.height = 3, fig.align = "center", message = FALSE, warning = FALSE>>=
library(incidence)
library(EpiEstim)
library(outbreaks)
library(ggplot2)
library(epitrix)
library(distcrete)
library(outbreaker)

# Simulation
sim_outbreak <- function() {
  mu <- 11.6
  cv <- 5.6 / 11.6
  R0 <- 2.02
  obs_time <- round(mu) * 8
  
  sim_si <- gamma_mucv2shapescale(mu, cv)
  si <- distcrete("gamma", shape = sim_si$shape, scale = sim_si$shape, w = 0, interval = 1)
  
  sim_test <- simOutbreak(R0 = R0, infec.curve = si$d(0:30), n.hosts = 1000000, duration = obs_time, seq.length = 10, stop.once.cleared = FALSE)
  
  sim_outbreak <- data.frame(id = sim_test$id,
                             inf_id = sim_test$ances,
                             onset = sim_test$onset)
  return(sim_outbreak)  
}

# Bad projection - set.seed = 2, 24?, 25?, 53?
# Other two projections - set.seed = 4, 12, 48
# 4, 5, 6, 9, 12, 38, 41, 48, 72, 77, 82, 92, 96
# for (i in 1:100) {
# set.seed(i)
# sim_linelist <- sim_outbreak()
# sim_i <- incidence(sim_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
# 
# if (sim_i$n > 300) {
#   print(i)
# }
# }
# plot(sim_i)

# Serial interval for projection
mu <- 11.6
cv <- 5.6 / 11.6
R0 <- 2.02
delta <- round(mu)
  
sim_si <- gamma_mucv2shapescale(mu, cv)
si <- distcrete("gamma", shape = sim_si$shape, scale = sim_si$shape, w = 0, interval = 1)

# Precise and accurate projection
set.seed(72)
sim_linelist <- sim_outbreak() 
sim_i <- incidence(sim_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
good_R <- get_R(sim_i[1:(delta * 6), ], si = si, max_R = 10)
good_proj <- project(sim_i[1:(delta * 6), ], R = sample_R(good_R, 1000), si = si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)
good_plot <- plot(sim_i[1:(8*12), ]) %>% add_projections(good_proj, quantiles = c(0.05, 0.5))
good_plot <- good_plot + theme(legend.position = "none") +
                         coord_cartesian(ylim = c(0, 65)) +
                         xlab("Time (days)") +
                         annotate("text", label = "A", x = 5, y = 65)

# Accurate but imprecise projection
set.seed(48)
wide_linelist <- sim_outbreak() 
wide_i <- incidence(wide_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
wide_R <- get_R(wide_i[1:(delta * 2), ], si = si, max_R = 10)
wide_proj <- project(wide_i[1:(delta * 2), ], R = sample_R(wide_R, 1000), si = si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
wide_plot <- plot(wide_i[1:(8*12), ]) %>% add_projections(wide_proj[(2*12):(3*12), ], quantiles = c(0.05, 0.5))# (wide_proj[(4 * 12):((5 * 12) - 1), ])
wide_plot <- wide_plot + theme(legend.position = "none") +
                         coord_cartesian(ylim = c(0, 65)) +
                         xlab("Time (days)") +
                         annotate("text", label = "B", x = 5, y = 65)

# An overall bad projection
set.seed(33)
bad_linelist <- sim_outbreak() 
bad_i <- incidence(bad_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
bad_R <- get_R(sim_i[1:(delta * 2), ], si = si, max_R = 10)
bad_proj <- project(bad_i[1:(delta * 2), ], R = sample_R(bad_R, 1000), si = si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
bad_plot <- plot(bad_i[1:(8*12), ]) %>% add_projections(bad_proj[(3*12):(4*12), ], quantiles = c(0.05, 0.5))
bad_plot <- bad_plot + theme(legend.position = "none") +
                       coord_cartesian(ylim = c(0, 65)) +
                       xlab("Time (days)") +
                       annotate("text", label = "C", x = 5, y = 65)

# Combine plots into one plot
multiplot(good_plot, wide_plot, bad_plot, cols = 3)
@
\caption{Examples of daily incidence forecasts for simulations of Ebola-like outbreaks. Daily incidence curves (black histogram) are shown with the forecasted daily incidence (purple line: median, 95\% prediction interval: pink line and shaded areas). Panel A shows a well-performing projection where the forecasted daily incidence along with its 95\% interval is near the true observed incidence. Panel B shows a projection where the median forecasted incidence is near the true observed values, but the spread of projections is wide. Panel C shows a projection where the median forecasted simulation is not near the true observed incidence and the spread of forecasts is wide.}
\label{good_bad_plot}
\end{figure}

\textcolor{red}{Should I put Fig. \ref{good_bad_plot} into the methods?}

\begin{figure}[h]
<<all_metric_plot, echo = FALSE, fig.width = 8, fig.height = 10, warning = FALSE>>=
# Only want calibration windows 1, 2, and 4
sub_table <- filter(total_table, cali_window_size == 1 | cali_window_size == 2 | cali_window_size == 4) 

# Make a new column so that I get all the combinations into one graph
sub_table$cali_proj <- paste(sub_table$cali_window_size, ":", sub_table$proj_window_no, sep = "")

# Residual
residual_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(residual), std = sd(residual), q1 = quantile(residual, probs = 0.025), q2 = quantile(residual, probs = 0.25), q3 = quantile(residual, probs = 0.75), q4 = quantile(residual, probs = 0.975))

residual_sub <- ggplot(residual_iqr, aes(cali_proj, y = med, ymin = q2, ymax = q3, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

residual_sub2 <- ggplot(residual_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    # coord_cartesian(ylim = quantile(sub_table$residual, c(0, 0.97))) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# MSE
mse_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(mse), std = sd(mse), q1 = quantile(mse, probs = 0.025), q2 = quantile(mse, probs = 0.25), q3 = quantile(mse, probs = 0.75), q4 = quantile(mse, probs = 0.975))

mse_sub <- ggplot(mse_iqr, aes(cali_proj, y = med, ymin = q1, ymax = q3, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

mse_sub2 <- ggplot(mse_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    # coord_cartesian(ylim = quantile(sub_table$residual, c(0, 0.97))) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

# Sharpness
sharpness_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.025), q2 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75), q4 = quantile(sharpness, probs = 0.975))

sharpness_sub <- ggplot(sharpness_iqr, aes(cali_proj, y = med, ymin = q1, ymax = q3, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    coord_cartesian(ylim = c(0, 1)) +
                    labs(y = "Sharpness", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

sharpness_sub2 <- ggplot(sharpness_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    coord_cartesian(ylim = c(0, 1)) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Sharpness", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

# Bias
bias_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(bias), std = sd(bias), q1 = quantile(bias, probs = 0.025), q2 = quantile(bias, probs = 0.25), q3 = quantile(bias, probs = 0.75), q4 = quantile(bias, probs = 0.975))

bias_sub <- ggplot(bias_iqr, aes(cali_proj, y = med, ymin = q1, ymax = q3, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    coord_cartesian(ylim = c(-1, 1)) +
                    labs(y = "Bias", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          strip.text.x = element_blank())

bias_sub2 <- ggplot(bias_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    coord_cartesian(ylim = c(-1, 1)) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Bias", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          strip.text.x = element_blank())


# Call plot
multiplot(residual_sub2, mse_sub2, sharpness_sub2, bias_sub2)
@
\caption{The medians and interquartile ranges of the four prediction metrics - average residual, mean-square error, sharpness, and bias, for Ebola-like, influenza-like, and SARS-like outbreaks for projection window's distance from the calibration window increases from 1 to 4 for calibration windows of size 1 (dark blue), 2 (blue), and 4 (light blue).}
\label{all_metric_plot}
\end{figure}
\FloatBarrier

Figure \ref{all_metric_plot} shows how the performance of the branching process model varies with different combinations of calibration window size and the distance between the calibration window size and projection window for the three simulated diseases. The full results for every calibration window size and projection window number can be found in section 2.3 of the Supplementary Information, though the general trend remains similar across calibration windows. Across all three simulated diseases, the average residual decreased and the MSE increased as the projection window moved further from the calibration window, implying overestimation of daily incidence. The Ebola-like outbreaks saw the greatest overestimations of daily incidence, while SARS forecasts tended to be closest to the true daily incidence. The extent of this overestimation reduced as the calibration window increased for the Ebola-like and influenza-like outbreaks, implying that having more days' worth of data improved the forecasted disease incidence estimates. This was not the case for SARS, which saw a slight increase in overestimation with increasing calibration window size, though the scale of the overestimation was smaller than that of influenza-like or Ebola-like outbreak.

The sharpness of the forecasts for the simulated outbreaks suggests that sharpness was higher with fewer calibration windows and decreased as calibration window size increased or as the projection window moved further from the calibration window. Coincidentally, the number of cases observed in these early calibration windows are low and the projections for these calibration windows include the possibility of no cases occurring (SI Fig. \ref{cali_zero_plot}). As sharpness is measured by observing how far from the median projection a single trajectory is, the sharpness can be expected to be high for a forecast that considers the chances of observing no cases likely. As the projection window moves further from the calibration window, the interquartile range widens. \textcolor{red}{in Discussion say that this is because some projections then may start predicting an outbreak and go further from those that don't - giving big differences}. The drop in sharpness seen as calibration size increases is partly explained by the increasing number of projections that have rejected the possibility of no cases occurring in the projection window as more trajectories deviate from the median at some point during the course of the projection (Fig. \ref{cali_zero_plot}). \textcolor{red}{I could compare diseases an hypothesise about the link between Ebola simulation having larger outbreaks and being the first to exhibit projections where zero incidence is not the case anymore.}

The forecasts' bias scores are mostly positive for Ebola-like and influenza-like outbreaks, though this reduces as calibration window size increases (Fig. \ref{all_metric_plot}). The bias also increases as the projection window moves further from the calibration window for the earlier calibration windows (REF SI). Towards the higher calibration windows, the daily incidence is increasingly often underestimated though the further into the future one forecasts, the more overprediction one sees.

An interesting trend is seen for the first projection window, where MSE increases with increasing calibration window size while the bias score decreases (Fig. \ref{all_metric_plot}). This reminds us that while the MSE gives an indication of whether or not the projection is far from the true daily incidence, unlike the average residual and bias, it does not give a clear indication of over- or under-prediction \textcolor{red}{reference SI MSE and Bias}. 

<<metric_residual_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
residual_lm_start <- lm(residual ~ cali_window_size
                      + disease_num
                      + log(no_cali_cases)
                      + proj_window_no
                      + pred_type,
                        data = total_table)

# AIC to find most parsimonious model
residual_lm_basic <- stepAIC(residual_lm_start)

# Compare basic model to ones with extra things
# Ratio
residual_lm_ratio <- lm(residual ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        + pred_type
                        + cali_proj_ratio,
                        data = total_table)

# Interaction                        
residual_lm_inter <- lm(residual ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        + pred_type
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
residual_lm_inter_ratio <- lm(residual ~ cali_window_size
                              + disease_num
                              + log(no_cali_cases)
                              + proj_window_no
                              + pred_type
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
residual_aic <- AIC(residual_lm_basic, residual_lm_ratio, residual_lm_inter, residual_lm_inter_ratio)

# Analysis of variance
residual_anova <- anova(residual_lm_basic)

# Summary
residual_summary <- summary(residual_lm_basic)
@

<<metric_mse_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
mse_lm_start <- lm(mse ~ cali_window_size
                      + disease_num
                      + log(no_cali_cases)
                      + proj_window_no
                      + pred_type,
                        data = total_table)

# AIC to find most parsimonious model
mse_lm_basic <- stepAIC(mse_lm_start)

# Compare basic model to ones with extra things
# Ratio
mse_lm_ratio <- lm(mse ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        + pred_type
                        + cali_proj_ratio,
                        data = total_table)

# Interaction                        
mse_lm_inter <- lm(mse ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        + pred_type
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
mse_lm_inter_ratio <- lm(mse ~ cali_window_size
                              + disease_num
                              + log(no_cali_cases)
                              + proj_window_no
                              + pred_type
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
mse_aic <- AIC(mse_lm_basic, mse_lm_ratio, mse_lm_inter, mse_lm_inter_ratio)

# Analysis of variance
mse_anova <- anova(mse_lm_inter_ratio)

# Summary
mse_summary <- summary(mse_lm_inter_ratio)
@

<<metric_sharpness_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
sharpness_lm_start <- lm(sharpness ~ cali_window_size
                      + disease_num
                      + log(no_cali_cases)
                      + proj_window_no
                      + pred_type,
                        data = total_table)

# AIC to find most parsimonious model
sharpness_lm_basic <- stepAIC(sharpness_lm_start)

# Compare basic model to ones with extra things
# Ratio
sharpness_lm_ratio <- lm(sharpness ~ cali_window_size
                         + disease_num
                         + log(no_cali_cases)
                         + proj_window_no
                         + pred_type
                         + cali_proj_ratio,
                         data = total_table)

# Interaction                        
sharpness_lm_inter <- lm(sharpness ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        + pred_type
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
sharpness_lm_inter_ratio <- lm(sharpness ~ cali_window_size
                              + disease_num
                              + log(no_cali_cases)
                              + proj_window_no
                              + pred_type
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
sharpness_aic <- AIC(sharpness_lm_basic, sharpness_lm_ratio, sharpness_lm_inter, sharpness_lm_inter_ratio)

# Analysis of variance
sharpness_anova <- anova(sharpness_lm_inter)

# Summary
sharpness_summary <- summary(sharpness_lm_inter)
@

<<metric_bias_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
bias_lm_start <- lm(bias ~ cali_window_size
                      + disease_num
                      + log(no_cali_cases)
                      + proj_window_no
                      + pred_type,
                        data = total_table)

# AIC to find most parsimonious model
bias_lm_basic <- stepAIC(bias_lm_start)

# Compare basic model to ones with extra things
# Ratio
bias_lm_ratio <- lm(bias ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        # + pred_type
                        + cali_proj_ratio,
                        data = total_table)

# Interaction                        
bias_lm_inter <- lm(bias ~ cali_window_size
                        + disease_num
                        + log(no_cali_cases)
                        + proj_window_no
                        # + pred_type
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
bias_lm_inter_ratio <- lm(bias ~ cali_window_size
                              + disease_num
                              + log(no_cali_cases)
                              + proj_window_no
                              # + pred_type
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
bias_aic <- AIC(bias_lm_basic, bias_lm_ratio, bias_lm_inter, bias_lm_inter_ratio)

# Analysis of variance
bias_anova <- anova(bias_lm_inter_ratio)

# Summary
bias_summary <- summary(bias_lm_inter_ratio)
@

<<metric_lm_table, echo = FALSE, results = "asis">>=
library(xtable)

# Best-performing models
residual_summary <- summary(residual_lm_inter)
mse_summary <- summary(mse_lm_inter_ratio)
sharpness_summary <- summary(sharpness_lm_inter)
bias_summary <- summary(bias_lm_inter)

lm_table <- array(NA, dim =c(29, 5))

# disease names
lm_table[1, 1] <- "Residual"
lm_table[8, 1] <- "MSE"
lm_table[16, 1] <- "Sharpness"
lm_table[23, 1] <- "Bias"

# Coefficient names
lm_table[c(1, 8, 16, 23), 2] <- "Intercept"
lm_table[c(2, 9, 17, 24), 2] <- "Cal. size \\textsuperscript{1}"# "Calibration window size"
lm_table[c(3, 10, 18, 25), 2] <- "Disease" # Ebola, influenza, SARS
lm_table[c(4, 11, 19, 26), 2] <- "No. cases \\textsuperscript{2}" # "No. cases in calibration window"
lm_table[c(5, 12, 20, 27), 2] <- "Proj. window \\textsuperscript{3}" # "Projection window no."
lm_table[c(6, 13, 21), 2] <- "Pred. group \\textsuperscript{4}"# "Prediction group type"
lm_table[c(14), 2] <- "Ratio \\textsuperscript{5}" # "Ratio of calibration window size to projection window number"
lm_table[c(7, 15, 22, 28), 2] <- "Interaction \\textsuperscript{6}" # "Interaction between calibration and projection window"

# Estimates and confidence intervals
for (i in 1:7){
lm_table[i, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[i, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[i, 1] - 1.96 * residual_summary$coefficients[i, 2], 2)), "-", sprintf("%.2f", round(residual_summary$coefficients[i, 1] + 1.96 * residual_summary$coefficients[i, 2], 2)), ")", sep = "")
}

for (i in 1:8){
lm_table[(i + 7), 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[i, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[i, 1] - 1.96 * mse_summary$coefficients[i, 2], 2)), "-", sprintf("%.2f", round(mse_summary$coefficients[i, 1] + 1.96 * mse_summary$coefficients[i, 2], 2)), ")", sep = "")
}

for (i in 1:7){
lm_table[(i + 15), 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[i, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[i, 1] - 1.96 * sharpness_summary$coefficients[i, 2], 2)), "-", sprintf("%.2f", round(sharpness_summary$coefficients[i, 1] + 1.96 * sharpness_summary$coefficients[i, 2], 2)), ")", sep = "")
}

for (i in 1:6){
lm_table[(i + 22), 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[i, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[i, 1] - 1.96 * bias_summary$coefficients[i, 2], 2)), "-", sprintf("%.2f", round(bias_summary$coefficients[i, 1] + 1.96 * bias_summary$coefficients[i, 2], 2)), ")", sep = "")
}

# p-value
lm_table[1:7, 4] <- signif(residual_summary$coefficients[, 4], 3)
lm_table[8:15, 4] <- signif(mse_summary$coefficients[, 4], 3)
lm_table[16:22, 4] <- signif(sharpness_summary$coefficients[, 4], 3)
lm_table[23:28, 4] <- signif(bias_summary$coefficients[, 4], 3)

# Adjusted R-squared
lm_table[1, 5] <- round(residual_summary$adj.r.squared, 2)
lm_table[8, 5] <- round(mse_summary$adj.r.squared, 2)
lm_table[16, 5] <- round(sharpness_summary$adj.r.squared, 2)
lm_table[23, 5] <- round(bias_summary$adj.r.squared, 2)

colnames(lm_table) <- c("Metric", "Coefficient", "Estimate (95\\% CI)", "p-value", "Adjusted $R^{2}$")

addtorow <- list(pos = list(29), command = NULL)

# command <- paste0("\\hline\n\\endhead\n",
# "\\hline\n",
# "\\multicolumn{", dim(lm_table)[2] + 1, "}{l}",
# "{\\footnotesize Footnote here}\n",
# "\\endfoot\n",
# "\\endlastfoot\n")
# add.to.row$command <- command

# addtorow$pos <- list(0, 0)
# addtorow$command <- c("& \\multicolumn{3}{X}{footnote here} & \\multicolumn{3}{X}{$\\beta$}  & \\multicolumn{3}{X}{$\\gamma$} \\\\\n", "$R_{0}$ & simulation & PE & min & max & PE & min & max \\\\\n")
addtorow$command <- c("\\multicolumn{5}{X}{\\textsuperscript{1} calibration window size \\newline \\textsuperscript{2} number of cases in calibration window \\newline \\textsuperscript{3} projection window \\newline \\textsuperscript{4} type of prediction \\newline \\textsuperscript{5} ratio of calibration window size to projection window number \\newline \\textsuperscript{6} additive interaction between calibration window size and projection window number} \\\\\n")

#print(x.big, hline.after=c(-1), add.to.row = add.to.row, tabular.environment = "longtable")

tab <- xtable(lm_table, caption = "The coefficients of the best-performing linear regression models for explaining variation in the average residuals, mean-square error, sharpness, and bias in the forecasts of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. CI: confidence interval, MSE: mean-square error. \\textcolor{red}{I need to get xtable to keep the correct number of decimal points}", label = "metric_lm_table")
digits(tab) <- 2
# align(tab) <- "lXX{2cm}XXX"
align(tab) = c("p{0.1\\textwidth}", "p{0.1\\textwidth}", "p{0.18\\textwidth}", "p{0.34\\textwidth}", "p{0.13\\textwidth}", "p{0.15\\textwidth}")
print(tab, hline.after=c(-1, 0, 7, 15, 22, 28), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "bottom", type = "latex", sanitize.rownames.function = identity, sanitize.colnames.function = identity, sanitize.text.function = identity, add.to.row = addtorow, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@

\textcolor{red}{When it comes to presenting p-values etc., I've presented the ones you get with summary(). Have I presented everything I need to present when it comes to the regression?}

<<metric_correlations, echo = FALSE, results = "hide", message = FALSE>>=
# Testing for correlations between metrics
# gives you the correlation coefficient
# res_mse_cor <- cor(x = total_table$residual, y = total_table$mse)
res_mse_cor <- cor.test(x = total_table$residual, y = total_table$mse)
res_sha_cor <- cor.test(x = total_table$residual, y = total_table$sharpness)
res_bia_cor <- cor.test(x = total_table$residual, y = total_table$bias)
mse_sha_cor <- cor.test(x = total_table$mse, y = total_table$sharpness)
mse_bia_cor <- cor.test(x = total_table$mse, y = total_table$bias)
sha_bia_cor <- cor.test(x = total_table$sharpness, y = total_table$bias)
@

<<sharpness_calculations, echo = FALSE, eval = TRUE, results = "hide">>=
# Proportion of prediction type by disease

# Ebola
ebola_1 <- sum(ebola_table$pred_type == 1) / nrow(ebola_table)
ebola_2 <- (sum(ebola_table$pred_type == 2) / nrow(ebola_table)) * 100

# SARS
sars_1 <- sum(sars_table$pred_type == 1) / nrow(sars_table)
sars_2 <- (sum(sars_table$pred_type == 2) / nrow(sars_table)) * 100

# Influenza
influenza_1 <- sum(influenza_table$pred_type == 1) / nrow(influenza_table)
influenza_2 <- (sum(influenza_table$pred_type == 2) / nrow(influenza_table)) * 100

@
\FloatBarrier

The most parsimonious linear regression models with an addition of interaction terms and where this lowered the AIC is presented for each of the four forecast performance metricsin Table \ref{metric_lm_table}. A comparison of the AICs of all the linear models can be found in Supplementary Information Table \ref{sim_metric_aic_table}. For the average residual and sharpness, the best model was one which contained all the explanatory variables and an additive interaction term between calibration window size and projection window number. For the MSE, the best model additionally included the ratio of calibration window size to projection window number. For bias, the best model included all explanatory variables excluding the prediction type of the forecast (whether or not the forecast includes the possibility of no incidence) and included the interaction term. Out of the metrics, the change in sharpness due to the explanatory variables is best explained by the linear regression model with an adjusted R-squared of 0.47, while the MSE was least explained with an adjuste R-squared of 0.06.

The correlations between metrics found that the correlation coefficient of the MSE and average residual was \Sexpr{round(res_mse_cor$estimate, 2)}, which means that MSE tends to decrease with increasing average residual. Bias also decreases with increasing average residual with a correlation coefficient of \Sexpr{round(res_bia_cor$estimate, 2)}. Sharpness, on the other hand, increase as the average residual increases with a correlation coefficient of \Sexpr{round(res_sha_cor$estimate, 2)} and decreases with increasing MSE (\Sexpr{round(mse_sha_cor$estimate, 2)}). Bias is positively correlated with MSE with a correlation coefficient of \Sexpr{round(mse_bia_cor$estimate, 2)}, while bias is negatively correlated with sharpness with a correlation coefficient of \Sexpr{round(sha_bia_cor$estimate, 2)}.

\textcolor{red}{I'm feeling a bit insecure about how I should talk about the linear regression table. Any advice is appreciated. I was thinking that it's overkill to go through each explanatory and metric separately in a case like this, but when I've done regression analysis before for more traditional epidemiological coursework, we discussed all variables in painstaking detail - this isn't very interesting way of doing things for a paper-style report though.}

The sharpness, described as a measure of how narrow the range of predictions for a given time window are from a scale of 0 (wide) to 1 (narrow), varied by the types of predictions that were given within a single projection of a simulation (SI Fig. \ref{sharpness_type_plot}). There are three possible types of projections: ones where all of the projection's 10,000 projection trajectories include a prediction of a daily incidence of 0 cases, ones where some of the trajectories include a prediction of daily incidence of 0 cases, and ones where none of the trajectories include a prediction of a daily incidence of 0 cases.

For projections of simulations that contained an estimate of a daily incidence of 0, the sharpness varied more wildly than for projections, also dipping to lower sharpnesses than the projections that did not contain a daily incidence of 0, reflecting the possibility of a projection to spiral out of control and have a wide range of predictions for a given projection window (SI Fig. \ref{sharpness_type_plot}). Additionally, the distribution of sharpness scores within the zero-incidence-including groups was split into two for each simulated disease - either sharpness was very higher than that of the group that did not include any zero-incidence, or sharpness was lower than the majority of the sharpness distribution for the prediction type group that did not include zero-incidence (SI Fig. \ref{sharpness_type_plot}). For the Ebola simulations' forecasts, \Sexpr{round(ebola_2, 1)}\% contained zero-incidence, while \Sexpr{round(influenza_2, 1)}\% and \Sexpr{round(sars_2, 1)}\% of influenza and SARS simulations' forecasts contained the prediction of zero-incidence, respectively. The high proportion of forecasts containing zero-incidence suggests that for the calibration windows used in this study, the branching process model could often not be used to decipher whether the outbreak would undergo stochastic extinction or not based on the calibration windows used in this study.

\FloatBarrier
\subsection{Empirical outbreaks}

%\subsubsection{Prediction metrics}

Daily incidence for the three empirical outbreaks, Ebola, influenza, and SARS, were forecasted and analysed using the same methods as those used for the simulated outbreaks. The incidence curves of the empirical outbreaks can be found in SI Figure \ref{empirical_plot}. Unlike for the simulated outbreaks, none of the projection trajectories for Ebola or influenza included the possibility of a daily incidence of zero for any of the calibration window and projection window combinations. For SARS, only the projections for the first calibration window included the possibility.

\begin{figure}[h]
<<all_real_metric_plot, echo = FALSE, fig.width = 8, fig.height = 10>>=
# Only want calibration windows 1, 2, and 4
sub_table <- filter(real_total_table, cali_window_size == 1 | cali_window_size == 2 | cali_window_size == 4) 

# Make a new column so that I get all the combinations into one graph
sub_table$cali_proj <- paste(sub_table$cali_window_size, ":", sub_table$proj_window_no, sep = "")

# Residual
residual_sub <- ggplot(sub_table, aes(x = cali_proj, y = residual, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# MSE
mse_sub <-  ggplot(sub_table, aes(x = cali_proj, y = mse, color = cali_window_size)) +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# Sharpness
sharpness_sub <- ggplot(sub_table, aes(x = cali_proj, y = sharpness, color = cali_window_size)) +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    coord_cartesian(ylim = c(0, 1)) +
                    labs(y = "Sharpness", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# Bias
bias_sub <- ggplot(sub_table, aes(x = cali_proj, y = bias, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    coord_cartesian(ylim = c(-1, 1)) +
                    labs(y = "Bias", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# Call plot
multiplot(residual_sub, mse_sub, sharpness_sub, bias_sub)
@
\caption{How the four prediction metrics for empirical Ebola, influenza, and SARS outbreaks differ as the projection window's distance from the calibration window increases from 1 to 4 for calibration windows of size 1 (dark blue), 2 (blue), and 4 (light blue).}
\label{all_real_metric_plot}
\end{figure}

As can be seen in \ref{all_real_metric_plot}, the branching process model fared worse at predicting daily incidence on the empirical outbreaks than on the simulated outbreaks according to the prediction metrics as exhibited by the scale of differences between the model's performance for different calibration and projection windows the the average residual and MSE. As with the simulated outbreaks, the full performance metrics for each calibration window size and projection window size combination is presented in the Supplementary Information (section 1.7).

The patterns of performance of the branching process model in forecasting incidence for the SARS outbreak is different than those of the Ebola and influenza outbreaks \ref{all_metric_plot}. The SARS outbreak analysed here is one which had two clusters of cases with a period of few cases in between. The outbreak window observed here only covers the first cluster, and the branching process model did not perform well at predicting future incidence for the time period of lower incidence after the first cluster. The Ebola and influenza outbreaks analysed in this study were more clearly still in the initial phase of the outbreak, where daily incidence may still be increasing. 

As the model was not predicting zero incidence as often for the empirical outbreaks as for the simulated outbreaks, the sharpness metric performs more as one might have expected it to for the simulated outbreaks as well. For Ebola and influenza, sharpness decreases as the projection window moves further from the calibration window, reflecting increasing uncertainty in the bounds of daily incidence. Additionally, sharpness is seen to increase with increasing calibration window size. Sharpness starts off high for SARS as a forecast of no cases was a possibility. The sharpness then decreases from there as the forecast moves further into the future. From here, the sharpness of the forecasts increases as forecasts move to later dates.

Compared to the simulated outbreaks, the empirical Ebola and influenza outbreaks were overestimating the number of cases in the projection window more consistently, as can be deducted from observing the bias metric for Ebola and influenza \ref{all_real_metric_plot}. The overestimation increased with as projection windows were further from calibration windows, as was also observed for the simulated outbreaks. SARS, on the other hand, underestimates incidence for the earlier calibration windows but then starts to overestimate incidence when 

%%%%%%%%%%%%%%%%
%% Discussion %%
%%%%%%%%%%%%%%%%

\newpage
\section{Discussion}
\textcolor{red}{Thibaut and I didn't discuss my Discussion much either, but he would like it to answer questions such as "When does the model work?" "When didn't it work so well" "Are we good at estimating uncertainty?" "Are there any systematic biases?" "I have X amount of data - how far can I project? Can I trust what I have projected for the next serial interval"}

% \subsection{Summary of findings and general discussion}
Overall, the analysis of forecasted incidence based on simulated outbreaks of Ebola-like and influenza-like outbreaks found that a forecast is more trustworthy the longer the calibration window is and the closer projection days of interest are to the calibration window (Fig. \ref{all_metric_plot}).

While the branching process model was better at forecasting daily incidence for simulated outbreaks according to the residual, MSE, and bias, the sharpness of the forecasts was arguably just as good or better for the empirical outbreaks' forecasts (Fig. \ref{all_metric_plot}, \ref{real_all_metric_plot}). This could be because the simulated outbreaks represented an ideal scenario where every case is reported, even the index case and the few sporadic cases that occurred before the outbreak started to gain momentum, unlike the empirical outbreaks for which the outbreak started at a point where there were already cases every day or nearly so (Fig. \ref{empirical_plot}). This meant that the simulated outbreaks had much uncertainty at the beginning of their outbreaks as to whether the outbreak would go extinct or not, resulting in many predictions having a wider scope of possible scenarios that could take place, especially as projection windows were taken further from the calibration windows.

Out of the performance metrics used in this study, the MSE may have been superfluous considering that the same trends could be seen with the average residual, though their scales were slightly different. Additionally, sharpness as a performance metric might be more informative for later stage outbreaks or outbreaks where the forecast suggests that the chances of the outbreak dying off are slim. For the simulated outbreaks, it may have been too early to evaluate sharpness in a coherent matter, as the forecasts were predicting the outbreak to die out.

While the branching process model had a tendency of overestimating the forecasted incidence of simulated outbreaks, this was exasperated for early calibration windows compared to later calibration windows and further projection windows in comparison to ones closer to the calibration window. The extent to which the reduction in overestimating with later calibration window size was due to there being more days' worth of cases or due to the outbreak being further along in its growth is a question that cannot be answered with the current model, as the force of infection calculation used for estimating the number of cases in the future does not work as intended if the contribution of cases from earlier on in the outbreak to the force of infection are ignored.

It is possible that Ebola-like outbreaks were observed to have the greatest overestimations in forecasted incidence due to them also having the largest outbreak sizes (Fig. \ref{sim_dist}). The average residual considers the absolute difference between the projection and true data rather than the relative difference. This means that if one model forecasts 20 cases when there were actually 10 and another 2 cases when there was 1, the average residual of the model that forecasted 20 cases will be worse than the one that predicted 2 cases even though both models forecasted two times too many cases.

The empirical SARS forecasts behaved differently from the Ebola and influenza forecasts and were quite far from the true incidence towards the end of the observation window. While retrospectively the reason for why this happened can be easily explained through noting that this particular outbreak consisted of two peaks of cases instead of one, it does provide a reminder of one of the limitations of the branching process model used for this exercise. The model assumes that that the growth of the outbreak, if it happens, will be exponential. Applying this to an outbreak that does not follow the assumption will result in subpar projections, much like applying Farr's law to the HIV epidemic \citep{Artzrouni1990}. The only problem with avoiding the wrong shape of outbreak for the tool in question is that when it comes to real-time forecasting of outbreaks, its shape is not known in advance. Others have also noted the difficulties faced by branching process modelling methods when encountering a non-exponentially growing outbreak \citep{Nouvellet2017, Viboud2017}.

One of the limitations of this study is that the performance of the forecasting method is mostly evaluated under ideal circumstances on simulated outbreaks that have been created using a similar model. This means that abnormalities, such as unexpected growth curves, are not a concern and are not considered. Regardless, as the performance of the forecasting method used in this study has not been evaluated before, starting from the ideal scenario could be considered to be a reasonable starting point.

%Points to discuss:
%\begin{itemize}
%  \item Sharpness gets better with more calibration windows regardless of the forecast being in the wrong place.
%\end{itemize}

% Forecasting further than three serial intervals onward based on one serial interval's worth of data resulted in an overestimation of the forecasted disease incidence for Ebola-like and influenza-like disease, and to a lesser extent, SARS-like disease (Fig. \ref{all_metric_plot}, SI section 1.3).

% \subsection{Limitations}
% 
% 
% \begin{itemize}
%   \item In this study I only look at the performance of the branching process model under ideal circumstances, where the serial interval is known and issues such as reporting delays do not exist. On top of this, the simulations that I am running my predictive branching process model on also come from a branching process model.
%   \item The projections are based on a branching process model assuming an infinite pool of susceptibles. This assumption is alright for early outbreak analysis, where depletion of susceptibles isn't that much of an issue.
%   \item Real outbreaks can miss the first few weeks of there not being that many cases due to surveillance methods not being in place. The issue with using real historical outbreak data (especially old data) is that the outbreaks that get recorded and remembered tend to be exceptional. This means that small outbreaks are hard to come by and that models that work nicely for early outbreak surveillance may not work well for these well-documented freak cases. This idea is tentatively supported by for example by the proportion of projection windows containing forecasts of zero-incidence. Two of the three real outbreaks barely had a time window suggesting the possibility of zero incidence while the majority of the simulated outbreaks did contain a possibility of a daily incidence of zero. 
% \end{itemize}
% 
% \subsection{Future work}
In future work, the impact of factors such as under-reporting, reporting delays, or super-spreading on the performance of the branching process model could be investigated to identify further possible precautions that should be kept in mind when forecasting.

\textcolor{red}{I feel like I should be comparing my findings to other peoples' work and reference some papers in here but I have not done that yet.}

\subsection{Conclusions}

\textcolor{red}{Conclusions here}

%%%%%%%%%%%%%%%%
%% References %%
%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{unsrtnat}
\begin{singlespace}
\begin{group}
   % \fontsize{11pt}
  \bibliography{/home/evelina/Documents/Mendeley/MRes_forecasting.bib}
\end{group}
\end{singlespace}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary information %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\etocsettagdepth{mtappendix}{subsubsection}

\renewcommand\thefigure{\thesection.\arabic{figure}} 
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{figure}{0}   
\renewcommand{\thesection}{\arabic{section}}
\newpage
\section*{Supplementary Information}
% \addcontentsline{toc}{section}{Supplementary Information}

%% Table of Contents
\subsection*{Table of Contents}
\vspace{-4em}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\setcounter{section}{1}
\section{Methods}
\subsection{Branching process projections}
\textcolor{red}{I haven't finished this part of the methods yet.}
In this supplementary information section I explain how the \textit{projections} package works as it is not documented elsewhere. 

To forecast disease incidence using a branching process model, such as the on used in the \textit{projections}-package, the following steps are taken.

First the observed daily incidence (cases per day) and known estimates of the serial interval distribution were used to obtain a likelihood distribution for the effective reproduction number, $R$. As there is no data on who infected whom, this can be inferred by calculating for each pair of cases the relative likelihood that one was infected by the other \citep{Wallinga2004}. Thus for a given case $j$, the effective reproduction number is the sum of the likelihoods $p$ that given cases $i$ were infected by case $j$:
\begin{align*}
R_{j} = \sum_{i}{p_{ij}}
\end{align*}
Estimating the case reproduction numbers for cases $j$ in the calibration window produces a distribution of values of $R$ \citep{Cori2013}. 

This estimation of $R$ is then utilised to estimate the incidence for the day directly after the end of the calibration window. For the following projection days, the forecasted daily incidence during the previous infection days is taken into account.

Explain how branching process models work. Take the calibration window, estimate the likelihood distribution of the reproduction number of the observed incidence.

Mention that the one used here is found in the \textit{projections}-package.

\FloatBarrier
\newpage
\section{Simulated outbreaks}
\subsection{Distribution of total outbreak size}

\begin{figure}[h]
<<sim_dist_plot, echo = FALSE, fig.width = 8, fig.height = 8>>=
ebola_dist_table <- ebola_table %>% group_by(dataset) %>% summarise(size = median(outbreak_size))
influenza_dist_table <- influenza_table %>% group_by(dataset) %>% summarise(size = median(outbreak_size))
sars_dist_table <- sars_table %>% group_by(dataset) %>% summarise(size = median(outbreak_size))

ebola_sim_dist <- ggplot(ebola_dist_table, aes(x = size)) +
                    geom_histogram(alpha = 0.4, binwidth = 7, position = "identity") + 
                    labs(y = "Frequency", x = "Outbreak size") +
                    # facet_wrap(~disease, scales = "free") +
                    theme(legend.position = "none") +
                    annotate("text", label = "A", x = 600, y = 5)

flu_sim_dist <- ggplot(influenza_dist_table, aes(x = size)) +
                    geom_histogram(alpha = 0.4, binwidth = 0.7, position = "identity") + 
                    labs(y = "Frequency", x = "Outbreak size") +
                    # facet_wrap(~disease, scales = "free") +
                    theme(legend.position = "none") +
                    annotate("text", label = "B", x = 48, y = 10)

sars_sim_dist <- ggplot(sars_dist_table, aes(x = size)) +
                    geom_histogram(alpha = 0.4, binwidth = 1.5, position = "identity") + 
                    labs(y = "Frequency", x = "Outbreak size") +
                    # facet_wrap(~disease, scales = "free") +
                    theme(legend.position = "none") +
                    annotate("text", label = "C", x = 115, y = 6)

multiplot(ebola_sim_dist, flu_sim_dist, sars_sim_dist)

@
\caption{Distribution of outbreak sizes for simulated outbreaks of Ebola-like (A), influenza-like (B), and severe acute respiratory syndrome (SARS)-like diseases (C).}
\label{sim_dist}
\end{figure}

\FloatBarrier
\newpage
\subsection{Full prediction metrics}


\subsubsection{Average residual}
\begin{figure}[h]
<<whole_residual, echo = FALSE, cache = FALSE, fig.width = 8, fig.height = 8, warning = FALSE>>=
residual_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(residual), std = sd(residual), q1 = quantile(residual, probs = 0.025), q2 = quantile(residual, probs = 0.25), q3 = quantile(residual, probs = 0.75), q4 = quantile(residual, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

residual_massive <- ggplot(residual_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

residual_massive2 <- ggplot(residual_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Residual", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +


# Print the figure
residual_massive2
@
\caption{The median average residual (circle) along with its interquatile range for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for simulated outbreaks of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases. The projections tend to approach the ideal average residual of 0 with increasing calibration window size. Additionally, the average residuals tend to be negative, implying overestimation of daily disease incidence. \textcolor{red}{make a labeller function to say "projection 1" etc. in strip}}
\label{whole_residual}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{MSE}
\begin{figure}[h]
<<whole_mse, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8, warning = FALSE>>=
mse_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(mse), std = sd(mse), q1 = quantile(mse, probs = 0.025), q2 = quantile(mse, probs = 0.25), q3 = quantile(mse, probs = 0.75), q4 = quantile(mse, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

mse_massive <- ggplot(mse_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

mse_massive2 <- ggplot(mse_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "MSE", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

# Print the figure
mse_massive2
@
\caption{The mean-square error (MSE) (circle) along with its interquatile range for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for simulated outbreaks of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases.}
\label{whole_mse}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Sharpness}
\begin{figure}[h]
<<whole_sharpness, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8, warning = FALSE>>=
sharpness_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.025), q2 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75), q4 = quantile(sharpness, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

sharpness_massive <- ggplot(sharpness_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "Sharpness", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(0, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

sharpness_massive2 <- ggplot(sharpness_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Sharpness", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(0, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

# Print the figure
sharpness_massive2
@
\caption{The sharpness (circle) along with its interquatile range for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for simulated outbreaks of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases.}
\label{whole_sharpness}
\end{figure}
\FloatBarrier

\begin{figure}[h]
<<sharpness_plot_total, echo = FALSE, fig.width = 7, fig.height = 3>>=
library(ggplot2)
text_size <- 5

# RMSE plot with calibration window to projection window ratio
sharpness_table <- total_table %>% group_by(disease, pred_type) %>% summarise(avg = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75))
                          
sharpness_type <- ggplot(total_table, aes(factor(pred_type),
                     sharpness, color = factor(disease), fill = factor(disease))) +
                     coord_cartesian(ylim = c(0, 1)) +
                     # scale_y_log10() +
                     facet_wrap(~disease) +
                     # geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     geom_boxplot(position = position_dodge(width = 1), width = 0.1, fill = "white", color = "black") +
                     labs(y = "Sharpness", x = "Prediction group type") +
                     theme(legend.position = "none") +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))

# Table that I print
sharpness_type
@
\caption{Sharpness by prediction type for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza. The different prediction types include ones which have trajectories predicting only zero incidence ("only 0"), no trajectories predicting zero incidence ("no 0"), or some trajectories predicting zero incidence ("include 0").}
\label{sharpness_type_plot}
\end{figure}

\newpage
\begin{figure}[h]
\subsubsection{Bias}
<<whole_bias, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8, warning = FALSE>>=
bias_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(bias), std = sd(bias), q1 = quantile(bias, probs = 0.025), q2 = quantile(bias, probs = 0.25), q3 = quantile(bias, probs = 0.75), q4 = quantile(bias, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

facet_labeller <- function(variable,value){
  if (variable == "facet1") {
    return(facet1_names[value])
  } else if (variable == "facet2") {
    return(facet2_names[value])
  } else {
    return(as.character(value))
  }
}

bias_massive <- ggplot(bias_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "Bias", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(-1, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

bias_massive2 <- ggplot(bias_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Bias", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(-1, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

# Print the figure
bias_massive2
@
\caption{The bias (circle) along with its interquatile range for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for simulated outbreaks of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases.}
\label{whole_bias}
\end{figure}
\FloatBarrier

\newpage
\subsection{Distribution of metrics}
\begin{figure}[h]
<<dist_plot, echo = FALSE, fig.width = 8, fig.height = 9>>=
new_mse_table <- total_table %>% mutate(mse_new = ifelse(mse > 30, 30, mse))
new_residual_table <- total_table %>% mutate(residual_new = ifelse(residual < -20, -20, residual))

mse_dist <- ggplot(new_mse_table, aes(x = mse_new, fill = factor(disease, levels = c("influenza", "sars", "ebola")))) +
                    geom_histogram(alpha = 0.4, binwidth = 0.5, position = "identity") + 
                    labs(y = "Frequency", x = "MSE") +
                    theme(legend.position = "none") +
                    guides(fill = guide_legend(reverse = TRUE))

residual_dist <- ggplot(new_residual_table, aes(x = residual_new, fill = disease)) +
                        geom_histogram(alpha = 0.4, binwidth = 0.5, position = "identity") + 
                        labs(y = "Frequency", x = "Average Residual") +
                        theme(legend.position = "none")

sharp_dist <- ggplot(total_table, aes(x = sharpness, fill = disease)) +
                     geom_histogram(alpha = 0.4, binwidth = 0.02, position = "identity") + 
                     labs(y = "Frequency", x = "Sharpness") +
                     theme(legend.position = "none")

bias_dist <- ggplot(total_table, aes(x = bias, fill = disease)) +
                     geom_histogram(alpha = 0.4, binwidth = 0.05, position = "identity") + 
                     labs(y = "Frequency", x = "Bias") +
                     theme(legend.position = "none")

bias_freq <- ggplot(total_table, aes(bias, colour = disease)) +
                    geom_freqpoly(binwidth = 0.05)

multiplot(mse_dist, residual_dist, sharp_dist, bias_dist)

@
\caption{The distributions of prediction metrics for Ebola (red), influenza (green), and SARS (blue).}
\label{dist_plot}
\end{figure}

\FloatBarrier
\newpage
\subsection{Zero-incidence}
\begin{figure}[h]
<<pred_case_plot, echo = FALSE, fig.width = 7, fig.height = 3>>=
# Convert days since case to SIs since case
total_table$si[total_table$disease == "ebola"] <- 11.6
total_table$si[total_table$disease == "influenza"] <- 2.6
total_table$si[total_table$disease == "sars"] <- 8.7
total_table$si_since_case <- total_table$days_since_case / total_table$si

pred_case_fig <- ggplot(total_table, aes(factor(pred_type),
                     si_since_case, color = factor(disease), fill = factor(disease))) +
                     facet_wrap(~disease, scales = "free") +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     geom_boxplot(width = 0.2, fill = "white", color = "black", position = position_dodge(width = 1)) +
                     labs(y = "Serial intervals since observed case", x = "Prediction group type") +
                     coord_cartesian(ylim = c(0, 8)) +
                     theme(legend.position = "none") +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))

pred_case_scatter <- ggplot(total_table, aes(si_since_case, prop_zero, color = factor(disease), fill = factor(disease))) +
                     facet_wrap(~disease, scales = "free") +
                     geom_point() +
                     labs(y = "Proportion predicting zero-incidence", x = "Serial intervals since last observed case") +
                     coord_cartesian(xlim = c(0, 8)) +
                     theme(legend.position = "none")

pred_case_scatter
@
\caption{The proportion of trajectories for each projection predicting a daily incidence of 0 for a given projection window by the number of serial intervals that have passed between the last observed case and the end of the calibration window for Ebola (red), influenza (green), and SARS (blue).}
\label{pred_case_plot}
\end{figure}

\begin{figure}[h]
<<zero_pred_plot, echo = FALSE, fig.width = 7, fig.height = 3>>=
zero_pred_scatter <- ggplot(total_table, aes(no_proj_cases, prop_zero, color = factor(disease), fill = factor(disease))) +
                     facet_wrap(~disease, scales = "free") +
                     geom_point() +
                     labs(y = "Proportion predicting zero-incidence", x = "Number of cases in prediction window") +
                     # coord_cartesian(xlim = c(0, 8)) +
                     theme(legend.position = "none")

zero_pred_scatter
@
\caption{The proportion of trajectories for each projection predicting a daily incidence of 0 for a given projection window compared to the true number of cases in that prediction window for Ebola (red), influenza (green), and SARS (blue).}
\label{zero_pred_plot}
\end{figure}

\textcolor{red}{Figure \ref{cali_zero_plot} is pretty ugly - as in I'm probably the only person who can make sense of it - but I think it has potential if I could make it less ugly. It essentially shows at what combination of windows you start to see projections where extinction is not included in the forecast}

\begin{figure}[h]
<<cali_zero_table, echo = FALSE, fig.width = 8, fig.height = 9>>=
cali_zero_plot <- ggplot(total_table, aes(factor(pred_type), no_cali_cases, fill = factor(disease), color = factor(disease))) + # , fill = factor(disease)
                     # coord_cartesian(ylim = c(0, 50)) +
                     facet_wrap(cali_window_size~proj_window_no, scale = "free") +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     # geom_boxplot(position = position_dodge(width = 1), width = 0.2, fill = "white", color = "black") +
                     labs(y = "Number of cases in calibration window", x = "Prediction type") +
                     # scale_x_discrete(limits = 0:2, labels = c("only 0", "no 0", "includes 0")) +
                     theme(legend.position = "none",
                           axis.text.x = element_text(angle = 45))

cali_zero_plot
@
\caption{The number of cases in a calibration window for each prediction type split by calibration window size (top number in headers over panels) and projection window (bottom number in headers over panels) - projections with trajectories that predict only a daily incidence of zero cases are referred to as "0", only non-zero cases as "1", and including trajectories of zero incidece "2" for Ebola (red), influenza (green), and SARS (blue).}
\label{cali_zero_plot}
\end{figure}

\FloatBarrier
\newpage
\section{Empirical outbreaks}
\subsection{Daily incidence of outbreaks}
<<real_projections, eval = TRUE, echo = FALSE, message = "hide", warning = FALSE>>=
library(epitrix)
library(distcrete)
library(incidence)
library(earlyR)
library(projections)
library(EpiEstim)
library(outbreaks)

set.seed(1)
# Ebola
setwd("/home/evelina/Development/forecasting/data/")
ebola_sl <- read.csv("sierraleone_ebola_2014_clean.csv")
ebola_sl$date <- as.Date(ebola_sl$date, format = "%d/%m/%Y")
ebola_i <- as.incidence(ebola_sl$new_cases, ebola_sl$date, interval = 1)
setwd("/home/evelina/Development/forecasting/simulations/real_outbreaks/ebola/")
delta <- 12

# Get serial interval and R calculation
# cv = sigma / mean 
ebola_sim_si <- gamma_mucv2shapescale(11.6, (5.6/11.6))
ebola_si <- distcrete("gamma", shape = ebola_sim_si$shape, scale = ebola_sim_si$shape, w = 0, interval = 1)
ebola_R1 <- get_R(ebola_i[1:(delta * 1), ], si = ebola_si, max_R = 10)
ebola_R2 <- get_R(ebola_i[1:(delta * 2), ], si = ebola_si, max_R = 10)
ebola_R3 <- get_R(ebola_i[1:(delta * 3), ], si = ebola_si, max_R = 10)
ebola_R4 <- get_R(ebola_i[1:(delta * 4), ], si = ebola_si, max_R = 10)
ebola_R5 <- get_R(ebola_i[1:(delta * 5), ], si = ebola_si, max_R = 10)
ebola_R6 <- get_R(ebola_i[1:(delta * 6), ], si = ebola_si, max_R = 10)
ebola_R7 <- get_R(ebola_i[1:(delta * 7), ], si = ebola_si, max_R = 10)

# Projections
ebola_proj1 <- project(ebola_i[1:delta, ], R = sample_R(ebola_R1, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj2 <- project(ebola_i[1:(delta*2), ], R = sample_R(ebola_R2, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj3 <- project(ebola_i[1:(delta*3), ], R = sample_R(ebola_R3, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj4 <- project(ebola_i[1:(delta*4), ], R = sample_R(ebola_R4, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj5 <- project(ebola_i[1:(delta*5), ], R = sample_R(ebola_R5, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 3), R_fix_within = TRUE)
ebola_proj6 <- project(ebola_i[1:(delta*6), ], R = sample_R(ebola_R6, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 2), R_fix_within = TRUE)
ebola_proj7 <- project(ebola_i[1:(delta*7), ], R = sample_R(ebola_R7, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)

# Influenza
data("Flu1918")
flu_1918 <- Flu1918
flu_i <- as.incidence(flu_1918$incidence, interval = 1)

delta <- 3

# Get serial interval and R calculation
# cv = sigma / mean 
flu_sim_si <- gamma_mucv2shapescale(2.6, (1.5/2.6))
flu_si <- distcrete("gamma", shape = flu_sim_si$shape, scale = flu_sim_si$shape, w = 0, interval = 1)
flu_R1 <- get_R(flu_i[1:(delta * 1), ], si = flu_si, max_R = 10)
flu_R2 <- get_R(flu_i[1:(delta * 2), ], si = flu_si, max_R = 10)
flu_R3 <- get_R(flu_i[1:(delta * 3), ], si = flu_si, max_R = 10)
flu_R4 <- get_R(flu_i[1:(delta * 4), ], si = flu_si, max_R = 10)
flu_R5 <- get_R(flu_i[1:(delta * 5), ], si = flu_si, max_R = 10)
flu_R6 <- get_R(flu_i[1:(delta * 6), ], si = flu_si, max_R = 10)
flu_R7 <- get_R(flu_i[1:(delta * 7), ], si = flu_si, max_R = 10)

# Projections
flu_proj1 <- project(flu_i[1:delta, ], R = sample_R(flu_R1, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj2 <- project(flu_i[1:(delta*2), ], R = sample_R(flu_R2, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj3 <- project(flu_i[1:(delta*3), ], R = sample_R(flu_R3, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj4 <- project(flu_i[1:(delta*4), ], R = sample_R(flu_R4, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj5 <- project(flu_i[1:(delta*5), ], R = sample_R(flu_R5, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 3), R_fix_within = TRUE)
flu_proj6 <- project(flu_i[1:(delta*6), ], R = sample_R(flu_R6, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 2), R_fix_within = TRUE)
flu_proj7 <- project(flu_i[1:(delta*7), ], R = sample_R(flu_R7, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)

# SARS
data("sars_canada_2003")
sars_2003 <- sars_canada_2003
sars_2003$total_cases <- rowSums(sars_2003[ , 2:5])
sars_2003$date <- as.Date(sars_2003$date)
sars_i <- as.incidence(sars_2003$total_cases, sars_2003$date, interval = 1)

delta <- 9

# Get serial interval and R calculation
# cv = sigma / mean 
sars_sim_si <- gamma_mucv2shapescale(8.7, (3.6/8.7))
sars_si <- distcrete("gamma", shape = sars_sim_si$shape, scale = sars_sim_si$shape, w = 0, interval = 1)
sars_R1 <- get_R(sars_i[1:(delta * 1), ], si = sars_si, max_R = 10)
sars_R2 <- get_R(sars_i[1:(delta * 2), ], si = sars_si, max_R = 10)
sars_R3 <- get_R(sars_i[1:(delta * 3), ], si = sars_si, max_R = 10)
sars_R4 <- get_R(sars_i[1:(delta * 4), ], si = sars_si, max_R = 10)
sars_R5 <- get_R(sars_i[1:(delta * 5), ], si = sars_si, max_R = 10)
sars_R6 <- get_R(sars_i[1:(delta * 6), ], si = sars_si, max_R = 10)
sars_R7 <- get_R(sars_i[1:(delta * 7), ], si = sars_si, max_R = 10)

# Projections
sars_proj1 <- project(sars_i[1:delta, ], R = sample_R(sars_R1, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj2 <- project(sars_i[1:(delta*2), ], R = sample_R(sars_R2, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj3 <- project(sars_i[1:(delta*3), ], R = sample_R(sars_R3, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj4 <- project(sars_i[1:(delta*4), ], R = sample_R(sars_R4, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj5 <- project(sars_i[1:(delta*5), ], R = sample_R(sars_R5, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 3), R_fix_within = TRUE)
sars_proj6 <- project(sars_i[1:(delta*6), ], R = sample_R(sars_R6, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 2), R_fix_within = TRUE)
sars_proj7 <- project(sars_i[1:(delta*7), ], R = sample_R(sars_R7, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)
@

\begin{figure}[h]
<<empirical_plot, eval = TRUE, echo = FALSE, fig.width = 8, fig.height = 3, fig.align = "center">>=
library(incidence)
library(EpiEstim)
library(outbreaks)
library(ggplot2)

set.seed(1)
# Ebola incidence
# load("proj_window_1.RData")
# ebola_projection_1 <- proj_window
ebola_plot <- plot(ebola_i[1:(8*12), ]) 
# %>% add_projections(ebola_proj1, quantiles = FALSE) %>% add_projections(ebola_proj2, quantiles = FALSE) %>% add_projections(ebola_proj3, quantiles = FALSE) %>% add_projections(ebola_proj4, quantiles = FALSE) %>% add_projections(ebola_proj5, quantiles = FALSE) %>% add_projections(ebola_proj6, quantiles = FALSE) %>% add_projections(ebola_proj7, quantiles = FALSE)

# SARS incidence
sars_plot <- plot(sars_i[1:(8*9), ]) 
# %>% add_projections(sars_proj1, quantiles = FALSE) %>% add_projections(sars_proj1, quantiles = FALSE) %>% add_projections(sars_proj2, quantiles = FALSE) %>% add_projections(sars_proj3, quantiles = FALSE) %>% add_projections(sars_proj4, quantiles = FALSE) %>% add_projections(sars_proj5, quantiles = FALSE) %>% add_projections(sars_proj6, quantiles = FALSE) %>% add_projections(sars_proj7, quantiles = FALSE)

# Influenza incidence
influenza_plot <- plot(flu_i[1:(8*3), ]) 
# %>% add_projections(flu_proj1, quantiles = FALSE) %>% add_projections(flu_proj2, quantiles = FALSE) %>% add_projections(flu_proj3, quantiles = FALSE) %>% add_projections(flu_proj4, quantiles = FALSE) %>% add_projections(flu_proj5, quantiles = FALSE) %>% add_projections(flu_proj6, quantiles = FALSE) %>% add_projections(flu_proj7, quantiles = FALSE)

# Combine plots into one plot
multiplot(ebola_plot, influenza_plot, sars_plot, cols = 3)
@
\caption{The early outbreak daily incidence curves for Ebola (left), H1N1 influenza (middle), and Severe Acute Respiratory Syndrome (SARS) (right) \citep{Leone2014, Cori2018, Campbell2018}.}
\label{empirical_plot}
\end{figure}

\FloatBarrier
\newpage
\subsection{Full prediction metrics}

\subsubsection{Average residual}
\begin{figure}[h]
<<whole_real_residual, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8>>=
facet1_names <- list(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- list(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

facet_labeller <- function(variable,value){
  if (variable == "facet1") {
    return(facet1_names[value])
  } else if (variable == "facet2") {
    return(facet2_names[value])
  } else {
    return(as.character(value))
  }
}

residual_massive <- ggplot(real_total_table, aes(cali_window_size, y = residual, color = dataset)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = facet_labeller) +
                    labs(y = "Residual", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
residual_massive
@
\caption{The average residual for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_residual}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{MSE}
\begin{figure}[h]
<<whole_real_mse, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8>>=
facet1_names <- list(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- list(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

facet_labeller <- function(variable,value){
  if (variable == "facet1") {
    return(facet1_names[value])
  } else if (variable == "facet2") {
    return(facet2_names[value])
  } else {
    return(as.character(value))
  }
}

mse_massive <- ggplot(real_total_table, aes(cali_window_size, y = mse, color = dataset)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = facet_labeller) +
                    labs(y = "MSE", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
mse_massive
@
\caption{The mean-square error for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_mse}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Sharpness}
\begin{figure}[h]
<<whole_real_sharpness, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8>>=
facet1_names <- list(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- list(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

facet_labeller <- function(variable,value){
  if (variable == "facet1") {
    return(facet1_names[value])
  } else if (variable == "facet2") {
    return(facet2_names[value])
  } else {
    return(as.character(value))
  }
}

sharpness_massive <- ggplot(real_total_table, aes(cali_window_size, y = sharpness, color = dataset)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = facet_labeller) +
                    labs(y = "Sharpness", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(0, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
sharpness_massive
@
\caption{Sharpness for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_sharpness}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Bias}
\begin{figure}[h]
<<whole_real_bias, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 8>>=
facet1_names <- list(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- list(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

facet_labeller <- function(variable,value){
  if (variable == "facet1") {
    return(facet1_names[value])
  } else if (variable == "facet2") {
    return(facet2_names[value])
  } else {
    return(as.character(value))
  }
}

bias_massive <- ggplot(real_total_table, aes(cali_window_size, y = bias, color = dataset)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = facet_labeller) +
                    labs(y = "Bias", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(-1, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
bias_massive
@
\caption{Bias for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_bias}
\end{figure}

\FloatBarrier
\newpage
\section{Linear regression}
\subsubsection{Simulated outbreaks: model comparison}
<<sim_metric_aic_table, echo = FALSE, results = "asis">>= 
library(xtable)

aic_table <- array(NA, dim =c(16, 4))

# disease names
aic_table[1, 1] <- "Residual"
aic_table[5, 1] <- "MSE"
aic_table[9, 1] <- "Sharpness"
aic_table[13, 1] <- "Bias"

# model names
aic_table[c(1, 5, 9, 13), 2] <- "Basic"
aic_table[c(2, 6, 10, 14), 2] <- "Basic + ratio"
aic_table[c(3, 7, 11, 15), 2] <- "Basic + interaction"
aic_table[c(4, 8, 12, 16), 2] <- "Basic + ratio + interaction"

# degrees of freedom
aic_table[1:4, 3] <- residual_aic$df[1:4]
aic_table[5:8, 3] <- mse_aic$df[1:4]
aic_table[9:12, 3] <- sharpness_aic$df[1:4]
aic_table[13:16, 3] <- bias_aic$df[1:4]

# AIC
aic_table[1:4, 4] <- round(residual_aic$AIC[1:4], digits = 2)
aic_table[5:8, 4] <- round(mse_aic$AIC[1:4], digits = 2)
aic_table[9:12, 4] <- round(sharpness_aic$AIC[1:4], digits = 2)
aic_table[13:16, 4] <- round(bias_aic$AIC[1:4], digits = 2)

colnames(aic_table) <- c("Metric", "Model", "df", "AIC")

tab <- xtable(aic_table, digits = 2, caption = "Comparisons of linear models for explaining variation in the average residuals, mean-square error (MSE), sharpness, and bias of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion.", label = "sim_metric_aic_table")
align(tab) <- "lXX{2cm}XX"
print(tab, hline.after=c(-1, 0, 16), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "bottom", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@

% \subsubsection{Empirical outbreaks: model comparison}

\FloatBarrier





























%%%%%%%%%%%%%%%%%%%%%%%%
%% Notes and outtakes %%
%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
<<data_sharpness_table, echo = FALSE, results = "asis">>=
library(xtable)

real_sharpness_table <- real_total_table %>% group_by(disease, pred_type) %>% summarise(avg = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75))

data_table <- array(NA, dim =c(5, 5))

# Disease
data_table[1, 1] <- "Ebola"
data_table[2, 1] <- "Influenza"
data_table[4, 1] <- "SARS"

# Prediction type
data_table[c(1, 2, 4), 2] <- "No 0"
data_table[c(3, 5), 2] <- "Include 0"

# Median
data_table[ ,3] <- round(real_sharpness_table$avg, 2)
# Lower quantile
data_table[, 4] <- round(real_sharpness_table$q1, 2)
# Upper quantile
data_table[, 5] <- round(real_sharpness_table$q3, 2)

colnames(data_table) <- c("Disease", "Prediction type", "Median", "Lower quartile", "Upper quartile")

tab <- xtable(data_table, digits = 2, caption = "Sharpness for different prediction type groups for real Ebola, influenza, and SARS outbreaks", label = "real_sharpness_table")
align(tab) <- "lXXXXX"
print(tab, hline.after=c(-1, 0, 5), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@
\FloatBarrier

<<data_bias_table, echo = FALSE, results = "asis">>=
library(xtable)

real_bias_table <- real_total_table %>% group_by(disease, pred_type) %>% summarise(avg = median(bias), q1 = quantile(bias, probs = 0.25), q3 = quantile(bias, probs = 0.75))

data_table <- array(NA, dim =c(5, 5))

# Disease
data_table[1, 1] <- "Ebola"
data_table[2, 1] <- "Influenza"
data_table[4, 1] <- "SARS"

# Prediction type
data_table[c(1, 2, 4), 2] <- "No 0"
data_table[c(3, 5), 2] <- "Include 0"

# Median
data_table[ ,3] <- round(real_bias_table$avg, 2)
# Lower quantile
data_table[, 4] <- round(real_bias_table$q1, 2)
# Upper quantile
data_table[, 5] <- round(real_bias_table$q3, 2)

colnames(data_table) <- c("Disease", "Prediction type", "Median", "Lower quantile", "Upper quantile")

tab <- xtable(data_table, digits = 2, caption = "Bias for different prediction type groups for real Ebola, influenza, and SARS outbreaks for varying types of prediction - no zero-incidence (no 0) or including zero-incidence (Include 0)", label = "real_bias_table")
align(tab) <- "lXXXXX"
print(tab, hline.after=c(-1, 0, 5), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@
\FloatBarrier

\subsubsection{Linear regression}

<<case_plots, echo = FALSE, eval = FALSE>>=
# Residual by the number of cases in calibration window
total_table$calno_proj_ratio <- total_table$no_cali_cases / total_table$proj_window_no
total_table$no_cali_bin <- findInterval(total_table$no_cali_cases, c(seq(1, 5, 1), seq(6, 14, 2), seq(16, 36, 10), 46))

residual_table <- total_table %>% group_by(no_cali_bin) %>% summarise(avg = median(residual), std = sd(residual), q1 = quantile(residual, probs=0.25), q3 = quantile(residual, probs = 0.75))

residual_ratio <- ggplot(residual_table, aes(factor(no_cali_bin),
                          y = avg, ymin = q1, ymax = q3)) +
                          # coord_cartesian(ylim = c(-5, 5)) +
                          geom_hline(yintercept = 0, linetype = "dashed") +
                          # facet_wrap(~disease, scales = "free") +
                          geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                          labs(y = "Average residual", x = "Number of cases in calibration window") +
                          theme(legend.position = "none")

sharpness_table <- total_table %>% group_by(no_cali_bin) %>% summarise(avg = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs=0.25), q3 = quantile(sharpness, probs = 0.75))

sharpness_cases <- ggplot(sharpness_table, aes(factor(no_cali_bin),
                          y = avg, ymin = q1, ymax = q3)) +
                          # coord_cartesian(ylim = c(-5, 5)) +
                          geom_hline(yintercept = 0, linetype = "dashed") +
                          # facet_wrap(~disease, scales = "free") +
                          geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                          labs(y = "Sharpness", x = "Number of cases in calibration window") +
                          theme(legend.position = "none")

sharpness_cases2 <- ggplot(total_table, aes(factor(no_cali_bin),
                     sharpness)) + # , fill = factor(disease)
                     coord_cartesian(ylim = c(0, 1)) +
                     # facet_wrap(~disease) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = "darkseagreen4") +
                     geom_boxplot(position = position_dodge(width = 1), width = 0.2, fill = "white", color = "black") +
                     labs(y = "Sharpness", x = "Number of cases in calibration window") +
                     scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6-7", "8-9", "10-11", "12-13", "14-15", "16-25", "26-35", "36-45", "46<")) +
                     theme(legend.position = "none")

no_cases_pred_type <- ggplot(total_table, aes(factor(pred_type),
                     no_cali_cases, fill = factor(disease))) + # , fill = factor(disease)
                     # coord_cartesian(ylim = c(0, 50)) +
                     facet_wrap(cali_window_size~proj_window_no, scale = "free") +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     geom_boxplot(position = position_dodge(width = 1), width = 0.2, fill = "white", color = "black") +
                     labs(y = "Number of cases in calibration window", x = "Prediction type") +
                     scale_x_discrete(labels = c("only 0", "no 0", "includes 0")) +
                     theme(legend.position = "none")

@

% Make 3-panel plots for each metric and disease showing how the metric varies with a combination of calibration window and projection window number.
% Show that the model's performance can also be skewed by prediction type (only non-incidence, some non-incidence, and all projections with normal incidence).
% Show results of regression for each metric.
\FloatBarrier
% \subsubsection{Average residual}
\begin{figure}[h]
<<residual_plot_total, echo = FALSE, fig.width = 8, fig.height = 3>>=
library(ggplot2)
text_size <- 5

residual_table <- total_table %>% group_by(disease, cali_proj_ratio) %>% summarise(avg = median(residual), std = sd(residual), q1 = quantile(residual, probs=0.25), q3 = quantile(residual, probs = 0.75))
real_residual_table <- real_total_table %>% group_by(disease, cali_proj_ratio) %>% summarise(avg = median(residual), std = sd(residual), q1 = quantile(residual, probs=0.25), q3 = quantile(residual, probs = 0.75))
                          
residual_ratio <- ggplot(residual_table, aes(factor(cali_proj_ratio),
                          y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
                          # coord_cartesian(ylim = c(-5, 5)) +
                          geom_hline(yintercept = 0, linetype = "dashed") +
                          facet_wrap(~disease, scales = "free") +
                          geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                          # geom_point(aes(factor(cali_proj_ratio), y = avg, color = factor(disease)), data = real_residual_table, shape = 2) +
                          labs(y = "Average residual", x = "Ratio of calibration window size to projection window number") +
                          theme(legend.position = "none")

residual_multi_table <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(avg = median(residual), std = sd(residual), q1 = quantile(residual, probs = 0.25), q3 = quantile(residual, probs = 0.75), null_avg = median(null_residual), null_std = sd(null_residual), null_q1 = quantile(null_residual, probs = 0.25), null_q3 = quantile(null_residual, probs = 0.75))

residual_multi <- ggplot(residual_multi_table, aes(factor(proj_window_no),
                     y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
                     # coord_cartesian(ylim = c(0, 10)) +
                     geom_hline(yintercept = 0, linetype = "dashed") +
                     facet_wrap(disease~cali_window_size, scales = "free") +
                     # geom_pointrange(aes(y = null_avg, ymin = null_q1, ymax = null_q3), size = 0.3, color = "black") +
                     geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                     labs(y = "Mean residual", x = "Projection window number") +
                     theme(legend.position = "none")

# Plot that I print
residual_ratio
@
\caption{The average residuals for the predicted incidence of simulated and real outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza by the ratio of calibration window size to the projection window number (how many time windows since the end of observed data). The dots refer to the median residual for simulated outbreaks for a given ratio, while the intervals mark the upper and lower end of the interquartile range. The triangles refer to the median residual calculated for the real disease outbreaks.}
\label{residual_plot}
\end{figure}

<<data_residual_table, echo = FALSE, eval = FALSE, results = "asis">>=
library(xtable)

data_table <- array(NA, dim =c(16, 4))

# Calibration window to projection window ratio
unique_ratios <- unique(real_total_table$cal_proj_ratio)
data_table[1:16, 1] <- unique_ratios

# Values
# Ebola
data_table[, 2] <- real_ebola$residual
# SARS
data_table[, 3] <- real_sars$residual
# Influenza
data_table[, 4] <- real_influenza$residual

colnames(data_table) <- c("Calibration window", "Ebola", "Influenza", "SARS")

tab <- xtable(data_table, digits = 2, caption = "Average residuals by time window for real Ebola, influenza, and SARS outbreaks", label = "residual_aic_table")
align(tab) <- "lXX{2cm}XX"
print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@

The average residual was used to measure the deviation of the predicted daily incidence from the true daily incidence. An average residual of 0 would imply a perfect prediction, while a negative residual implies an overestimation and a positive residual implies an underestimation of predicted daily incidence. The simulated Ebola and influenza outbreaks' interquartile ranges reach the perfect average residual of zero when the ratio of calibration window to projection window size is 1. The SARS outbreak simulations seemed to be less vulnerable to long projections with a short calibration window than Ebola and influenza simulations (Fig. \ref{residual_plot}).

The daily incidence was grossly overestimated at the lowest ratio of calibration window size to projection window number for both the simulated and real Ebola outbreaks, though the predictive model's overestimation of daily incidence reduced as the ratio of calibration window size to projection window number increased (Fig. \ref{residual_plot}). For the simulated influenza outbreaks, the predictive model's overestimation of daily incidence also reduced as the ratio increased, though its incidence estimations were more overestimated for the real influenza epidemic data (Fig. \ref{residual_plot}). Unlike for Ebola and influenza, the daily incidence of the true SARS outbreak was consistently overestimated for all ratios without as clear of a trend in the direction of the average residual with ratio size (Fig. \ref{residual_plot}).

For the simulated Ebola outbreaks, the best quality linear regression model was one which took into account the calibration window size, number of cases in the calibration window, the projection window number, the ratio of calibration window size to projection window number, and the interaction between the calibration and projection window (SI Table \ref{residual_aic_table}). The best-fit model for the simulated SARS and influenza outbreaks was similar to that of Ebola, with the addition of prediction group, whereas for the simulated influenza outbreaks the best-fit linear regression model included all of the explanatory variables and interaction but did not take into account the ratio of the calibration window size to projection window number (SI Table \ref{residual_aic_table}).

% Within the best-fit linear regression model for Ebola, the greatest slope with the strongest evidence was seen as a decrease in the residual with an increasing projection window number, accounting for the other explanatory variables (coefficient -21.77, p-value 8.45e-83, SI Table \ref{residual_lm_table}). This association between the residual decreasing with increasing projection window number accounting for other explanatory variables held true for the influenza simulations as well (coefficient -2.53, p-value 2.18e-100, SI Table \ref{residual_lm_table}). This implies that for these two diseases, as the projection window number increases, the residual decreases, which is likely due to an overestimation of the daily incidence for both the Ebola and influenza simulations with increasing projection window number (Fig. \ref{residual_plot}). The regression analysis of the SARS simulations showed the opposite relationship. The residual increased with increasing projection window size, though the evidence for the association was not as strong (coefficient 0.07, p-value 0.0125, SI Table \ref{residual_lm_table}). The predicted incidences for the simulated SARS outbreaks were not overestimated by as much as for Ebola and influenza (Fig. \ref{residual_plot}).

\textcolor{red}{I am not sure about how best to discuss the effect of interaction between calibration window and projection window on the response variables. Should I make interaction plots to show what happens to the metrics with different combinations or would that be deviating from the focus too much?}

% The explanatory variable with the strongest evidence for affecting the slope of change in the residual for the simulated SARS outbreaks' daily incidence predictions after accounting for the other explanatory variables was the number of cases observed in the calibration window (coefficient -0.44, p-value 2.63e-54, SI Table \ref{residual_lm_table}). The same negative relationship between the number of cases in the calibration window and the residual was observed for the Ebola and influenza simulations' predictions (coefficients -3.69 and -1.37 respectively, p-values 1.59e-10 and 4.18e-50 respectively, SI Table \ref{residual_lm_table}). 
% This would imply that controlling for the effect of other explanatory variables, the average residual decreases with increasing numbers of cases in the calibration window. 

\textcolor{red}{I could run the models for more calibration windows etc. but currently Ebola for example is observed for up to 12 * 8 = 96 days (flu 24 days, SARS 72 days) - though I'm not sure how early outbreak it would be if one would extend further. I've also had a look at a 2-month observation period for each disease - see figures in the "Notes and outtakes"-section}
\FloatBarrier

% \subsubsection{Mean-square error}
\FloatBarrier
\begin{figure}[h]
<<rmse_plot_total, echo = FALSE, fig.width = 8, fig.height = 3>>=
library(ggplot2)
text_size <- 5

# RMSE plot with calibration window to projection window ratio
mse_table <- total_table %>% group_by(disease, cali_proj_ratio) %>% summarise(avg = median(mse), std = sd(mse), q1 = quantile(mse, probs = 0.25), q3 = quantile(mse, probs = 0.75), null_avg = median(null_mse), null_std = sd(null_mse), null_q1 = quantile(null_mse, probs = 0.25), null_q3 = quantile(null_mse, probs = 0.75))
# real_mse_table <- real_total_table %>% group_by(disease, cali_proj_ratio) %>% summarise(avg = median(mse), null_avg = median(null_mse), q1 = quantile(mse, probs = 0.25), q3 = quantile(mse, probs = 0.75), null_avg = median(null_mse), null_std = sd(null_mse), null_q1 = quantile(null_mse, probs = 0.25), null_q3 = quantile(null_mse, probs = 0.75))                     
     
rmse_ratio <- ggplot(mse_table, aes(factor(cali_proj_ratio),
                     y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
                     # coord_cartesian(ylim = c(0, 10)) +
                     scale_y_log10() +
                     facet_wrap(~disease) + #, scales = "free") +
                     # geom_pointrange(aes(y = null_avg, ymin = null_q1, ymax = null_q3), size = 0.3, color = "gray30") +
                     geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                     # geom_point(aes(factor(cali_proj_ratio), y = avg, color = factor(disease)), data = real_mse_table, shape = 2) +
                     # geom_point(aes(factor(cali_proj_ratio), y = null_avg), data = real_mse_table, color = "gray30", shape = 2) +
                     labs(y = "MSE", x = "Ratio of calibration window size to projection window number") +
                     theme(legend.position = "none")

# RMSE plot with everything separately
# rmse_multi_table <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(avg = median(rmse), std = sd(rmse), q1 = quantile(rmse, probs = 0.25), q3 = quantile(rmse, probs = 0.75), null_avg = median(null_rmse), null_std = sd(null_rmse), null_q1 = quantile(null_rmse, probs = 0.25), null_q3 = quantile(null_rmse, probs = 0.75))
# rmse_multi <- ggplot(rmse_table, aes(factor(proj_window_no),
#                      y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
#                      # coord_cartesian(ylim = c(0, 10)) +
#                      scale_y_log10() +
#                      facet_wrap(disease~cali_window_size) + #, scales = "free") +
#                      geom_pointrange(aes(y = null_avg, ymin = null_q1, ymax = null_q3), size = 0.3, color = "black") +
#                      geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
#                      labs(y = "RMSE", x = "Ratio of calibration window size to projection window number") +
#                      theme(legend.position = "none")

# Table that I print
rmse_ratio
@
\caption{Root-mean-square errors (RMSE) of predicted daily incidence for simulated and real outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza for a branching process model (coloured) and a null model (black) for different ratios of calibration window size to projection window number. The points represent the median RMSE of the simulated outbreaks for a given ratio, while the vertical lines represent the interquartile ranges. The coloured triangles represent the RMSE of the real outbreaks, while the black triangles represent the null model's RMSE for the real outbreak data.}
\label{rmse_plot}
\end{figure}
\FloatBarrier

The root-mean-square error (RMSE) was also used to quantify the deviation of the models' predicted daily incidence from the true daily incidence, with an emphasis on the outliers. For the Ebola-like simulations' predicted incidence, the null model outperformed the branching process model until a ratio of calibration window size to projection window number of 1 was reached. After this, the interquartile ranges of the branching process model began overlapping with those of the null model, and finally at ratios 6 and 7 the median RMSE for the branching process model was the same or less than that of the null model, though the interquartile ranges still overlap (Fig. \ref{rmse_plot}). The median RMSE for the branching process model was always higher than that of the null model for the influenza and SARS simulations' predicted incidences, though the median RMSEs of the two models did move closer to one another with increasing calibration window size to projection window number ratio (Fig. \ref{rmse_plot}).

As for the average residuals, the RMSE suggests that the branching process model's performance at forecasting the daily incidence of real outbreaks was not as good as its performance when forecasting simulated outbreaks (Fig. \ref{rmse_plot}). The differences between the RMSEs of the branching process model and null model tend to be higher for the true data (note that the plot's y-axis is on a logarithmic scale)(Fig. \ref{rmse_plot}).

The best-fit linear regression model describing the relationship between RMSE and the explanatory variables was one which included all the explanatory variables, the ratio of calibration window size to projection window number, and the interaction between the two windows for the SARS and influenza simulations, while the Ebola regression models excluded prediction group type from the best-fit model (SI Table \ref{rmse_aic_table}, \ref{rmse_lm_table}). As for the average residual, the association with the strongest evidence after accounting for the other explanatory variables was that between RMSE and projection window number for Ebola and influenza (coefficients 47.79 and 5.54 respectively, p-value 3.62e-133 and 8.77e-187 respectively, SI Table \ref{rmse_lm_table}). The relationship was also positive for SARS (coefficient 0.14, p-value 5.33e-11). This implies that as the projection window number increases, so does the RMSE. This is plausible as a low RMSE is an indicator of reduced deviation from the true incidence and the further a projection window is from the calibration window, the higher the chances are of the predicted values not being near the true values.

\textcolor{red}{I feel that I often start discussing the regression analysis from nowhere. Would it be better to discuss all the regression analysis for all response variables in one go after showing all the plots?}

\FloatBarrier

% \subsubsection{Sharpness}

\begin{figure}[h]
<<sharpness_plot_total_out, echo = FALSE, fig.width = 7, fig.height = 3>>=
library(ggplot2)
text_size <- 5

# RMSE plot with calibration window to projection window ratio
sharpness_table <- total_table %>% group_by(disease, pred_type) %>% summarise(avg = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75))
                          
sharpness_type <- ggplot(total_table, aes(factor(pred_type),
                     sharpness, color = factor(disease), fill = factor(disease))) +
                     coord_cartesian(ylim = c(0, 1)) +
                     # scale_y_log10() +
                     facet_wrap(~disease) +
                     # geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     geom_boxplot(position = position_dodge(width = 1), width = 0.1, fill = "white", color = "black") +
                     labs(y = "Sharpness", x = "Prediction group type") +
                     theme(legend.position = "none") +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))

# Table that I print
sharpness_type
@
\caption{Sharpness by prediction type for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza. The different prediction types include ones which have trajectories predicting only zero incidence ("only 0"), no trajectories predicting zero incidence ("no 0"), or some trajectories predicting zero incidence ("include 0").}
\label{sharpness_type_plot_out}
\end{figure}

<<sharpness_calculations_out, echo = FALSE, eval = TRUE, results = "hide">>=
# Proportion of prediction type by disease

# Ebola
ebola_1 <- sum(ebola_table$pred_type == 1) / nrow(ebola_table)
ebola_2 <- (sum(ebola_table$pred_type == 2) / nrow(ebola_table)) * 100

# SARS
sars_1 <- sum(sars_table$pred_type == 1) / nrow(sars_table)
sars_2 <- (sum(sars_table$pred_type == 2) / nrow(sars_table)) * 100

# Influenza
influenza_1 <- sum(influenza_table$pred_type == 1) / nrow(influenza_table)
influenza_2 <- (sum(influenza_table$pred_type == 2) / nrow(influenza_table)) * 100

@

% \FloatBarrier
% 
% The sharpness, described as a measure of how narrow the range of predictions for a given time window are from a scale of 0 (wide) to 1 (narrow), varied by the types of predictions that were given within a single projection of a simulation (Fi. \ref{sharpness_type_plot}). There are three possible types of projections: ones where all of the projection's 10,000 projection trajectories include a prediction of a daily incidence of 0 cases, ones where some of the trajectories include a prediction of daily incidence of 0 cases, and ones where none of the trajectories include a prediction of a daily incidence of 0 cases.
% 
% For projections of simulations that contained an estimate of a daily incidence of 0, the sharpness varied more wildly than for projections, also dipping to lower sharpnesses than the projections that did not contain a daily incidence of 0, reflecting the possibility of a projection to spiral out of control and have a wide range of predictions for a given projection window (Fig. \ref{sharpness_type_plot}). Additionally, the distribution of sharpness scores within the zero-incidence-including groups was split into two for each simulated disease - either sharpness was very higher than that of the group that did not include any zero-incidence, or sharpness was lower than the majority of the sharpness distribution for the prediction type group that did not include zero-incidence (Fig. \ref{sharpness_type_plot}). For the Ebola simulations' forecasts, \Sexpr{round(ebola_2, 1)}\% contained zero-incidence, while \Sexpr{round(influenza_2, 1)}\% and \Sexpr{round(sars_2, 1)}\% of influenza and SARS simulations' forecasts contained the prediction of zero-incidence, respectively. The high proportion of forecasts containing zero-incidence suggests that for the calibration windows used in this study, the branching process model could often not decipher whether the outbreak would undergo stochastic extinction or not.
% 
% The median sharpness of the forecasted incidence based on calibration windows obtained from the real Ebola outbreak was higher than that of the Ebola simulations (Table \ref{real_sharpness_table}, Fig. \ref{sharpness_type_plot}). For influenza, there was only one time window that included a forecast of zero-incidence, and the predictions for time windows that did not contain zero-incidence were slightly sharper than their simulated counterparts (Table \ref{real_sharpness_table}, Fig. \ref{sharpness_type_plot}). The forecasts for the real SARS outbreak, on the other hand, only included one projection window with no zero-incidence and the rest did include the possibility of zero-incidence, the medians and and interquartile ranges of which did not drastically differ from those of the simulations. 
% 
% For sharpness, the best-fit linear regression model for the Ebola and influenza simulations' predictions was one that included all the explanatory variables and the interaction between calibration window size and projection window number (SI Table \ref{sharpness_aic_table}). For SARS, the best-fit linear regression model additionally included the ratio of calibration window size to projection window number (SI Table \ref{sharpness_aic_table}).  
% 
% \textcolor{red}{I could talk about how zero-incidence reduces with increasing size of calibration windows here with a table showing the percentages in each group by calibration window size or something if that would be helpful/not completely off-topic.}
% 
% \FloatBarrier
% \subsubsection{Bias}
% 
% \begin{figure}[h]
<<bias_plot_total, eval = FALSE, echo = FALSE, fig.width = 7, fig.height = 3, message = FALSE>>=
library(ggplot2)
library(tidyr)
text_size <- 5

# Bias plot split by prediction type
bias_table <- total_table %>% dplyr::select(disease, pred_type, bias)
bias_table$bias_type <- "branching"
null_bias_table <- total_table %>% dplyr::select(disease, pred_type, null_bias)
null_bias_table$bias_type <- "null"
names(null_bias_table)[names(null_bias_table) == "null_bias"] <- "bias"
total_bias_table <- bind_rows(bias_table, null_bias_table)

bias_type <- ggplot(total_bias_table, aes(factor(pred_type),
                     bias, group = interaction(pred_type, bias_type), color = factor(bias_type), fill = factor(bias_type))) +
                     coord_cartesian(ylim = c(-1, 1)) +
                     # scale_y_log10() +
                     facet_wrap(~disease) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     # geom_violin(aes(factor(pred_type), null_bias), trim = TRUE, scale = "width", position = position_dodge(width = 2), color = "grey", fill = "grey") +
                     geom_boxplot(width = 0.2, fill = "white", color = "black", position = position_dodge(width = 1)) +
                     # geom_boxplot(aes(factor(pred_type), null_bias), position = position_dodge(width = 2), width = 0.2, fill = "white", color = "black") +
                     labs(y = "Bias", x = "Prediction group type") +
                     theme(legend.position = "none") +
                     scale_fill_manual(values = c("chartreuse3", "grey")) +
                     scale_color_manual(values = c("chartreuse3", "grey")) +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))

bias_fig <- ggplot(total_table, aes(factor(pred_type),
                     bias, color = factor(disease), fill = factor(disease))) +
                     coord_cartesian(ylim = c(-1, 1)) +
                     # scale_y_log10() +
                     facet_wrap(~disease) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     # geom_violin(aes(factor(pred_type), null_bias), trim = TRUE, scale = "width", position = position_dodge(width = 2), color = "grey", fill = "grey") +
                     geom_boxplot(width = 0.2, fill = "white", color = "black", position = position_dodge(width = 1)) +
                     # geom_boxplot(aes(factor(pred_type), null_bias), position = position_dodge(width = 2), width = 0.2, fill = "white", color = "black") +
                     labs(y = "Bias", x = "Prediction group type") +
                     theme(legend.position = "none") +
                     # scale_fill_manual(values = c("chartreuse3", "grey")) +
                     # scale_color_manual(values = c("chartreuse3", "grey")) +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))
# Table that I print
bias_fig
@
% \caption{Bias by prediction type for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza for the branching process model (green) and null model (grey). The different prediction types include ones which have trajectories predicting only zero incidence ("only 0"), no trajectories predicting zero incidence ("no 0"), or some trajectories predicting zero incidence ("include 0").}
% \label{bias_type_plot}
% \end{figure}
% 
% The bias of the predicted incidence of the branching process model, whether the prediction was above or below the true incidence regardless of the magnitude of the difference, was measured against the performance of the null model. For the forecasts of simulated outbreaks, the null model often underestimated the daily incidence for prediction window types where the branching process model did not predict zero-incidence at all (Fig. \ref{bias_type_plot}). For prediction windows where the branchinsg process model predicted zero-incidence for some trajectories, the extent of the null model's bias was more varying. For Ebola, the distribution of biases of the null model ranged the whole range from -1 to 1 for prediction windows that the branching process model has included the possibility of zero-incidence for. The projections pertaining to the real Ebola and influenza outbreaks tended to overestimate daily incidence, while the SARS outbreak's projections tended to underestimate daily incidence (Table \ref{real_bias_table}).  
% 
% \textcolor{red}{I put my regression and model comparison tables into the Supplementary Information section for now because I felt that otherwise I would have had too many tables in my Results and they take up a lot of space. Do you think any of my tables are so vital that they should be in the Results section rather than the Supplementary Information? Maybe I should try to condense the analyses into a summary table..? Anything that goes in the SI will not be taken into account when marking so everything vital needs to be in Results.}
% 
% \textcolor{red}{Real papers are not as heavily subsectioned as the Results section currently is. Should I get rid of subsections once I'm done editing my report?}

% <<residual_lm, echo = FALSE, results = "hide", message = FALSE>>=
% library(MASS)
% 
% # Ebola
% ebola_lm_start <- lm(residual ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = ebola_table)
% 
% ebola_lm_basic <- stepAIC(ebola_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% ebola_lm_ratio <- lm(residual ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    # + pred_type
%                    + cali_proj_ratio,
%                      data = ebola_table)
% 
% # Basic model + interaction between calibration window and projection window
% ebola_lm_inter <- lm(residual ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    # + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = ebola_table)
% 
% ebola_lm_inter_ratio <- lm(residual ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       # + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = ebola_table)
% 
% ebola_residual_aic <- AIC(ebola_lm_basic, ebola_lm_ratio, ebola_lm_inter, ebola_lm_inter_ratio)
% 
% # SARS
% sars_lm_start <- lm(residual ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = sars_table)
% 
% sars_lm_basic <- stepAIC(sars_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% sars_lm_ratio <- lm(residual ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + cali_proj_ratio,
%                      data = sars_table)
% 
% # Basic model + interaction between calibration window and projection window
% sars_lm_inter <- lm(residual ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = sars_table)
% 
% sars_lm_inter_ratio <- lm(residual ~ cali_window_size
%                         + log(no_cali_cases)
%                         + proj_window_no
%                         + pred_type
%                         + cali_proj_ratio
%                         + proj_window_no * cali_window_size,
%                           data = sars_table)
% 
% sars_residual_aic <- AIC(sars_lm_basic, sars_lm_ratio, sars_lm_inter, sars_lm_inter_ratio)
% 
% # Influenza
% influenza_lm_start <- lm(residual ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type,
%                          data = influenza_table)
% 
% influenza_lm_basic <- stepAIC(influenza_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% influenza_lm_ratio <- lm(residual ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio,
%                         data = influenza_table)
% 
% # Basic model + interaction between calibration window and projection window
% influenza_lm_inter <- lm(residual ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type
%                        + proj_window_no * cali_window_size,
%                          data = influenza_table)
% 
% influenza_lm_inter_ratio <- lm(residual ~ cali_window_size
%                              + log(no_cali_cases)
%                              + proj_window_no
%                              + pred_type
%                              + cali_proj_ratio
%                              + proj_window_no * cali_window_size,
%                                data = influenza_table)
% 
% influenza_residual_aic <- AIC(influenza_lm_basic, influenza_lm_ratio, influenza_lm_inter, influenza_lm_inter_ratio)
% 
% @
% 
% <<residual_aic_table, echo = FALSE, results = "asis">>= 
% library(xtable)
% 
% aic_table <- array(NA, dim =c(12, 4))
% 
% # disease names
% aic_table[1, 1] <- "Ebola"
% aic_table[5, 1] <- "SARS"
% aic_table[9, 1] <- "Influenza"
% 
% # model names
% aic_table[c(1, 5, 9), 2] <- "Basic"
% aic_table[c(2, 6, 10), 2] <- "Basic + ratio"
% aic_table[c(3, 7, 11), 2] <- "Basic + interaction"
% aic_table[c(4, 8, 12), 2] <- "Basic + ratio + interaction"
% 
% # degrees of freedom
% aic_table[1:4, 3] <- ebola_residual_aic$df[1:4]
% aic_table[5:8, 3] <- sars_residual_aic$df[1:4]
% aic_table[9:12, 3] <- influenza_residual_aic$df[1:4]
% 
% # AIC
% aic_table[1:4, 4] <- round(ebola_residual_aic$AIC[1:4], digits = 2)
% aic_table[5:8, 4] <- round(sars_residual_aic$AIC[1:4], digits = 2)
% aic_table[9:12, 4] <- round(influenza_residual_aic$AIC[1:4], digits = 2)
% 
% colnames(aic_table) <- c("Disease", "Model", "df", "AIC")
% 
% tab <- xtable(aic_table, digits = 2, caption = "Comparisons of linear models for explaining variation in the mean residuals of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion. \\textcolor{red}{I still need to convince R to keep the trailing zeroes and figure out how footnotes work on xtable.}", label = "residual_aic_table")
% align(tab) <- "lXX{2cm}XX"
% print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% <<residual_lm_table, echo = FALSE, results = "asis">>=
% library(xtable)
% 
% # Best-performing models
% ebola_summary <- summary(ebola_lm_inter_ratio)
% sars_summary <- summary(sars_lm_inter_ratio)
% influenza_summary <- summary(influenza_lm_inter)
% 
% lm_table <- array(NA, dim =c(19, 6))
% 
% # disease names
% lm_table[1, 1] <- "Ebola"
% lm_table[7, 1] <- "SARS"
% lm_table[14, 1] <- "Influenza"
% 
% # Coefficient names
% lm_table[c(1, 7, 14), 2] <- "Intercept"
% lm_table[c(2, 8, 15), 2] <- "Cal. size"# "Calibration window size"
% lm_table[c(3, 9, 16), 2] <- "No. cases" # "No. cases in calibration window"
% lm_table[c(4, 10, 17), 2] <- "Proj. window" # "Projection window no."
% lm_table[c(11, 18), 2] <- "Pred. group"# "Prediction group type"
% lm_table[c(5, 12), 2] <- "Ratio" # "Ratio of calibration window size to projection window number"
% lm_table[c(6, 13, 19), 2] <- "Interaction" # "Interaction between calibration and projection window"
% 
% # Estimate
% lm_table[1:6, 3] <- round(ebola_summary$coefficients[, 1], 2)
% lm_table[7:13, 3] <- round(sars_summary$coefficients[, 1], 2)
% lm_table[14:19, 3] <- round(influenza_summary$coefficients[, 1], 2)
% 
% # 95% CI estimate +- 1.96*SE
% # Lower bound
% lm_table[1:6, 4] <- round(ebola_summary$coefficients[, 1] - 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 4] <- round(sars_summary$coefficients[, 1] - 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:19, 4] <- round(influenza_summary$coefficients[, 1] - 1.96 * influenza_summary$coefficients[, 2], 2)
% # Upper bound
% lm_table[1:6, 5] <- round(ebola_summary$coefficients[, 1] + 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 5] <- round(sars_summary$coefficients[, 1] + 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:19, 5] <- round(influenza_summary$coefficients[, 1] + 1.96 * influenza_summary$coefficients[, 2], 2)
% 
% # p-value
% lm_table[1:6, 6] <- signif(ebola_summary$coefficients[, 4], 3)
% lm_table[7:13, 6] <- signif(sars_summary$coefficients[, 4], 3)
% lm_table[14:19, 6] <- signif(influenza_summary$coefficients[, 4], 3)
% 
% colnames(lm_table) <- c("Disease", "Coefficient", "Estimate", "95% CI lower", "95% CI upper", "p-value")
% 
% tab <- xtable(lm_table, caption = "The coefficients of the best-performing linear models for explaining variation in the mean residuals of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion.", label = "residual_lm_table")
% digits(tab) <- c(2, 2, 2, 2, 2, 2, -10)
% align(tab) <- "lXXXXXX"
% print(tab, hline.after=c(-1, 0, 6, 13, 19), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% \FloatBarrier
% \subsubsection{RMSE}
% <<rmse_lm, echo = FALSE, results = "hide">>=
% library(MASS)
% 
% # Ebola
% ebola_lm_start <- lm(rmse ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = ebola_table)
% 
% ebola_lm_basic <- stepAIC(ebola_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% ebola_lm_ratio <- lm(rmse ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    # + pred_type
%                    + cali_proj_ratio,
%                      data = ebola_table)
% 
% # Basic model + interaction between calibration window and projection window
% ebola_lm_inter <- lm(rmse ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    # + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = ebola_table)
% 
% ebola_lm_inter_ratio <- lm(rmse ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       # + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = ebola_table)
% 
% ebola_rmse_aic <- AIC(ebola_lm_basic, ebola_lm_ratio, ebola_lm_inter, ebola_lm_inter_ratio)
% 
% # SARS
% sars_lm_start <- lm(rmse ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = sars_table)
% 
% sars_lm_basic <- stepAIC(sars_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% sars_lm_ratio <- lm(rmse ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + cali_proj_ratio,
%                      data = sars_table)
% 
% # Basic model + interaction between calibration window and projection window
% sars_lm_inter <- lm(rmse ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = sars_table)
% 
% sars_lm_inter_ratio <- lm(rmse ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = sars_table)
% 
% sars_rmse_aic <- AIC(sars_lm_basic, sars_lm_ratio, sars_lm_inter, sars_lm_inter_ratio)
% 
% # Influenza
% influenza_lm_start <- lm(rmse ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type,
%                          data = influenza_table)
% 
% influenza_lm_basic <- stepAIC(influenza_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% influenza_lm_ratio <- lm(rmse ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio,
%                         data = influenza_table)
% 
% # Basic model + interaction between calibration window and projection window
% influenza_lm_inter <- lm(rmse ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type
%                        + proj_window_no * cali_window_size,
%                          data = influenza_table)
% 
% influenza_lm_inter_ratio <- lm(rmse ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = influenza_table)
% 
% influenza_rmse_aic <- AIC(influenza_lm_basic, influenza_lm_ratio, influenza_lm_inter, influenza_lm_inter_ratio)
% 
% @
% 
% <<rmse_aic_table, echo = FALSE, results = "asis">>= 
% library(xtable)
% 
% aic_table <- array(NA, dim =c(12, 4))
% 
% # disease names
% aic_table[1, 1] <- "Ebola"
% aic_table[5, 1] <- "SARS"
% aic_table[9, 1] <- "Influenza"
% 
% # model names
% aic_table[c(1, 5, 9), 2] <- "Basic"
% aic_table[c(2, 6, 10), 2] <- "Basic + ratio"
% aic_table[c(3, 7, 11), 2] <- "Basic + interaction"
% aic_table[c(4, 8, 12), 2] <- "Basic + ratio + interaction"
% 
% # degrees of freedom
% aic_table[1:4, 3] <- ebola_rmse_aic$df[1:4]
% aic_table[5:8, 3] <- sars_rmse_aic$df[1:4]
% aic_table[9:12, 3] <- influenza_rmse_aic$df[1:4]
% 
% # AIC
% aic_table[1:4, 4] <- round(ebola_rmse_aic$AIC[1:4], digits = 2)
% aic_table[5:8, 4] <- round(sars_rmse_aic$AIC[1:4], digits = 2)
% aic_table[9:12, 4] <- round(influenza_rmse_aic$AIC[1:4], digits = 2)
% 
% colnames(aic_table) <- c("Disease", "Model", "df", "AIC")
% 
% tab <- xtable(aic_table, digits = 2, caption = "Comparisons of linear models for explaining variation in the root-mean-squared error (RMSE) of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion", label = "rmse_aic_table")
% align(tab) <- "lXX{2cm}XX"
% print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% <<rmse_lm_table, echo = FALSE, results = "asis">>=
% library(xtable)
% 
% # Best-performing models
% ebola_summary <- summary(ebola_lm_inter_ratio)
% sars_summary <- summary(sars_lm_inter_ratio)
% influenza_summary <- summary(influenza_lm_inter_ratio)
% 
% lm_table <- array(NA, dim =c(20, 6))
% 
% # disease names
% lm_table[1, 1] <- "Ebola"
% lm_table[7, 1] <- "SARS"
% lm_table[14, 1] <- "Influenza"
% 
% # Coefficient names
% lm_table[c(1, 7, 14), 2] <- "Intercept"
% lm_table[c(2, 8, 15), 2] <- "Cal. size"# "Calibration window size"
% lm_table[c(3, 9, 16), 2] <- "No. cases" # "No. cases in calibration window"
% lm_table[c(4, 10, 17), 2] <- "Proj. window" # "Projection window no."
% lm_table[c(11, 18), 2] <- "Pred. group"# "Prediction group type"
% lm_table[c(5, 12, 19), 2] <- "Ratio" # "Ratio of calibration window size to projection window number"
% lm_table[c(6, 13, 20), 2] <- "Interaction" # "Interaction between calibration and projection window"
% 
% # Estimate
% lm_table[1:6, 3] <- round(ebola_summary$coefficients[, 1], 2)
% lm_table[7:13, 3] <- round(sars_summary$coefficients[, 1], 2)
% lm_table[14:20, 3] <- round(influenza_summary$coefficients[, 1], 2)
% 
% # 95% CI estimate +- 1.96*SE
% # Lower bound
% lm_table[1:6, 4] <- round(ebola_summary$coefficients[, 1] - 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 4] <- round(sars_summary$coefficients[, 1] - 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:20, 4] <- round(influenza_summary$coefficients[, 1] - 1.96 * influenza_summary$coefficients[, 2], 2)
% # Upper bound
% lm_table[1:6, 5] <- round(ebola_summary$coefficients[, 1] + 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 5] <- round(sars_summary$coefficients[, 1] + 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:20, 5] <- round(influenza_summary$coefficients[, 1] + 1.96 * influenza_summary$coefficients[, 2], 2)
% 
% # p-value
% lm_table[1:6, 6] <- signif(ebola_summary$coefficients[, 4], 3)
% lm_table[7:13, 6] <- signif(sars_summary$coefficients[, 4], 3)
% lm_table[14:20, 6] <- signif(influenza_summary$coefficients[, 4], 3)
% 
% colnames(lm_table) <- c("Disease", "Coefficient", "Estimate", "95% CI lower", "95% CI upper", "p-value")
% 
% tab <- xtable(lm_table, digits = c(2, 2, 2, 2, 2, 2, -2), caption = "The coefficients of the best-performing linear models for explaining variation in the root-mean-squared error (RMSE) of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion", label = "rmse_lm_table")
% align(tab) <- "lX{2cm}XXXXX"
% print(tab, hline.after=c(-1, 0, 6, 13, 20), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% \FloatBarrier
% \subsubsection{Sharpness}
% <<sharpness_lm, echo = FALSE, results = "hide">>=
% library(MASS)
% 
% # Ebola
% ebola_lm_start <- lm(sharpness ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = ebola_table)
% 
% ebola_lm_basic <- stepAIC(ebola_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% ebola_lm_ratio <- lm(sharpness ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + cali_proj_ratio,
%                      data = ebola_table)
% 
% # Basic model + interaction between calibration window and projection window
% ebola_lm_inter <- lm(sharpness ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = ebola_table)
% 
% ebola_lm_inter_ratio <- lm(sharpness ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = ebola_table)
% 
% ebola_sharpness_aic <- AIC(ebola_lm_basic, ebola_lm_ratio, ebola_lm_inter, ebola_lm_inter_ratio)
% 
% # SARS
% sars_lm_start <- lm(sharpness ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = sars_table)
% 
% sars_lm_basic <- stepAIC(sars_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% sars_lm_ratio <- lm(sharpness ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + cali_proj_ratio,
%                      data = sars_table)
% 
% # Basic model + interaction between calibration window and projection window
% sars_lm_inter <- lm(sharpness ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = sars_table)
% 
% sars_lm_inter_ratio <- lm(sharpness ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = sars_table)
% 
% sars_sharpness_aic <- AIC(sars_lm_basic, sars_lm_ratio, sars_lm_inter, sars_lm_inter_ratio)
% 
% # Influenza
% influenza_lm_start <- lm(sharpness ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type,
%                          data = influenza_table)
% 
% influenza_lm_basic <- stepAIC(influenza_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% influenza_lm_ratio <- lm(sharpness ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio,
%                         data = influenza_table)
% 
% # Basic model + interaction between calibration window and projection window
% influenza_lm_inter <- lm(sharpness ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type
%                        + proj_window_no * cali_window_size,
%                          data = influenza_table)
% 
% influenza_lm_inter_ratio <- lm(sharpness ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = influenza_table)
% 
% influenza_sharpness_aic <- AIC(influenza_lm_basic, influenza_lm_ratio, influenza_lm_inter, influenza_lm_inter_ratio)
% 
% @
% 
% <<sharpness_aic_table, echo = FALSE, results = "asis">>= 
% library(xtable)
% 
% aic_table <- array(NA, dim =c(12, 4))
% 
% # disease names
% aic_table[1, 1] <- "Ebola"
% aic_table[5, 1] <- "SARS"
% aic_table[9, 1] <- "Influenza"
% 
% # model names
% aic_table[c(1, 5, 9), 2] <- "Basic"
% aic_table[c(2, 6, 10), 2] <- "Basic + ratio"
% aic_table[c(3, 7, 11), 2] <- "Basic + interaction"
% aic_table[c(4, 8, 12), 2] <- "Basic + ratio + interaction"
% 
% # degrees of freedom
% aic_table[1:4, 3] <- ebola_sharpness_aic$df[1:4]
% aic_table[5:8, 3] <- sars_sharpness_aic$df[1:4]
% aic_table[9:12, 3] <- influenza_sharpness_aic$df[1:4]
% 
% # AIC
% aic_table[1:4, 4] <- round(ebola_sharpness_aic$AIC[1:4], digits = 2)
% aic_table[5:8, 4] <- round(sars_sharpness_aic$AIC[1:4], digits = 2)
% aic_table[9:12, 4] <- round(influenza_sharpness_aic$AIC[1:4], digits = 2)
% 
% colnames(aic_table) <- c("Disease", "Model", "df", "AIC")
% 
% tab <- xtable(aic_table, digits = 2, caption = "Comparisons of linear models for explaining variation in the sharpness of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion", label = "sharpness_aic_table")
% align(tab) <- "lXX{2cm}XX"
% print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% <<sharpness_lm_table, echo = FALSE, results = "asis">>=
% library(xtable)
% 
% # Best-performing models
% ebola_summary <- summary(ebola_lm_inter)
% sars_summary <- summary(sars_lm_inter_ratio)
% influenza_summary <- summary(influenza_lm_inter)
% 
% lm_table <- array(NA, dim =c(19, 6))
% 
% # disease names
% lm_table[1, 1] <- "Ebola"
% lm_table[7, 1] <- "SARS"
% lm_table[14, 1] <- "Influenza"
% 
% # Coefficient names
% lm_table[c(1, 7, 14), 2] <- "Intercept"
% lm_table[c(2, 8, 15), 2] <- "Cal. size"# "Calibration window size"
% lm_table[c(3, 9, 16), 2] <- "No. cases" # "No. cases in calibration window"
% lm_table[c(4, 10, 17), 2] <- "Proj. window" # "Projection window no."
% lm_table[c(5, 11, 18), 2] <- "Pred. group"# "Prediction group type"
% lm_table[c(12), 2] <- "Ratio" # "Ratio of calibration window size to projection window number"
% lm_table[c(6, 13, 19), 2] <- "Interaction" # "Interaction between calibration and projection window"
% 
% # Estimate
% lm_table[1:6, 3] <- round(ebola_summary$coefficients[, 1], 2)
% lm_table[7:13, 3] <- round(sars_summary$coefficients[, 1], 2)
% lm_table[14:19, 3] <- round(influenza_summary$coefficients[, 1], 2)
% 
% # 95% CI estimate +- 1.96*SE
% # Lower bound
% lm_table[1:6, 4] <- round(ebola_summary$coefficients[, 1] - 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 4] <- round(sars_summary$coefficients[, 1] - 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:19, 4] <- round(influenza_summary$coefficients[, 1] - 1.96 * influenza_summary$coefficients[, 2], 2)
% # Upper bound
% lm_table[1:6, 5] <- round(ebola_summary$coefficients[, 1] + 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 5] <- round(sars_summary$coefficients[, 1] + 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:19, 5] <- round(influenza_summary$coefficients[, 1] + 1.96 * influenza_summary$coefficients[, 2], 2)
% 
% # p-value
% lm_table[1:6, 6] <- signif(ebola_summary$coefficients[, 4], 3)
% lm_table[7:13, 6] <- signif(sars_summary$coefficients[, 4], 3)
% lm_table[14:19, 6] <- signif(influenza_summary$coefficients[, 4], 3)
% 
% colnames(lm_table) <- c("Disease", "Coefficient", "Estimate", "95% CI lower", "95% CI upper", "p-value")
% 
% tab <- xtable(lm_table, digits = c(2, 2, 2, 2, 2, 2, -2), caption = "The coefficients of the best-performing linear models for explaining variation in the sharpness of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion", label = "sharpness_lm_table")
% align(tab) <- "lX{2cm}XXXXX"
% print(tab, hline.after=c(-1, 0, 6, 13, 19), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% \FloatBarrier
% \subsubsection{Bias}
% <<bias_lm, echo = FALSE, results = "hide">>=
% library(MASS)
% 
% # Ebola
% ebola_lm_start <- lm(bias ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = ebola_table)
% 
% ebola_lm_basic <- stepAIC(ebola_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% ebola_lm_ratio <- lm(bias ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + cali_proj_ratio,
%                      data = ebola_table)
% 
% # Basic model + interaction between calibration window and projection window
% ebola_lm_inter <- lm(bias ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = ebola_table)
% 
% ebola_lm_inter_ratio <- lm(bias ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = ebola_table)
% 
% ebola_bias_aic <- AIC(ebola_lm_basic, ebola_lm_ratio, ebola_lm_inter, ebola_lm_inter_ratio)
% 
% # SARS
% sars_lm_start <- lm(bias ~ cali_window_size
%                   + log(no_cali_cases)
%                   + proj_window_no
%                   + pred_type,
%                     data = sars_table)
% 
% sars_lm_basic <- stepAIC(sars_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% sars_lm_ratio <- lm(bias ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + cali_proj_ratio,
%                      data = sars_table)
% 
% # Basic model + interaction between calibration window and projection window
% sars_lm_inter <- lm(bias ~ cali_window_size
%                    + log(no_cali_cases)
%                    + proj_window_no
%                    + pred_type
%                    + proj_window_no * cali_window_size,
%                      data = sars_table)
% 
% sars_lm_inter_ratio <- lm(bias ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = sars_table)
% 
% sars_bias_aic <- AIC(sars_lm_basic, sars_lm_ratio, sars_lm_inter, sars_lm_inter_ratio)
% 
% # Influenza
% influenza_lm_start <- lm(bias ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type,
%                          data = influenza_table)
% 
% influenza_lm_basic <- stepAIC(influenza_lm_start)
% 
% # Basic model + ratio of calibration window to projection window
% influenza_lm_ratio <- lm(bias ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio,
%                         data = influenza_table)
% 
% # Basic model + interaction between calibration window and projection window
% influenza_lm_inter <- lm(bias ~ cali_window_size
%                        + log(no_cali_cases)
%                        + proj_window_no
%                        + pred_type
%                        + proj_window_no * cali_window_size,
%                          data = influenza_table)
% 
% influenza_lm_inter_ratio <- lm(bias ~ cali_window_size
%                       + log(no_cali_cases)
%                       + proj_window_no
%                       + pred_type
%                       + cali_proj_ratio
%                       + proj_window_no * cali_window_size,
%                         data = influenza_table)
% 
% influenza_bias_aic <- AIC(influenza_lm_basic, influenza_lm_ratio, influenza_lm_inter, influenza_lm_inter_ratio)
% 
% @
% 
% <<bias_aic_table, echo = FALSE, results = "asis">>= 
% library(xtable)
% 
% aic_table <- array(NA, dim =c(12, 4))
% 
% # disease names
% aic_table[1, 1] <- "Ebola"
% aic_table[5, 1] <- "SARS"
% aic_table[9, 1] <- "Influenza"
% 
% # model names
% aic_table[c(1, 5, 9), 2] <- "Basic"
% aic_table[c(2, 6, 10), 2] <- "Basic + ratio"
% aic_table[c(3, 7, 11), 2] <- "Basic + interaction"
% aic_table[c(4, 8, 12), 2] <- "Basic + ratio + interaction"
% 
% # degrees of freedom
% aic_table[1:4, 3] <- ebola_bias_aic$df[1:4]
% aic_table[5:8, 3] <- sars_bias_aic$df[1:4]
% aic_table[9:12, 3] <- influenza_bias_aic$df[1:4]
% 
% # AIC
% aic_table[1:4, 4] <- round(ebola_bias_aic$AIC[1:4], digits = 2)
% aic_table[5:8, 4] <- round(sars_bias_aic$AIC[1:4], digits = 2)
% aic_table[9:12, 4] <- round(influenza_bias_aic$AIC[1:4], digits = 2)
% 
% colnames(aic_table) <- c("Disease", "Model", "df", "AIC")
% 
% tab <- xtable(aic_table, digits = 2, caption = "Comparisons of linear models for explaining variation in the bias of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion", label = "bias_aic_table")
% align(tab) <- "lXX{2cm}XX"
% print(tab, hline.after=c(-1, 0, 12), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
% @
% 
% <<bias_lm_table, echo = FALSE, results = "asis">>=
% library(xtable)
% 
% # Best-performing models
% ebola_summary <- summary(ebola_lm_inter)
% sars_summary <- summary(sars_lm_inter_ratio)
% influenza_summary <- summary(influenza_lm_inter_ratio)
% 
% lm_table <- array(NA, dim =c(20, 6))
% 
% # disease names
% lm_table[1, 1] <- "Ebola"
% lm_table[7, 1] <- "SARS"
% lm_table[14, 1] <- "Influenza"
% 
% # Coefficient names
% lm_table[c(1, 7, 14), 2] <- "Intercept"
% lm_table[c(2, 8, 15), 2] <- "Cal. size"# "Calibration window size"
% lm_table[c(3, 9, 16), 2] <- "No. cases" # "No. cases in calibration window"
% lm_table[c(4, 10, 17), 2] <- "Proj. window" # "Projection window no."
% lm_table[c(5, 11, 18), 2] <- "Pred. group"# "Prediction group type"
% lm_table[c(12, 19), 2] <- "Ratio" # "Ratio of calibration window size to projection window number"
% lm_table[c(6, 13, 20), 2] <- "Interaction" # "Interaction between calibration and projection window"
% 
% # Estimate
% lm_table[1:6, 3] <- round(ebola_summary$coefficients[, 1], 2)
% lm_table[7:13, 3] <- round(sars_summary$coefficients[, 1], 2)
% lm_table[14:20, 3] <- round(influenza_summary$coefficients[, 1], 2)
% 
% # 95% CI estimate +- 1.96*SE
% # Lower bound
% lm_table[1:6, 4] <- round(ebola_summary$coefficients[, 1] - 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 4] <- round(sars_summary$coefficients[, 1] - 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:20, 4] <- round(influenza_summary$coefficients[, 1] - 1.96 * influenza_summary$coefficients[, 2], 2)
% # Upper bound
% lm_table[1:6, 5] <- round(ebola_summary$coefficients[, 1] + 1.96 * ebola_summary$coefficients[, 2], 2)
% lm_table[7:13, 5] <- round(sars_summary$coefficients[, 1] + 1.96 * sars_summary$coefficients[, 2], 2)
% lm_table[14:20, 5] <- round(influenza_summary$coefficients[, 1] + 1.96 * influenza_summary$coefficients[, 2], 2)
% 
% # p-value
% lm_table[1:6, 6] <- signif(ebola_summary$coefficients[, 4], 3)
% lm_table[7:13, 6] <- signif(sars_summary$coefficients[, 4], 3)
% lm_table[14:20, 6] <- signif(influenza_summary$coefficients[, 4], 3)
% 
% colnames(lm_table) <- c("Disease", "Coefficient", "Estimate", "95% CI lower", "95% CI upper", "p-value")
% 
% tab <- xtable(lm_table, digits = c(2, 2, 2, 2, 2, 2, -2), caption = "The coefficients of the best-performing linear models for explaining variation in the bias of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion", label = "bias_lm_table")
% align(tab) <- "lX{2cm}XXXXX"
% print(tab, hline.after=c(-1, 0, 6, 13, 20), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@

% \newpage
% \section{Notes and outtakes}
% I've also been running some simulations with a time window of 1 week for an 8-week outbreak all-in-all. This means that for the figures below, I have run the outbreak for 2 months for each disease. I explored this because I figured that weekly projections of outbreaks would be closer to what would be done in real life and thought that it would be interesting to compare how the model's performance for difference diseases changes when you change the time window. The figures below are exactly the same ones as those in the real report, but the data is for a time window of 1 week rather than 1 serial interval. Also, I only ran these for 50 simulations instead of 80.
% 
% <<8week_data, echo = FALSE>>=
% library(data.table)
% library(dplyr)
% # Combo tables for the diseases
% # Ebola
% # list all files in directory named full_proj_metrics.csv
% output_files <- list.files("/home/evelina/Development/forecasting/simulations/ebola_8weeks_null/", pattern = "full_proj_metrics.csv", 
%                            full.names = TRUE, recursive = TRUE)
% # read and row bind all data sets
% ebola_table <- rbindlist(lapply(output_files, fread))
% ebola_table$cali_proj_ratio <- round(ebola_table$cali_window_size / ebola_table$proj_window_no, 1)
% ebola_table$pred_type[ebola_table$pred_type == 1] <- 0
% ebola_table$pred_type[ebola_table$pred_type == 3] <- 1
% # SARS
% # list all files in directory named full_proj_metrics.csv
% output_files <- list.files("/home/evelina/Development/forecasting/simulations/sars_8weeks_null/", pattern = "full_proj_metrics.csv", 
%                            full.names = TRUE, recursive = TRUE)
% # read and row bind all data sets
% sars_table <- rbindlist(lapply(output_files, fread))
% sars_table$cali_proj_ratio <- round(sars_table$cali_window_size / sars_table$proj_window_no, 1)
% sars_table$pred_type[sars_table$pred_type == 1] <- 0
% sars_table$pred_type[sars_table$pred_type == 3] <- 1
% # Influenza
% # list all files in directory named full_proj_metrics.csv
% output_files <- list.files("/home/evelina/Development/forecasting/simulations/influenza_8weeks_null/", pattern = "full_proj_metrics.csv", 
%                            full.names = TRUE, recursive = TRUE)
% # read and row bind all data sets
% influenza_table <- rbindlist(lapply(output_files, fread))
% influenza_table$cali_proj_ratio <- round(influenza_table$cali_window_size / influenza_table$proj_window_no, 1)
% influenza_table$pred_type[influenza_table$pred_type == 1] <- 0
% influenza_table$pred_type[influenza_table$pred_type == 3] <- 1
% 
% # Combine all diseases into one table
% total_table <- bind_rows(ebola_table, sars_table, influenza_table)
% @
% 
% \begin{figure}[h]
% <<residual_plot_8week, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 3>>=
% library(ggplot2)
% text_size <- 5
% 
% # Residual by the number of cases in calibration window
% total_table$calno_proj_ratio <- total_table$no_cali_cases / total_table$proj_window_no
% 
% residual_calratio <- ggplot(total_table, aes(calno_proj_ratio, residual, color = disease)) + 
%              # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
%              geom_point(size = 0.5, shape = 1) +
%              labs(y = "Residual", x = "Ratio of no. of cases in calibration window to projection window number") +
%              coord_cartesian(ylim = c(-25, 25))
%              # theme(legend.position = "none") +
% 
% residual_table <- total_table %>% group_by(disease, cali_proj_ratio) %>% summarise(avg = median(residual), std = sd(residual), q1 = quantile(residual, probs=0.25), q3 = quantile(residual, probs = 0.75))
%                           
% residual_ratio <- ggplot(residual_table, aes(factor(cali_proj_ratio),
%                           y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
%                           # coord_cartesian(ylim = c(-5, 5)) +
%                           geom_hline(yintercept = 0, linetype = "dashed") +
%                           facet_wrap(~disease, scales = "free") +
%                           geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
%                           labs(y = "Mean residual", x = "Ratio of calibration window size to projection window number") +
%                           theme(legend.position = "none")
% 
% # Plot that I print
% residual_ratio
% @
% \caption{The average residuals for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza by the ratio of calibration window size to the projection window number (how many time windows since the end of observed data). The dots refer to the median residual for a given ratio, while the intervals mark the upper and lower end of the interquartile range.}
% \label{residual_plot_8week}
% \end{figure}
% 
% Regarding the residual and RMSE, Ebola was looking the worst at low ratios, now it's influenza. I'm suspecting that it has to do with influenza's low mean serial interval. That being said, SARS is still doing well and better than Ebola. 
% 
% \begin{figure}[h]
% <<rmse_plot_8week, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 3>>=
% library(ggplot2)
% text_size <- 5
% 
% # RMSE plot with calibration window to projection window ratio
% rmse_table <- total_table %>% group_by(disease, cali_proj_ratio) %>% summarise(avg = median(rmse), std = sd(rmse), q1 = quantile(rmse, probs = 0.25), q3 = quantile(rmse, probs = 0.75), null_avg = median(null_rmse), null_std = sd(null_rmse), null_q1 = quantile(null_rmse, probs = 0.25), null_q3 = quantile(null_rmse, probs = 0.75))
%                           
% rmse_ratio <- ggplot(rmse_table, aes(factor(cali_proj_ratio),
%                      y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
%                      # coord_cartesian(ylim = c(0, 10)) +
%                      scale_y_log10() +
%                      facet_wrap(~disease) + #, scales = "free") +
%                      geom_pointrange(aes(y = null_avg, ymin = null_q1, ymax = null_q3), size = 0.3, color = "gray30") +
%                      geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
%                      labs(y = "RMSE", x = "Ratio of calibration window size to projection window number") +
%                      theme(legend.position = "none")
% 
% # RMSE plot with everything separately
% rmse_multi_table <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(avg = median(rmse), std = sd(rmse), q1 = quantile(rmse, probs = 0.25), q3 = quantile(rmse, probs = 0.75), null_avg = median(null_rmse), null_std = sd(null_rmse), null_q1 = quantile(null_rmse, probs = 0.25), null_q3 = quantile(null_rmse, probs = 0.75))
% rmse_multi <- ggplot(rmse_table, aes(factor(proj_window_no),
%                      y = avg, ymin = q1, ymax = q3, color = factor(disease))) +
%                      # coord_cartesian(ylim = c(0, 10)) +
%                      scale_y_log10() +
%                      facet_wrap(disease~cali_window_size) + #, scales = "free") +
%                      geom_pointrange(aes(y = null_avg, ymin = null_q1, ymax = null_q3), size = 0.3, color = "black") +
%                      geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
%                      labs(y = "RMSE", x = "Ratio of calibration window size to projection window number") +
%                      theme(legend.position = "none")
% 
% # Table that I print
% rmse_ratio
% @
% \caption{Root-mean-square errors (RMSE) of predicted daily incidence for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza for a branching process model (coloured) and a null model (black) for different ratios of calibration window size to projection window number. The points represent the median RMSE for a given ratio, while the vertical lines represent the interquartile ranges.}
% \label{rmse_plot_8week}
% \end{figure}
% 
% \begin{figure}[h]
% <<sharpness_plot_8week, echo = FALSE, cache = TRUE, fig.width = 7, fig.height = 3>>=
% library(ggplot2)
% text_size <- 5
% 
% # RMSE plot with calibration window to projection window ratio
% sharpness_table <- total_table %>% group_by(disease, pred_type) %>% summarise(avg = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75))
%                           
% sharpness_type <- ggplot(total_table, aes(factor(pred_type),
%                      sharpness, color = factor(disease), fill = factor(disease))) +
%                      coord_cartesian(ylim = c(0, 1)) +
%                      # scale_y_log10() +
%                      facet_wrap(~disease) +
%                      # geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
%                      geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
%                      geom_boxplot(position = position_dodge(width = 1), width = 0.1, fill = "white", color = "black") +
%                      labs(y = "Sharpness", x = "Prediction group type") +
%                      theme(legend.position = "none") +
%                      scale_x_discrete(labels = c("only 0", "no 0", "include 0"))
% 
% # Table that I print
% sharpness_type
% @
% \caption{Sharpness by prediction type for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza.}
% \label{sharpness_type_plot_8week}
% \end{figure}
% 
% \begin{figure}[h]
% <<bias_plot_8week, echo = FALSE, cache = TRUE, fig.width = 7, fig.height = 3>>=
% library(ggplot2)
% library(tidyr)
% text_size <- 5
% 
% # Bias plot split by prediction type
% bias_table <- total_table %>% dplyr::select(disease, pred_type, bias)
% bias_table$bias_type <- "branching"
% null_bias_table <- total_table %>% dplyr::select(disease, pred_type, null_bias)
% null_bias_table$bias_type <- "null"
% names(null_bias_table)[names(null_bias_table) == "null_bias"] <- "bias"
% total_bias_table <- bind_rows(bias_table, null_bias_table)
% 
% bias_type <- ggplot(total_bias_table, aes(factor(pred_type),
%                      bias, group = interaction(pred_type, bias_type), color = factor(bias_type), fill = factor(bias_type))) +
%                      coord_cartesian(ylim = c(-1, 1)) +
%                      # scale_y_log10() +
%                      facet_wrap(~disease) +
%                      geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
%                      # geom_violin(aes(factor(pred_type), null_bias), trim = TRUE, scale = "width", position = position_dodge(width = 2), color = "grey", fill = "grey") +
%                      geom_boxplot(width = 0.2, fill = "white", color = "black", position = position_dodge(width = 1)) +
%                      # geom_boxplot(aes(factor(pred_type), null_bias), position = position_dodge(width = 2), width = 0.2, fill = "white", color = "black") +
%                      labs(y = "Bias", x = "Prediction group type") +
%                      theme(legend.position = "none") +
%                      scale_fill_manual(values = c("chartreuse3", "grey")) +
%                      scale_color_manual(values = c("chartreuse3", "grey")) +
%                      scale_x_discrete(labels = c("only 0", "no 0", "include 0"))
% # Table that I print
% bias_type
% @
% \caption{Bias by prediction type for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza for the branching process model (green) and null model (grey).}
% \label{bias_type_plot_8week}
% \end{figure}

\end{document}