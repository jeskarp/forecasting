%%%%%%%%%%%%%%%%%%%%%
%% Document set-up %%
%%%%%%%%%%%%%%%%%%%%%

% Requirements:
  % Double-spaced
  % minimum 11pt font
  % Arial or Verdana font
  % 2 cm margins

\documentclass[a4paper, 12pt]{article} % sets document shape and font size

\usepackage[margin=2.0cm]{geometry} % set margins to 2cm
% \usepackage[document]{ragged2e} % make text left-aligned

\usepackage{setspace, caption}
\captionsetup{font=doublespacing} %double-spaced float captions
\doublespacing %double-spaced document
\setlength{\parindent}{2em} % 5 space indent

% change font to Arial
\renewcommand{\rmdefault}{phv} % Arial
\renewcommand{\sfdefault}{phv} % Arial

\renewcommand*\contentsname{} % removes Table of Contents' title

\usepackage{amsmath} % Needed for maths equations
\usepackage{graphicx}
\graphicspath{ {/home/evelina/Development/forecasting/figs/} } % Where the images will be found 

\usepackage[numbers]{natbib}

\usepackage{multirow} % for combining rows in tables

\usepackage{float} % for forcing figure placement

\usepackage{fontspec}

\usepackage[section]{placeins}

\usepackage{tabularx} % make table page-wide

% \usepackage{caption} % makes Figure and Table captions 11pt
% \DeclareCaptionFont{mysize}{\fontsize{11}}
% \captionsetup{font=mysize}

\usepackage{etoc} % to make Supplementary Info its own ToC

%%%%%%%%%%%%%%%%%%%%%%%
%% Start of document %%
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \SweaveOpts{concordance=TRUE}
\setmainfont[Ligatures=TeX]{Verdana}

%%%%%%%%%%%%%%%%%%%%%%%%
%% Functions and data %%
%%%%%%%%%%%%%%%%%%%%%%%%

<<functions_data, echo = FALSE, results = "hide", message = FALSE>>=
# Function for making multi-panel plots
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

library(data.table)
library(dplyr)
# Combo tables for the diseases
# Ebola
# list all files in directory named full_proj_metrics.csv
output_files <- list.files("/home/evelina/Development/forecasting/simulations/ebola_8si_norm_new/", pattern = "full_proj_metrics.csv", 
                           full.names = TRUE, recursive = TRUE)
# read and row bind all data sets
ebola_table <- rbindlist(lapply(output_files, fread))
ebola_table$cali_proj_ratio <- round(ebola_table$cali_window_size / ebola_table$proj_window_no, 1)
ebola_table$pred_type[ebola_table$pred_type == 1] <- 0
ebola_table$pred_type[ebola_table$pred_type == 3] <- 1
# SARS
# list all files in directory named full_proj_metrics.csv
output_files <- list.files("/home/evelina/Development/forecasting/simulations/sars_8si_norm_new/", pattern = "full_proj_metrics.csv", 
                           full.names = TRUE, recursive = TRUE)
# read and row bind all data sets
sars_table <- rbindlist(lapply(output_files, fread))
sars_table$cali_proj_ratio <- round(sars_table$cali_window_size / sars_table$proj_window_no, 1)
sars_table$pred_type[sars_table$pred_type == 1] <- 0
sars_table$pred_type[sars_table$pred_type == 3] <- 1
# Influenza
# list all files in directory named full_proj_metrics.csv
output_files <- list.files("/home/evelina/Development/forecasting/simulations/influenza_8si_norm_new/", pattern = "full_proj_metrics.csv", 
                           full.names = TRUE, recursive = TRUE)
# read and row bind all data sets
influenza_table <- rbindlist(lapply(output_files, fread))
influenza_table$cali_proj_ratio <- round(influenza_table$cali_window_size / influenza_table$proj_window_no, 1)
influenza_table$pred_type[influenza_table$pred_type == 1] <- 0
influenza_table$pred_type[influenza_table$pred_type == 3] <- 1

# Combine all diseases into one table
total_table <- bind_rows(ebola_table, sars_table, influenza_table)
@

<<real_outbreaks, echo = FALSE, results = "hide", message = FALSE, warning = FALSE>>=
real_ebola <- read.csv("/home/evelina/Development/forecasting/simulations/real_outbreaks/ebola/full_proj_metrics.csv")
# real_ebola$cali_window_size <- real_ebola$cali_window_size / real_ebola$proj_window_size
real_ebola$cali_proj_ratio <- round(real_ebola$cali_window_size / real_ebola$proj_window_no, 1)
real_ebola$pred_type[real_ebola$pred_type == 1] <- 0
real_ebola$pred_type[real_ebola$pred_type == 3] <- 1
real_sars <- read.csv("/home/evelina/Development/forecasting/simulations/real_outbreaks/sars/full_proj_metrics.csv") 
# real_sars$cali_window_size <- real_sars$cali_window_size / real_sars$proj_window_size
real_sars$cali_proj_ratio <- round(real_sars$cali_window_size / real_sars$proj_window_no, 1)
real_sars$pred_type[real_sars$pred_type == 1] <- 0
real_sars$pred_type[real_sars$pred_type == 3] <- 1
real_influenza <- read.csv("/home/evelina/Development/forecasting/simulations/real_outbreaks/influenza/full_proj_metrics.csv")
# real_influenza$cali_window_size <- real_influenza$cali_window_size / real_influenza$proj_window_size
real_influenza$cali_proj_ratio <- round(real_influenza$cali_window_size / real_influenza$proj_window_no, 1)
real_influenza$pred_type[real_influenza$pred_type == 1] <- 0
real_influenza$pred_type[real_influenza$pred_type == 3] <- 1
  
real_total_table <- bind_rows(real_ebola, real_sars, real_influenza)
@

<<global_options, echo = FALSE>>=
knitr::opts_chunk$set(fig.pos = 'H')
@

<<points_for_tibo, echo = FALSE, results = "hide", message = FALSE, warning = FALSE>>=
# Average number of cases in each calibration window by disease
library(dplyr)
case_table <- total_table %>% group_by(disease, cali_window_size) %>% summarise(mean_cases = mean(no_cali_cases), std_cases = sd(no_cali_cases))
@

%%%%%%%%%%%
%% Title %%
%%%%%%%%%%%

\begin{titlepage}
    \begin{center}
        \vspace*{2.5cm}
        
        \textbf{Evaluating incidence forecasting for informing outbreak response}
        
        \vspace{0.5cm}
        Project 2
        
        MRes Biomedical Research 
        
        Epidemiology, Evolution, and Control of Infectious Diseases Stream 
        
        \vspace{0.5cm}
        
        \textbf{Janetta E. Skarp}
        
        \vspace{2.5cm}
        
        \includegraphics[width=0.4\textwidth]{ICL_crest}
        % \includegraphics{ICL_crest.png}
        
        \vspace{2.5cm}
        
        Supervisors: Thibaut Jombart, Anne Cori\\ 
        Submitted: August 2018\\
        Department of Surgery and Cancer, Imperial College London
        
    \end{center}
    
\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Statement of Originality %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Statement of Originality}

I certify that this thesis, and the research to which it refers, are the product of my own work, conducted during the current year of the MRes in Biomedical Research at Imperial College London. Any ideas or quotations from the work of other people, published or otherwise, or from my own previous work are fully acknowledged in accordance with the standard referencing practices of the discipline.

The methods of forecasting disease incidence that I evaluated in this work were developed by members of the R Epidemics Consortium \citep{RECON2018}. The empirical outbreak linelists were openly accessible from Open Data Sierra Leone (Ebola), and the \textit{EpiEstim} (SARS) and \textit{outbreaks} (influenza) R packages.

\section*{Acknowledgements}
I would like to thank Thibaut Jombart and Anne Cori for providing me with advice throughout the course of my project and giving me feedback on my thesis. 

% \textcolor{red}{The second paragraph needs to essentially state what I did not do myself but rather built up on: "The statement should clearly and specifically acknowledge any work of other researchers which you have used, and clearly state which parts of the research were performed by you."}
% 
% 
% \textcolor{red}{
% Things I need to take into account:
% \begin{itemize}
%   \item I used packages developed by other people to make my projections
%   \item The empirical outbreak data came from openly accessible sources
%   \item I did the metrics
% \end{itemize}
% }

%%%%%%%%%%%%%%%%%%%%%%%
%% Table of contents %%
%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
% \section*{Table of Contents}
% \textcolor{red}{We don't have to have a Table of Contents. This will be deleted when done to save pages}
% % \addcontentsline{toc}{section}{Table of Contents}
% \vspace{-4em}
% % \tableofcontents
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}
% \tableofcontents

%%%%%%%%%%%%%%
%% Abstract %%
%%%%%%%%%%%%%%
\newpage
\section*{Abstract}

Branching process models are often used for forecasting disease incidence in real-time to aid outbreak response. In this work, I set out to evaluate the performance of such models for early outbreak analysis purposes. A branching process model was used to predict disease incidence at different stages of the outbreak for three empirical outbreaks of Ebola, influenza, and SARS in addition to 80 simulated outbreaks based on the $R_{0}$ and serial interval distribution of each of the three outbreaks. The performance of the projected daily incidence was explored through four metrics: the average residual, mean-square relative error, sharpness, and bias. My work suggested that in general the branching process model tends to overestimate the forecasted daily incidence, though forecasts do improve as tje amount of observed data increases and the length of the forecast decreases. When forecasting daily incidence for simulated outbreaks, the model gave a reasonable forecast of up to four mean serial intervals into the future, given a calibration window size of at least two mean serial intervals. For the empirical outbreaks investigated here, it is recommended to not forecast as far with a calibration window of equal size due to the possibility of greatly overestimating daily incidence. 

% \textcolor{red}{end with a general guideline sentence and make less dry} 

%Predictive branching process model works best on SARS. Average residual and MSE get better with increasing proportion of calibration window size to projection window number. Sharpness and bias vary by prediction type. It is not a good idea to project too far from the calibration window especially if you have a short calibration window.

%%%%%%%%%%%%%%%%%%
%% Introduction %%
%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\renewcommand{\thesection}{\arabic{section}}
\newpage
\section{Introduction}

%%%%%%%%%%

% \textcolor{red}{Thibaut's suggested structure:
% 1) transmissibility at the core of epi-modelling 2) used for informing response 3) models 4) limited evaluation of performances 5) aim of my project.}
% \textcolor{red}{Subsections will be removed once done.}

% \subsection{Transmissibility at the core of epi-modelling}
% Infectious diseases have been estimated to be the cause of approximately XX deaths in 2016 \textcolor{red}{CITE}. While the top of the list is dominated by diseases that are in constant transmission such as malaria and HIV/AIDS, outbreaks of other diseases such as haemorrhagic fevers can be devastating if they spread. Often interventions can be implemented to control outbreak spread, but knowing how the outbreak is developing may be useful when deciding what intervention methods to use and figuring out if the outbreak will worsen. Real-time modelling can be used to make an educated guess as to whether the outbreak is dying out or if one may expect it to grow further.
% Global burden of disease study?

Infectious disease outbreaks can cause considerable damage to human populations. The devastating impacts of major events, such as the Black Death of the 1300s, the Spanish influenza of 1918, and the ongoing HIV/AIDS pandemic live on in our collective memories. Often interventions can be implemented to control outbreak spread, but knowing how the outbreak is developing may be useful when deciding what intervention methods to use. Modelling the outbreak's trajectory in real-time as it is unfolding can be used to make an educated guess as to whether it is dying out or if one may expect it to grow further.

When modelling infectious disease outbreaks, the disease's effective reproduction number, often denoted as $R$, is of particular interest. $R$ refers to the number of susceptible individuals that an infectious individual infects on average \citep{Vynnycky2010}. Due to this, $R$ can be used as an indicator for whether the disease's transmissibility is increasing or decreasing at a given timepoint \citep{Vynnycky2010}. Disease incidence, defined as the number of new cases for a given time period, is impacted by $R$. If $R$ is above 1, each infected individual infects on average more than one susceptible individual and thus the number of incident cases is increasing \citep{Vynnycky2010}. If $R$ is below 1, each infected individual infects fewer than one person on average, implying that the number of incident cases is on the decline \citep{Vynnycky2010}.

% \subsection{Epi-modelling informing response}

Having an estimation of the current $R$ and numbers of future incidence can be useful when informing outbreak response and scaling. Numerous past outbreaks have utilised forecasting to answer questions pertaining to the future of the outbreak. For instance, Middle-Eastern Respiratory Syndrome (MERS) incidence was predicted for a given time period of increased tourist activity in 2014 in the Kingdom of Saudi Arabia \citep{Lessler2014} and the peak of the pandemic H1N1 influenza outbreak of 2009 was modelled in England \citep{Baguelin2010}. Other interesting studies have aimed to predict where an outbreak might spread next, as was the case during the 2015-2016 yellow fever outbreak in Angola and the Democratic Republic of Congo \citep{Kraemer2017}, and whether existing healthcare infrastructure is sufficient for successfully handling an outbreak through forecasting incidence, which was done during the Ebola outbreak of May-July of 2018 in the Democratic Republic of Congo \citep{Barry2018}. One of the earliest cases of forecasting was applied by William Farr on the smallpox epidemic of 1837 to 1839 by assuming that the ratio between his estimates of quarterly reproduction numbers remained constant \citep{Neuberger2013, Santillana2018}. Since Farr's initial attempts, many methods have been developed for inferring $R$ from incidence and estimating the number of future disease cases.

For instance, $R$ can be estimated and used for forecasting incidence using a exponential growth rate model where a growth rate $r$ is fit to the cumulative incidence, which can then be used to estimate $R$ \citep{Chowell2007, Wallinga2007}. This relies on the assumption that the early growth of the epidemic is exponential \citep{Chowell2007}. A compartmental model, characterised by it having compartments for every disease category of interest and rates describing the movement in and out of the compartments, is another method that can be used to forecast daily incidence \citep{Anderson1991, Funk2018}. This is achieved by fitting the model to existing incidence data through estimating ranges of values that the model parameters can take by and then using the fitted model to obtain daily incidence forecasts \citep{Funk2018}.

Branching process models are another method for estimating transmissibility and forecasting incidence in real-time during infectious disease outbreaks \citep{Nouvellet2017}. A branching process can be used to stochastically model the number of offspring produced by a given generation of individuals in discrete time \textcolor{red}{REF}. This theory can be applied to inferring $R$ and forecasting disease incidence.

Given an observation window containing the daily incidence of symptom onset for a number of days and a serial interval distribution, a branching process model can be used to infer $R$ \citep{Cori2013}. This is achieved through considering the relative likelihood of a given case infecting any other case in the observation window i.e. how likely is it that another case was the offspring of that given case \citep{Wallinga2004, Cori2013}. These case-specific $R$s can then be used to obtain a $R$ likelihood distribution for the whole observation window \citep{Cori2013}. This in turn is used to forecast daily incidence, as $R$ states how many susceptibles a case infects on average \citep{Nouvellet2017}. Compared to the previously discussed modelling methods, branching process models have a few distinctive features. These include their ability to consider the full serial interval distribution when forecasting incidence \citep{Wallinga2004, Cori2013}. Additionally, unlike compartmental models, they typically assume an infinite pool of susceptibles. 

In addition to the models discussed here, other incidence forecasting methods exist, though not all methods perform equally well when forecasting incidence. In a forecasting challenge organised by the RAPIDD (Research and Policy for Infectious Disease Dynamics) program, different modelling groups were issued with the task of forecasting the incidence of simulated Ebola epidemics and nine different models were compared for the occasion \citep{Viboud2017}. A group utilising the branching process modelling method for its forecasts showed the most promising results, outperforming groups using methods such as compartmental models, logistic growth regression, and other agent-based models \citep{Viboud2017, Nouvellet2017}. However, branching process model forecasting tools' ability to accurately predict the course of an epidemic in real time has not been evaluated.

% \subsection{Aim}
In this study, I aim to decipher the conditions under which a branching process model can or cannot be reliably used to forecast daily incidence. To accomplish this, I have chosen metrics for evaluating the performance of an existing forecasting branching process model with a particular focus being placed on the methods' performance during the early stages of outbreaks \citep{Jombart2018}. I have then used the branching process model to forecast daily incidence on Ebola-like, influenza-like, and severe acute respiratory syndrome (SARS)-like simulated outbreaks. This was followed by an assessment of the performance of the model with the performance metrics. I also illustrate the application of this analysis on three empirical outbreaks. 

% Takeouts from the Introduction

% Parametric methods include approaches such as branching process and compartmental models, while the usage of epidemic trees represents a non-parametric approach \citep{Wallinga2004, Ferrari2005, Haydon2003}. These inferential methods would be unlikely to result in exactly identical estimated values of $R$ when given the same data. One study compared the $R$ estimates of multiple different models, an exponential growth rate model, two types of compartmental SEIR model, and a stochastic compartmental SIR model on the same Spanish flu outbreak data and found that while there were differences in the $R$ estimates, all of them fell within an acceptable range \citep{Chowell2007}. Altering one's assumptions while keeping the model otherwise unchanged can also result in differing estimates. For example, assumptions regarding contact patterns between individuals affected the R estimate for a model of a H1N1 influenza outbreak \citep{Ajelli2014}.

% One study compared the $R$ estimates of multiple different models, an exponential growth rate model, two types of compartmental SEIR model, and a stochastic compartmental SIR model on the same Spanish flu outbreak data and found that while there were differences in the $R$ estimates, all of them fell within an acceptable range \citep{Chowell2007}. Altering one's assumptions while keeping the model otherwise unchanged can also result in differing estimates. For example, assumptions regarding contact patterns between individuals affected the R estimate for a model of a H1N1 influenza outbreak, though all estimates were found to be within an acceptable range \citep{Ajelli2014}.

% \begin{align*}
% R = \frac{\beta \times S}{\gamma}
% \end{align*}

% (from project description). 
% It is possible, for instance, that they perform better on specific types of outbreak, such as outbreaks with an exponential growth phase, or with larger outbreaks.

% (The impact of factors such as under-reporting, reporting delays, or super-spreading may also be considered.)

% Transmissibility is estimated by calculating the likelihood of each susceptible individual being infected by a given infected individual, normalising for the likelihood that that susceptible was infected by another infected individual \citep{Wallinga2004}. From this estimate of $R$, incidence can be forecasted by using a branching process model again \citep{Nouvellet2017}.

% The branching process modelling approach could be used to model outbreaks in real-time. Here $R_{t}$, the effective reproduction number for a time window, can be used as the subject of estimation \citep{Wallinga2004, Cori2013}. Knowledge of the $R_{t}$ of the most recent timepoint can be used for forecasting incidence in the following time period \citep{Nouvellet2017}. Disease incidence forecasts can be used to aid decision-making in outbreak situations. Forecasts suggesting a major increase in the number of cases in the following weeks could for example highlight a need for additional hospital staff in the near future. 

% Longer explanations of forecasting

% H1N1 influenza
% For instance, during the 2009 pandemic H1N1 influenza in England, the peak of the epidemic as well as the cost-effectiveness of various vaccination strategies for the autumn of 2009 were predicted as the pandemic progressed in real-time \citep{Baguelin2010}.

% MERS
%The Middle-Eastern Respiratory Syndrome (MERS) outbreak in the Kingdom of Saudi Arabia in March to June 2014 raised concerns regarding the possible spread of infection during the Hajj that October, prompting a request for predicting disease incidence. Disease incidences from different pre-October time windows were used to forecast incidence in October under pessimistic and optimistic scenarios \citep{Lessler2014}. This modelling study found that an outbreak amongst Hajj pilgrims was not likely but recommended that some preparations for an outbreak should be made \citep{Lessler2014}. 
% \textcolor{red}{This one feels a bit less relevant than the rest, can remove if better that way.}

% Yellow fever
% Another example is the 2015-2016 yellow fever outbreak in Angola and the Democratic Republic of Congo, during which modellers were attempting to predict where to yellow fever would spread next \citep{Kraemer2017}. This was done by utilising data sets on consisting of the information such as what areas the mosquito vectors were viable in and human mobility in addition to basic information on daily incidence \citep{Kraemer2017}.  

% Ebola
% More recently, incidence forecasting was used during the Ebola outbreak of 2018 in the Democratic Republic of Congo, where data from April 30th to May 24th was available \citep{Barry2018}. Here it was determined that even in the worst case scenario, the affected areas would have been capable of isolating all required individuals as long as the outbreak did not expand \citep{Barry2018}. 

% This led to the birth of Farr's law stating that an epidemic's shape can be approximated by a normal curve, which has since faced some criticism regarding the conditions under which it can be applied \citep{Neuberger2013, Artzrouni1990}.

% \subsection{Models}

% \subsubsection{Other models}
% \textcolor{red}{Do not go into detail on how the other models work (provide references instead?). Talk about usage instead?}

% Compartmental models have compartments for every disease category of interest, in addition to rates describing the movement in and out of the compartments \citep{Anderson1991}. A common example of this type of model is the SIR model, where there are compartments for susceptible (S), infected (I), and recovered (R) individuals, and rates of infection and recovery linking S to I and I to R respectively \citep{Anderson1991}. A compartmental model can be fit to existing incidence data through estimating ranges of values that the model parameters can take by for example using particle Markov Chain Monte Carlo \citep{Funk2018}. This fitted model can then be used to obtain daily incidence forecasts \citep{Funk2018}.

% For such a model, $R$ can be estimated from the growth rate $r$ if the generation interval's duration is assumed to be constant \citep{Wallinga2007}.

%  ($\beta$ and $\gamma$ respectively) ($S$):$R = \frac{\beta \times S}{\gamma}$

% \subsubsection{Branching process model}
% \textcolor{red}{Explain what a branching process model is and maybe give examples of applications}

% \begin{figure}[h]
% \centering
% \includegraphics[scale = 0.25]{branching_process_example}
% \caption{A visualisation of what a branching process where each individual in a generation has a random number of offspring could look like.}
% \label{branching_process_ex}
% \end{figure}

% \textcolor{red}{I don't need to find a paper that right out states this, it's enough to see this from the lack of a group in the modelling process?}.

% \textcolor{red}{Thibaut suggested talking about how $R$ can be estimated from the growth rate somewhere in this section - essentially Wallinga \& Lipsitch 2007 - but I felt like if I would put that bit somewhere, it would be in the compartmental model paragraph and that could get confusing.}

% \subsection{Limited evaluation of performance}

% During the Ebola epidemic of 2013-2016, a lack of a ready-to-use tool for which there was a widespread consensus on how it should be used for forecasting epidemics was noted \citep{Cori2017}. 
%\textcolor{red}{I know this from talking to people who were involved with analysing the linelists etc., I tried to find a fitting paper to back this statement up and the closest thing I found was your paper from 2017. It doesn't fully make this point but rather says that there were disagreements on how forecasts should be used. I can change or delete this sentence if misleading.} 
% This led to the development of new tools and accompanying suggestions  for data analysis protocols by groups such as the R Epidemics Consortium (RECON) \citep{RECON2018}. RECON's R package \textit{projections}, for instance, is a forecasting tool that uses a branching process model to predict the number of cases during each of the forecasted days \citep{Jombart2018}. 

%%%%%%%%%%%%%
%% Methods %%
%%%%%%%%%%%%%

\newpage
\section{Methods}
% 5-10 pages

All data simulation and analyses presented in this report were conducted on R, the statistical software \citep{RCoreTeam2018}.

%%%%%%%%%%

\subsection{Outbreaks and predictive branching process model}

Empirical daily incidence data for three diseases, Ebola, influenza, and SARS, were chosen to assess the performance of the branching process model \citep{Leone2014, Cori2018, Jombart2018a}. Table \ref{outbreak_table} shows the published $R_{0}$, and the mean and standard deviation (in days) of the serial interval for these empirical outbreaks.

For each given disease, 80 simulated outbreaks based on the published $R_{0}$  serial interval (in days) for each empiricial disease outbreaks were run (Table \ref{outbreak_table}). Each simulation was run for eight times the rounded mean serial interval for a population size of 1 million. The length of the simulation was therefore 96 days for Ebola, 24 days for influenza, and 72 days for SARS. The simulations were conducted using the \textit{simOutbreak} function from the package \textit{outbreaker} \citep{Jombart2014}. For projection purposes, a section of this data would later be used to calibrate the predictive branching process model while other sections would be hidden to test the performance of said model.

%\textcolor{red}{REMOVE OUTBREAK DATA SOURCE}
% \textcolor{white}{\citep{Team2014, Fraser2011, Campbell2018}}
% Source for ebola data: http://opendatasl.gov.sl/dataset/ebola-virus-data
\vspace{2em}
<<outbreak_table, echo = FALSE, results = "asis", message = FALSE>>=
library(xtable)

outbreak_table <- array(NA, dim =c(3, 5))

# Disease names
outbreak_table[1, 1] <- "Ebola"
outbreak_table[2, 1] <- "Influenza"
outbreak_table[3, 1] <- "SARS"

# Disease R0
outbreak_table[1, 2] <- 2.02
outbreak_table[2, 2] <- 2.7
outbreak_table[3, 2] <- 1.77

# Disease SI mean
outbreak_table[1, 3] <- 11.6
outbreak_table[2, 3] <- 8.7
outbreak_table[3, 3] <- 2.6

# Disease SI SD
outbreak_table[1, 4] <- 5.6
outbreak_table[2, 4] <- 3.6
outbreak_table[3, 4] <- 1.5

# Source
outbreak_table[1, 5] <- "\\citep{Team2014}" # "WHO 2014, SL data"
outbreak_table[2, 5] <- "\\citep{Fraser2011}" # "EpiEstim, Fraser (2011)"
outbreak_table[3, 5] <- "\\citep{Campbell2018}" # "Campbell et al. (2018)"

colnames(outbreak_table) <- c("Disease", "$R_{0}$", "SI mean (days)", "SI SD (days)", "Source")
rownames(outbreak_table) <- c("Ebola", "Influenza", "SARS")

tab <- xtable(outbreak_table, digits = 2, caption = "The basic reproduction number, $R_{0}$, and serial interval (SI) mean and standard deviation (SD) in days used for simulating outbreaks of Ebola, influenza, and Severe Acute Respiratory Syndrome (SARS).", label = "outbreak_table")
align(tab) <- "lXXXXX"
align(tab) = c("p{0.15\\textwidth}", "p{0.2\\textwidth}", "p{0.15\\textwidth}", "p{0.2\\textwidth}", "p{0.2\\textwidth}", "p{0.1\\textwidth}")

print(tab, hline.after=c(-1, 0, 3), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "top", type = "latex", sanitize.rownames.function = identity,  sanitize.colnames.function = identity, sanitize.text.function = identity, tabular.environment = "tabularx", width = "\\textwidth")
@
\clearpage

To forecast daily disease incidence, a branching process forecasting method based on Cori et al.'s method for estimating $R$ was used \citep{Cori2013, Jombart2018}. The full details on how the forecasting branching process model evaluated in this study works can be found in Supplementary Information section 1.1. The information required for forecasting are data on observed daily incidence and a Gamma-distributed serial interval for the disease in question.

% The future incidence for each real and simulated outbreak was forecasted for a given number of time windows by first estimating the reproduction number ($R$) for the outbreak so far using the \textit{get\_R} function in the \textit{earlyR}-package (Fig. \ref{projection_ex})\citep{Jombart2017}. Future dates' incidence could then be projected based on the likelihood distribution of this $R$ using the \textit{project} function of the \textit{projections}-package \citep{Jombart2018}.

\textcolor{red}{more in-depth explanation} 
Before forecasting daily incidence, the force of infection for the observed time window was calculated. This was then used to calculate the likelihoods of varying values of the effective reproduction number, $R$. This $R$ likelihood distribution was then sampled from to obtain plausible values of $R$ used for projection purposes. This in turn is then used to predict force of infection and thus the number of new cases.

Forecasts for the daily incidence of each empirical and simulated outbreak were obtained as follows. First a distribution of likely values of $R$ were obtained based on the observed incidence in the calibration window and a discretised serial interval \textcolor{red}{get\_R}. This distribution is sampled from in the projection process to obtain likely values of $R$ for the first forecast day \textcolor{red}{sample\_R}. For the following forecast days, the projected daily incidence of the previous forecast days is taken into account when obtaining a forecast. $R$ is resampled for each day. Each projection is composed of 10,000 individual projection trajectories, from which the projection median and projection intervals are calculated. The full details on how the branching process model is used for forecasting can be found in the Supplementary Information section 1.1.

A single time window was defined as one mean serial interval, in days. A single projection window was the size of this time window, while calibration windows were multiples of the time window, with all calibration windows starting from day 0. The differing calibration window lengths are due to the oubreak's force of infection and thus $R$ for a given time window being affected by the cases from previous time windows. The incidence data was hidden for dates that were not within the calibration window.

The calibration windows and projection windows were combined so that the outbreak was either projected for maximum four time windows, or so that the sum of time windows (calibration or projection) was eight time windows. This would mean for instance that a calibration window the size of one time window for an outbreak would be used to predict four time windows ahead, while a calibration window consisting of seven time windows would only be used to predict one time window ahead. This limitation in the total number of time windows used was done in an attempt to keep the focus on the performance of the branching process model during early outbreak analysis and avoid the simulated outbreaks reaching a point where the outbreak has burned through the susceptibles.  

\begin{figure}[h]
<<branching_process_ex, echo = FALSE, warning = FALSE, fig.width = 6, fig.height = 4, fig.align = "center">>=
library(epitrix)
library(distcrete)
library(incidence)
library(earlyR)
library(projections)
library(EpiEstim)
library(outbreaks)
library(ggplot2)
library(grid)
library(gridBase)
library(magrittr)

# Influenza
data("Flu1918")
flu_1918 <- Flu1918
flu_i <- as.incidence(flu_1918$incidence, interval = 1)

delta <- 3

set.seed(1)

# Get serial interval and R calculation, do projection
# cv = sigma / mean 
flu_sim_si <- gamma_mucv2shapescale(2.6, (1.5/2.6))
flu_si <- distcrete("gamma", shape = flu_sim_si$shape, scale = flu_sim_si$shape, w = 0, interval = 1)
flu_R3 <- get_R(flu_i[1:(delta * 2), ], si = flu_si, max_R = 10)
flu_proj3 <- project(flu_i[1:(delta * 2), ], R = sample_R(flu_R3, 1000), si = flu_si, 
                     n_sim = 10000, n_days = (delta * 4 + 1), R_fix_within = TRUE)

# Plot R distribution
R_dataframe <- data.frame(grid = flu_R3$R_grid,
                          ml = flu_R3$R_ml,
                          ll = flu_R3$R_like)

R_plot <- ggplot(R_dataframe, aes(x = grid, y = ll)) +
                 geom_line(color = "dodgerblue") +
                 geom_area(fill = "dodgerblue", alpha = 0.5) +
                 geom_vline(xintercept = R_dataframe$ml, linetype = "dashed") +
                 labs(x = "Reproduction number", y = "Likelihood", size = 5) +
                 annotate("text", label = paste("R =", round(R_dataframe$ml, 2), sep = " "), 
                          x = R_dataframe$ml + 1, y = max(R_dataframe$ll), size = 4) +
                 coord_cartesian(xlim = c(0.0, 10.0)) +
                 theme(axis.text.y = element_blank(),
                       axis.ticks.y = element_blank())


# Plot influenza incidence
influenza_incidence <- plot(flu_i[1:19, ]) %>% add_projections(flu_proj3, quantiles = c(0.05, 0.5)) # the base projection plot
projection_plot <- influenza_incidence + 
                   geom_vline(xintercept = 7, linetype = "dashed") +
                   labs(x = "Day") +
                   coord_cartesian(xlim = c(1, 18)) +
                   annotation_custom(ggplotGrob(R_plot), xmin = 0.2, xmax = 6.9, ymin = 100, ymax = 200) +
                   geom_segment(aes(x = 1, xend = 6.9, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 6.9, xend = 1, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 7.1, xend = 18.8, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 18.8, xend = 7.1, y = 70, yend = 70), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 7.1, xend = 9.9, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 9.9, xend = 7.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 10.1, xend = 12.9, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 12.9, xend = 10.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 13.1, xend = 15.9, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 15.9, xend = 13.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 16.1, xend = 18.8, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   geom_segment(aes(x = 18.8, xend = 16.1, y = 50, yend = 50), size = 0.5,
                   arrow = arrow(length = unit(0.3, "cm"))) +
                   annotate("text", label = "Calibration window", x = 4, y = 77) +
                   annotate("text", label = "Full projection", x = 13, y = 77) +
                   annotate("text", label = "1", x = 8.6, y = 55) +
                   annotate("text", label = "2", x = 11.6, y = 55) +
                   annotate("text", label = "3", x = 14.6, y = 55) +
                   annotate("text", label = "4", x = 17.6, y = 55) +
                   theme(legend.position = "none")
# Print plot
projection_plot
@
% \centerline{\includegraphics[width=0.8\textwidth]{projection_example}}
\caption{A visualisation of a calibration window (true daily incidence: black bars), utilised for obtaining a likelihood distribution of the reproduction number ($R$) for the outbreak, and full projection for daily incidence forecasts (median: purple line, 95\% projection interval: shaded area). The full projection is split into individual projection windows later analysed by performance metrics (arrows labelled 1-4). As the projection progresses, the forecasted daily incidence of previous projections is taken into account when estimating the projected $R$.}
\label{projection_ex}
\end{figure}

\FloatBarrier
\newpage
\subsection{Performance metrics and their analysis}

The performance of different calibration windows with varying projection windows was quantified by observing the average residual, mean-square relative error (MSE), sharpness, and bias of the projections for each projection window of each simulated or real outbreak.

The average residual, measuring whether the model is over- or under-predicting values while taking into account the magnitude of the difference, was calculated for a given projection day $t$ as the mean of differences between the true incidence $x$ and projected incidence $p$ for all $I$ stochastic trajectories:
\begin{equation}
\epsilon_{t} = \frac{\sum_{i = 1}^{I}{(x_{t} - p_{i t})}}{I}
\end{equation}
Here a negative residual implies that the model is overpredicting the daily incidence, and a positive residual implies that the model is underpredicting the daily incidence. A perfect prediction would thus have a residual of 0.

The MSE, the sample squared standard deviation of the differences between predicted and observed values, was calculated. Much like the mean residual, this measures whether the model is over- or under-predicting values, but it is more successful at emphasising outliers as the errors are squared \textcolor{red}{REFERENCE}. 

For a given prediction day the MSE is calculated as:
\begin{equation}
MSE_{t} = \frac{\sum_{i = 1}^{I}{(x_{t} - p_{i t})^{2}}}{I \times (x + 1)^{2}}
\end{equation}
where the mean of the squared difference between the true incidence $x$ and projected incidence $p$ for trajectory $i$ is taken for all trajectories $I$. This is then divided by $x + 1$ to make the measure scale-independent in order to make the metric comparable between diseases and to avoid division by zero.  

Sharpness ($S$), is a measure of how narrow the range of predictions provided by the model are, was calculated following Funk et al.'s approach \citep{Funk2018}. Here the median absolute difference around the median for the collection of stochastic trajectories ($p$) for a given timepoint $t$ is calculated as:
\begin{equation}
S_{t} = 1 - \frac{median(|(p_{t} + 1) - (median(p_{t} + 1))|)}{median(p_{t} + 1)}
\end{equation}
To avoid dividing by 0, 1 is added to the forecasted incidence. Sharpness ranges from 0 to 1, where 1 is perfect sharpness.

Bias, showing systematic over- or under-prediction of daily incidence for a prediction window, was also calculated by adjusting Funk et al.'s approach to be applicable to a branching process model \citep{Funk2018}. Bias $B$ for a particular projection for a given projection day $t$ is:
\begin{equation}
B_{t} = 2(E_{t}(H(p_{i t} - x_{t})) - 0.5)
\end{equation}
where a Heaviside step function with half-maximum convention was applied to the difference between projected and true incidence for each stochastic trajectory $i$ on a given projection day $t$. The mean of this was then taken to obtain the expectation for day $t$, $E_{t}$. This metric is not a measure of how much a forecast over- or underpredicts, but rather how many stochastic trajectories over- or underpredict. An unbiased model would have a $B$ of 0, while a consistently overestimating and underestimating models would have a $B$ of 1 and -1, respectively.

To provide a single score for each projection window, the mean of the daily values for average residual, MSE, sharpness, and bias were calculated. The performance of the predictions was analysed in the same manner for both the simulated and empirical outbreaks. 

% Reliability, the predictive model's ability to assess uncertainty, was assessed by identifying how likely it was that the true incidence for a given day would have come from the distribution of predictions for that given timepoint. This was calculated by using Funk et al.'s approach where the uniformity of predictions' cumulative distribution functions is tested with an Anderson-Darling test, with a modification allowing for the discrete Poisson distribution to be perceived as continuous. EXPLAIN HOW I DID THIS.
%The RMSE and bias for the branching process model's prediction windows was compared against a null model where the predicted incidence for the projection windows was the mean of the incidence of the calibration window.while for reliability, the p-value for the Anderson-Darling test on the data for the given prediction window was calculated.

In order to determine which aspects of the outbreaks affect the accuracy of the branching process model's predictions as measured by the metrics, a linear regression analysis was undertaken for each metric of the simulated outbreaks. The explanatory variables taken into consideration were disease ($d$), the size of the calibration window ($c$), the number of cases within said calibration window ($n$), the distance in serial intervals between the end of the calibration window and the end of the projection window ($p$), and whether the projection window contained projections that predicted only zero incidence, some zero incidence, or no zero incidence ($z$). Thus, the equation for the linear regression consisting of all these explanatory variables for a metric $y$ would be:
\begin{equation}
  y = a + b_{0}d + b_{1}c + b_{2}n + b_{3}p + b_{4}z + \epsilon
\end{equation}
where $b$ represents the regression slope for each variable, $\epsilon$ is the error, and $a$ is the intercept. Dummy variables are used to incorporate the categorical variables disease $d$ and projection type $p$ into the model.  

The most parsimonius model was selected stepwise out of a starting model consisting of all the above explanatory variables by using Akaike Information Criterion (AIC) by implementing the \textit{stepAIC} function in the R package \textit{MASS} \citep{Venables2002}. This model was then treated as the "basic" model. After this, the additional benefit to the quality of the model gained by adding either an additive interaction between calibration window size and distance of projection window, the ratio of calibration window size to prediction window number, or both interaction and ratio to the basic model was assessed by comparing the AICs of each model. The correlation between the performance metrics was also explored through observing the correlation coefficients for every combination of metric pairs.

% \textcolor{red}{Here Thibaut had left a note in my thesis - "These are other response variables". I find this a bit cryptic as we didn't discuss this in our meeting and I only saw this later}.
%\textcolor{red}{Let me know if you think that I should have just put everything (interactions and all) into the "basic" model and then just found the most parsimonius model using stepwise AIC and not have bothered with comparing the "basic" model with additions.}

%%%%%%%%%%%%%
%% Results %%
%%%%%%%%%%%%%

\FloatBarrier
\newpage
\section{Results}

\subsection{Simulated outbreaks}

% The real and simulated outbreaks were fitted with multiple calibration windows and projection windows, as exemplified with the real outbreak datasets in Fig. \ref{projection_plot}. As is seen in the figure, some time windows are projected for more than once, as often multiple projection windows at different distances from the calibration window are predicted based on a single calibration window. The predicted daily incidence for the real Ebola and influenza outbreaks is overestimated especially when forecasts are based on early outbreak disease incidence data and incidence is forecasted for many projection windows
% \subsubsection{Prediction metrics}

The simulated outbreaks varied in total outbreak size both within and between diseases. The length of simulated outbreak also varied between diseases as each simulation was run for eight times the mean serial interval of a given disease. Consquently, the Ebola-like simulations experienced the largest outbreaks of up to 600 cases, whilst the influenza outbreaks barely reached 50 cases (SI Fig. \ref{sim_dist}). This wide variety in outbreak size resulted in forecasts of irregular quality. Figure \ref{good_bad_plot} exemplifies this with three representative forecasts for simulated Ebola-like outbreaks. While some forecasts were near the true incidence and with a relatively narrow range of projections, others had either a wide range of projections, a range that did more often not include the true daily incidence, or both.  

\begin{figure}[h]
<<good_bad_plot, cache = TRUE, echo = FALSE, fig.width = 8, fig.height = 2.5, fig.align = "center", message = FALSE, warning = FALSE>>=
library(incidence)
library(EpiEstim)
library(outbreaks)
library(ggplot2)
library(epitrix)
library(distcrete)
library(outbreaker)

# Simulation
sim_outbreak <- function() {
  mu <- 11.6
  cv <- 5.6 / 11.6
  R0 <- 2.02
  obs_time <- round(mu) * 8
  
  sim_si <- gamma_mucv2shapescale(mu, cv)
  si <- distcrete("gamma", shape = sim_si$shape, scale = sim_si$shape, w = 0, interval = 1)
  
  sim_test <- simOutbreak(R0 = R0, infec.curve = si$d(0:30), n.hosts = 1000000, duration = obs_time, seq.length = 10, stop.once.cleared = FALSE)
  
  sim_outbreak <- data.frame(id = sim_test$id,
                             inf_id = sim_test$ances,
                             onset = sim_test$onset)
  return(sim_outbreak)  
}

# Bad projection - set.seed = 2, 24?, 25?, 53?
# Other two projections - set.seed = 4, 12, 48
# 4, 5, 6, 9, 12, 38, 41, 48, 72, 77, 82, 92, 96
# for (i in 1:100) {
# set.seed(i)
# sim_linelist <- sim_outbreak()
# sim_i <- incidence(sim_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
# 
# if (sim_i$n > 300) {
#   print(i)
# }
# }
# plot(sim_i)

# Serial interval for projection
mu <- 11.6
cv <- 5.6 / 11.6
R0 <- 2.02
delta <- round(mu)
  
sim_si <- gamma_mucv2shapescale(mu, cv)
si <- distcrete("gamma", shape = sim_si$shape, scale = sim_si$shape, w = 0, interval = 1)

# Precise and accurate projection
set.seed(72)
sim_linelist <- sim_outbreak() 
sim_i <- incidence(sim_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
good_R <- get_R(sim_i[1:(delta * 6), ], si = si, max_R = 10)
good_proj <- project(sim_i[1:(delta * 6), ], R = sample_R(good_R, 1000), si = si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)
good_plot <- plot(sim_i[1:(8*12), ]) %>% add_projections(good_proj, quantiles = c(0.05, 0.5))
good_plot <- good_plot + theme(legend.position = "none") +
                         coord_cartesian(ylim = c(0, 65)) +
                         xlab("Time (days)") +
                         annotate("text", label = "A", x = 5, y = 65)

# Accurate but imprecise projection
set.seed(48)
wide_linelist <- sim_outbreak() 
wide_i <- incidence(wide_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
wide_R <- get_R(wide_i[1:(delta * 2), ], si = si, max_R = 10)
wide_proj <- project(wide_i[1:(delta * 2), ], R = sample_R(wide_R, 1000), si = si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
wide_plot <- plot(wide_i[1:(8*12), ]) %>% add_projections(wide_proj[(2*12):(3*12), ], quantiles = c(0.05, 0.5))# (wide_proj[(4 * 12):((5 * 12) - 1), ])
wide_plot <- wide_plot + theme(legend.position = "none") +
                         coord_cartesian(ylim = c(0, 65)) +
                         xlab("Time (days)") +
                         annotate("text", label = "B", x = 5, y = 65)

# An overall bad projection
set.seed(33)
bad_linelist <- sim_outbreak() 
bad_i <- incidence(bad_linelist$onset, interval = 1, last_date = (round(11.6) * 8))
bad_R <- get_R(sim_i[1:(delta * 2), ], si = si, max_R = 10)
bad_proj <- project(bad_i[1:(delta * 2), ], R = sample_R(bad_R, 1000), si = si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
bad_plot <- plot(bad_i[1:(8*12), ]) %>% add_projections(bad_proj[(3*12):(4*12), ], quantiles = c(0.05, 0.5))
bad_plot <- bad_plot + theme(legend.position = "none") +
                       coord_cartesian(ylim = c(0, 65)) +
                       xlab("Time (days)") +
                       annotate("text", label = "C", x = 5, y = 65)

# Combine plots into one plot
multiplot(good_plot, wide_plot, bad_plot, cols = 3)
@
\caption{Examples of daily incidence forecasts for simulations of Ebola-like outbreaks. Daily incidence curves (black histogram) are shown with the forecasted daily incidence (purple line: median, 95\% prediction interval: pink line and shaded areas). Panel A shows a well-performing projection where the forecasted daily incidence along with its 95\% interval is near the true observed incidence. Panel B shows a projection where the median forecasted incidence is near the true observed values, but the spread of projections is wide. Panel C shows a projection where the median forecasted simulation is not near the true observed incidence and the spread of forecasts is wide.}
\label{good_bad_plot}
\end{figure}

\begin{figure}[h]
<<all_metric_plot, echo = FALSE, fig.width = 8, fig.height = 10, warning = FALSE>>=
# Only want calibration windows 1, 2, and 4
sub_table <- filter(total_table, cali_window_size == 1 | cali_window_size == 2 | cali_window_size == 4) 

# Make a new column so that I get all the combinations into one graph
sub_table$cali_proj <- paste(sub_table$cali_window_size, ":", sub_table$proj_window_no, sep = "")

# Residual
residual_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(residual), std = sd(residual), q1 = quantile(residual, probs = 0.025), q2 = quantile(residual, probs = 0.25), q3 = quantile(residual, probs = 0.75), q4 = quantile(residual, probs = 0.975))

residual_sub <- ggplot(residual_iqr, aes(cali_proj, y = med, ymin = q2, ymax = q3, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

residual_sub2 <- ggplot(residual_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    # coord_cartesian(ylim = quantile(sub_table$residual, c(0, 0.97))) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# MSE
mse_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(mse), std = sd(mse), q1 = quantile(mse, probs = 0.025), q2 = quantile(mse, probs = 0.25), q3 = quantile(mse, probs = 0.75), q4 = quantile(mse, probs = 0.975))

mse_sub <- ggplot(mse_iqr, aes(cali_proj, y = med, ymin = q1, ymax = q3, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

mse_sub2 <- ggplot(mse_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    # coord_cartesian(ylim = quantile(sub_table$residual, c(0, 0.97))) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

# Sharpness
sharpness_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.025), q2 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75), q4 = quantile(sharpness, probs = 0.975))

sharpness_sub <- ggplot(sharpness_iqr, aes(cali_proj, y = med, ymin = q1, ymax = q3, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    coord_cartesian(ylim = c(0, 1)) +
                    labs(y = "Sharpness", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

sharpness_sub2 <- ggplot(sharpness_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    coord_cartesian(ylim = c(0, 1)) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Sharpness", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank(),
                          strip.text.x = element_blank())

# Bias
bias_iqr <- sub_table %>% group_by(disease, cali_proj, cali_window_size) %>% summarise(med = median(bias), std = sd(bias), q1 = quantile(bias, probs = 0.025), q2 = quantile(bias, probs = 0.25), q3 = quantile(bias, probs = 0.75), q4 = quantile(bias, probs = 0.975))

bias_sub <- ggplot(bias_iqr, aes(cali_proj, y = med, ymin = q1, ymax = q3, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 16) +
                    facet_wrap(~disease, scales = "free") +
                    coord_cartesian(ylim = c(-1, 1)) +
                    labs(y = "Bias", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          strip.text.x = element_blank())

bias_sub2 <- ggplot(bias_iqr, aes(x = cali_proj, color = cali_window_size)) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    coord_cartesian(ylim = c(-1, 1)) +
                    facet_wrap(~disease, scales = "free") +
                    labs(y = "Bias", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          strip.text.x = element_blank())


# Call plot
multiplot(residual_sub2, mse_sub2, sharpness_sub2, bias_sub2)
@
\caption{The medians (horizontal line in box), interquartile ranges (box), 95\% quantile ranges (whiskers) of the four prediction metrics - average residual, mean-square error (MSE), sharpness, and bias, for Ebola-like (left), influenza-like (middle), and SARS-like (right) outbreaks for projection window's distance from the calibration window increases from 1 to 4 for calibration windows of size 1 (dark blue), 2 (blue), and 4 (light blue).}
\label{all_metric_plot}
\end{figure}
\FloatBarrier

Figure \ref{all_metric_plot} shows how the performance of the branching process model varies with different combinations of calibration window size and the distance between the calibration window size and projection window for the three simulated diseases. The full results for every combination of calibration window size and projection window number can be found in section 2.2 of the Supplementary Information, though the general trend remains similar across calibration windows. Across all three simulated diseases, the average residual decreased and the MSE increased as the projection window moved further from the calibration window, implying overestimation of daily incidence. The Ebola-like outbreaks saw the greatest overestimations of daily incidence, while forecasts for the SARS-like outbreaks tended to be closest to the true daily incidence. The extent of this overestimation reduced as the calibration window increased for the Ebola-like and influenza-like outbreaks, implying that having more days' worth of data improved the accuracy of forecasted disease incidence estimates. This was not the case for SARS, which saw a slight increase in overestimation with increasing calibration window size, though the scale of the overestimation was smaller than that of influenza-like or Ebola-like outbreak.

The median sharpness of the forecasts for the simulated outbreaks was higher with fewer calibration windows and decreased as calibration window size increased or as the projection window moved further from the calibration window. The number of cases observed in these early calibration windows are low and the projections for these calibration windows include the possibility of no cases occurring (SI Fig. \ref{cali_zero_plot}). 

As sharpness is measured by observing how far from the median predicted incidence a single trajectory is, a forecast's sharpness can be expected to be high for a forecast for which many of the 10,000 individual trajectories that it consists of forecast no new cases. This is because the median predicted incidence of these 10,000 trajectories will be zero and thus many trajectories will be exactly on the median. This in turn has an effect on the distribution of the values that sharpness takes, where forecasts that include the possibility of no new cases either have a high sharpness or sharpness that is lower than that of forecasts that do not predict extinction (SI Fig. \ref{sharpness_type_plot}). The drop in sharpness seen as calibration size increases is partly explained by the increasing number of projections that have rejected the possibility of no cases occurring in the projection window as more trajectories deviate from the median at some point during the course of the projection (SI Fig. \ref{cali_zero_plot}). 

% \textcolor{red}{say something about widening 95\% quantiles?} 
% \textcolor{red}{in Discussion say that this is because some projections then may start predicting an outbreak and go further from those that don't - giving big differences}. 
% \textcolor{red}{I could compare diseases an hypothesise about the link between Ebola simulation having larger outbreaks and being the first to exhibit projections where zero incidence is not the case anymore.}
% \textcolor{red}{The drop in sharpness seen when projections are made later in the epidemic are partly due to simulations where the epidemic goes exctinct but the projected trajectories fail to mirror/predict this.}

The forecasts' median bias scores are mostly positive for Ebola-like and influenza-like outbreaks suggesting that the forecasts tend to overestimate disease incidence, though this overestimation reduces as calibration window size increases (Fig. \ref{all_metric_plot}, SI Fig. \ref{whole_bias}). This is in line with the average residual's suggestions that the forecasts are overestimating daily incidence. The 95\% quantile range of bias also widens as the projection window moves further into the future from the calibration window.

An interesting trend is seen for the first projection window, where MSE increases with increasing calibration window size while the bias score decreases (Fig. \ref{all_metric_plot}). This reminds us that while the MSE gives an indication of whether or not the projection is far from the true daily incidence, unlike the average residual and bias, it does not give a clear indication of over- or under-prediction (SI Fig. \ref{whole_mse}, SI Fig. \ref{whole_bias}). 

<<metric_residual_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
residual_lm_start <- lm(residual ~ cali_window_size
                      + factor(disease_num)
                      + log(no_cali_cases)
                      + proj_window_no
                      + factor(pred_type),
                        data = total_table)

# AIC to find most parsimonious model
residual_lm_basic <- stepAIC(residual_lm_start)

# Compare basic model to ones with extra things
# Ratio
residual_lm_ratio <- lm(residual ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + cali_proj_ratio,
                        data = total_table)

# Interaction                        
residual_lm_inter <- lm(residual ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
residual_lm_inter_ratio <- lm(residual ~ cali_window_size
                              + factor(disease_num)
                              + log(no_cali_cases)
                              + proj_window_no
                              + factor(pred_type)
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
residual_aic <- AIC(residual_lm_basic, residual_lm_ratio, residual_lm_inter, residual_lm_inter_ratio)

# Analysis of variance
residual_anova <- anova(residual_lm_basic)

# Summary
residual_summary <- summary(residual_lm_basic)
@

<<metric_mse_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
mse_lm_start <- lm(mse ~ cali_window_size
                      + factor(disease_num)
                      + log(no_cali_cases)
                      + proj_window_no
                      + factor(pred_type),
                        data = total_table)

# AIC to find most parsimonious model
mse_lm_basic <- stepAIC(mse_lm_start)

# Compare basic model to ones with extra things
# Ratio
mse_lm_ratio <- lm(mse ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + cali_proj_ratio,
                        data = total_table)

# Interaction                        
mse_lm_inter <- lm(mse ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
mse_lm_inter_ratio <- lm(mse ~ cali_window_size
                              + factor(disease_num)
                              + log(no_cali_cases)
                              + proj_window_no
                              + factor(pred_type)
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
mse_aic <- AIC(mse_lm_basic, mse_lm_ratio, mse_lm_inter, mse_lm_inter_ratio)

# Analysis of variance
mse_anova <- anova(mse_lm_inter_ratio)

# Summary
mse_summary <- summary(mse_lm_inter_ratio)
@

<<metric_sharpness_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
sharpness_lm_start <- lm(sharpness ~ cali_window_size
                      + factor(disease_num)
                      + log(no_cali_cases)
                      + proj_window_no
                      + factor(pred_type),
                        data = total_table)

# AIC to find most parsimonious model
sharpness_lm_basic <- stepAIC(sharpness_lm_start)

# Compare basic model to ones with extra things
# Ratio
sharpness_lm_ratio <- lm(sharpness ~ cali_window_size
                         + factor(disease_num)
                         + log(no_cali_cases)
                         + proj_window_no
                         + factor(pred_type)
                         + cali_proj_ratio,
                         data = total_table)

# Interaction                        
sharpness_lm_inter <- lm(sharpness ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
sharpness_lm_inter_ratio <- lm(sharpness ~ cali_window_size
                              + factor(disease_num)
                              + log(no_cali_cases)
                              + proj_window_no
                              + factor(pred_type)
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
sharpness_aic <- AIC(sharpness_lm_basic, sharpness_lm_ratio, sharpness_lm_inter, sharpness_lm_inter_ratio)

# Analysis of variance
sharpness_anova <- anova(sharpness_lm_inter)

# Summary
sharpness_summary <- summary(sharpness_lm_inter)
@

<<metric_bias_lm, echo = FALSE, results = "hide", message = FALSE>>=
library(MASS)

# Number diseases
total_table$disease_num <- 1
total_table$disease_num[total_table$disease == "influenza"] <- 2
total_table$disease_num[total_table$disease == "sars"] <- 3
  
# Full model
bias_lm_start <- lm(bias ~ cali_window_size
                      + factor(disease_num)
                      + log(no_cali_cases)
                      + proj_window_no
                      + factor(pred_type),
                        data = total_table)

# AIC to find most parsimonious model
bias_lm_basic <- stepAIC(bias_lm_start)

# Compare basic model to ones with extra things
# Ratio
bias_lm_ratio <- lm(bias ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + cali_proj_ratio,
                        data = total_table)

# Interaction                        
bias_lm_inter <- lm(bias ~ cali_window_size
                        + factor(disease_num)
                        + log(no_cali_cases)
                        + proj_window_no
                        + factor(pred_type)
                        + proj_window_no * cali_window_size, # additive effect
                        data = total_table)

# Ratio and interaction
bias_lm_inter_ratio <- lm(bias ~ cali_window_size
                              + factor(disease_num)
                              + log(no_cali_cases)
                              + proj_window_no
                              + factor(pred_type)
                              + cali_proj_ratio
                              + proj_window_no * cali_window_size, # additive effect
                              data = total_table)

# AIC to compare the different models
bias_aic <- AIC(bias_lm_basic, bias_lm_ratio, bias_lm_inter, bias_lm_inter_ratio)

# Analysis of variance
bias_anova <- anova(bias_lm_inter_ratio)

# Summary
bias_summary <- summary(bias_lm_inter_ratio)
@

<<metric_lm_table, echo = FALSE, results = "asis">>=
library(xtable)

# Best-performing models
residual_summary <- summary(residual_lm_inter)
mse_summary <- summary(mse_lm_inter_ratio)
sharpness_summary <- summary(sharpness_lm_inter)
bias_summary <- summary(bias_lm_inter)

lm_table <- array(NA, dim =c(38, 5))

# disease names
lm_table[1, 1] <- "Residual"
lm_table[10, 1] <- "MSE"
lm_table[20, 1] <- "Sharpness"
lm_table[29, 1] <- "Bias"

# Coefficient names
lm_table[c(1, 10, 20, 29), 2] <- "Intercept"
lm_table[c(2, 11, 21, 30), 2] <- "Cal. size \\textsuperscript{1}"# "Calibration window size"
# lm_table[c(3, 14, 26, 37), 2] <- "Disease 1 \\textsuperscript{2}" # Ebola
lm_table[c(3, 12, 22, 31), 2] <- "Disease 2 \\textsuperscript{2}" # influenza
lm_table[c(4, 13, 23, 32), 2] <- "Disease 3 \\textsuperscript{2}" # SARS
lm_table[c(5, 14, 24, 33), 2] <- "No. cases \\textsuperscript{3}" # "No. cases in calibration window"
lm_table[c(6, 15, 25, 34), 2] <- "Proj. window \\textsuperscript{4}" # "Projection window no."
# lm_table[c(8, 19, 31, 42), 2] <- "Pred. group 1 \\textsuperscript{5}"# "Prediction group type"
lm_table[c(7, 16, 26, 35), 2] <- "Pred. group 2 \\textsuperscript{5}"# "Prediction group type"
lm_table[c(8, 17, 27, 36), 2] <- "Pred. group 3 \\textsuperscript{5}"# "Prediction group type"
lm_table[c(18), 2] <- "Ratio \\textsuperscript{6}" # "Ratio of calibration window size to projection window number"
lm_table[c(9, 19, 28, 37), 2] <- "Interaction \\textsuperscript{7}" # "Interaction between calibration and projection window"

# Estimates and confidence intervals

# Residual
lm_table[1, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[1, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[1, 1] - 1.96 * residual_summary$coefficients[1, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[1, 1] + 1.96 * residual_summary$coefficients[1, 2], 2)), ")", sep = "")

lm_table[2, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[2, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[2, 1] - 1.96 * residual_summary$coefficients[2, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[2, 1] + 1.96 * residual_summary$coefficients[2, 2], 2)), ")", sep = "")

# lm_table[3, 3] <- "baseline"

lm_table[3, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[3, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[3, 1] - 1.96 * residual_summary$coefficients[3, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[3, 1] + 1.96 * residual_summary$coefficients[3, 2], 2)), ")", sep = "")

lm_table[4, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[4, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[4, 1] - 1.96 * residual_summary$coefficients[4, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[4, 1] + 1.96 * residual_summary$coefficients[4, 2], 2)), ")", sep = "")

lm_table[5, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[5, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[5, 1] - 1.96 * residual_summary$coefficients[5, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[5, 1] + 1.96 * residual_summary$coefficients[5, 2], 2)), ")", sep = "")

lm_table[6, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[6, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[6, 1] - 1.96 * residual_summary$coefficients[6, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[6, 1] + 1.96 * residual_summary$coefficients[6, 2], 2)), ")", sep = "")

# lm_table[8, 3] <- "baseline"

lm_table[7, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[7, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[7, 1] - 1.96 * residual_summary$coefficients[7, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[7, 1] + 1.96 * residual_summary$coefficients[7, 2], 2)), ")", sep = "")

lm_table[8, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[8, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[8, 1] - 1.96 * residual_summary$coefficients[8, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[8, 1] + 1.96 * residual_summary$coefficients[8, 2], 2)), ")", sep = "")

lm_table[9, 3] <- paste(sprintf("%.2f", round(residual_summary$coefficients[9, 1], 2)), "(", sprintf("%.2f", round(residual_summary$coefficients[9, 1] - 1.96 * residual_summary$coefficients[9, 2], 2)), " -- ", sprintf("%.2f", round(residual_summary$coefficients[9, 1] + 1.96 * residual_summary$coefficients[9, 2], 2)), ")", sep = "")

# mse
lm_table[10, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[1, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[1, 1] - 1.96 * mse_summary$coefficients[1, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[1, 1] + 1.96 * mse_summary$coefficients[1, 2], 2)), ")", sep = "")

lm_table[11, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[2, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[2, 1] - 1.96 * mse_summary$coefficients[2, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[2, 1] + 1.96 * mse_summary$coefficients[2, 2], 2)), ")", sep = "")

# lm_table[14, 3] <- "baseline"

lm_table[12, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[3, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[3, 1] - 1.96 * mse_summary$coefficients[3, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[3, 1] + 1.96 * mse_summary$coefficients[3, 2], 2)), ")", sep = "")

lm_table[13, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[4, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[4, 1] - 1.96 * mse_summary$coefficients[4, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[4, 1] + 1.96 * mse_summary$coefficients[4, 2], 2)), ")", sep = "")

lm_table[14, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[5, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[5, 1] - 1.96 * mse_summary$coefficients[5, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[5, 1] + 1.96 * mse_summary$coefficients[5, 2], 2)), ")", sep = "")

lm_table[15, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[6, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[6, 1] - 1.96 * mse_summary$coefficients[6, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[6, 1] + 1.96 * mse_summary$coefficients[6, 2], 2)), ")", sep = "")

# lm_table[19, 3] <- "baseline"

lm_table[16, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[7, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[7, 1] - 1.96 * mse_summary$coefficients[7, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[7, 1] + 1.96 * mse_summary$coefficients[7, 2], 2)), ")", sep = "")

lm_table[17, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[8, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[8, 1] - 1.96 * mse_summary$coefficients[8, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[8, 1] + 1.96 * mse_summary$coefficients[8, 2], 2)), ")", sep = "")

lm_table[18, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[9, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[9, 1] - 1.96 * mse_summary$coefficients[9, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[9, 1] + 1.96 * mse_summary$coefficients[9, 2], 2)), ")", sep = "")

lm_table[19, 3] <- paste(sprintf("%.2f", round(mse_summary$coefficients[10, 1], 2)), "(", sprintf("%.2f", round(mse_summary$coefficients[10, 1] - 1.96 * mse_summary$coefficients[10, 2], 2)), " -- ", sprintf("%.2f", round(mse_summary$coefficients[10, 1] + 1.96 * mse_summary$coefficients[10, 2], 2)), ")", sep = "")

# sharpness
lm_table[20, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[1, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[1, 1] - 1.96 * sharpness_summary$coefficients[1, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[1, 1] + 1.96 * sharpness_summary$coefficients[1, 2], 2)), ")", sep = "")

lm_table[21, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[2, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[2, 1] - 1.96 * sharpness_summary$coefficients[2, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[2, 1] + 1.96 * sharpness_summary$coefficients[2, 2], 2)), ")", sep = "")

# lm_table[26, 3] <- "baseline"

lm_table[22, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[3, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[3, 1] - 1.96 * sharpness_summary$coefficients[3, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[3, 1] + 1.96 * sharpness_summary$coefficients[3, 2], 2)), ")", sep = "")

lm_table[23, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[4, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[4, 1] - 1.96 * sharpness_summary$coefficients[4, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[4, 1] + 1.96 * sharpness_summary$coefficients[4, 2], 2)), ")", sep = "")

lm_table[24, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[5, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[5, 1] - 1.96 * sharpness_summary$coefficients[5, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[5, 1] + 1.96 * sharpness_summary$coefficients[5, 2], 2)), ")", sep = "")

lm_table[25, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[6, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[6, 1] - 1.96 * sharpness_summary$coefficients[6, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[6, 1] + 1.96 * sharpness_summary$coefficients[6, 2], 2)), ")", sep = "")

# lm_table[31, 3] <- "baseline"

lm_table[26, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[7, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[7, 1] - 1.96 * sharpness_summary$coefficients[7, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[7, 1] + 1.96 * sharpness_summary$coefficients[7, 2], 2)), ")", sep = "")

lm_table[27, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[8, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[8, 1] - 1.96 * sharpness_summary$coefficients[8, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[8, 1] + 1.96 * sharpness_summary$coefficients[8, 2], 2)), ")", sep = "")

lm_table[28, 3] <- paste(sprintf("%.2f", round(sharpness_summary$coefficients[9, 1], 2)), "(", sprintf("%.2f", round(sharpness_summary$coefficients[9, 1] - 1.96 * sharpness_summary$coefficients[9, 2], 2)), " -- ", sprintf("%.2f", round(sharpness_summary$coefficients[9, 1] + 1.96 * sharpness_summary$coefficients[9, 2], 2)), ")", sep = "")

# bias
lm_table[29, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[1, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[1, 1] - 1.96 * bias_summary$coefficients[1, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[1, 1] + 1.96 * bias_summary$coefficients[1, 2], 2)), ")", sep = "")

lm_table[30, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[2, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[2, 1] - 1.96 * bias_summary$coefficients[2, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[2, 1] + 1.96 * bias_summary$coefficients[2, 2], 2)), ")", sep = "")

# lm_table[37, 3] <- "baseline"

lm_table[31, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[3, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[3, 1] - 1.96 * bias_summary$coefficients[3, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[3, 1] + 1.96 * bias_summary$coefficients[3, 2], 2)), ")", sep = "")

lm_table[32, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[4, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[4, 1] - 1.96 * bias_summary$coefficients[4, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[4, 1] + 1.96 * bias_summary$coefficients[4, 2], 2)), ")", sep = "")

lm_table[33, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[5, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[5, 1] - 1.96 * bias_summary$coefficients[5, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[5, 1] + 1.96 * bias_summary$coefficients[5, 2], 2)), ")", sep = "")

lm_table[34, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[6, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[6, 1] - 1.96 * bias_summary$coefficients[6, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[6, 1] + 1.96 * bias_summary$coefficients[6, 2], 2)), ")", sep = "")

# lm_table[42, 3] <- "baseline"

lm_table[35, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[7, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[7, 1] - 1.96 * bias_summary$coefficients[7, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[7, 1] + 1.96 * bias_summary$coefficients[7, 2], 2)), ")", sep = "")

lm_table[36, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[8, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[8, 1] - 1.96 * bias_summary$coefficients[8, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[8, 1] + 1.96 * bias_summary$coefficients[8, 2], 2)), ")", sep = "")

lm_table[37, 3] <- paste(sprintf("%.2f", round(bias_summary$coefficients[9, 1], 2)), "(", sprintf("%.2f", round(bias_summary$coefficients[9, 1] - 1.96 * bias_summary$coefficients[9, 2], 2)), " -- ", sprintf("%.2f", round(bias_summary$coefficients[9, 1] + 1.96 * bias_summary$coefficients[9, 2], 2)), ")", sep = "")

# p-value
# Residual
lm_table[1, 4] <- signif(residual_summary$coefficients[1, 4], 3)
lm_table[2, 4] <- signif(residual_summary$coefficients[2, 4], 3)
# lm_table[3, 4] <- 
lm_table[3, 4] <- signif(residual_summary$coefficients[3, 4], 3)
lm_table[4, 4] <- signif(residual_summary$coefficients[4, 4], 3)
lm_table[5, 4] <- signif(residual_summary$coefficients[5, 4], 3)
lm_table[6, 4] <- signif(residual_summary$coefficients[6, 4], 3)
# lm_table[8, 4] <- 
lm_table[7, 4] <- signif(residual_summary$coefficients[7, 4], 3)
lm_table[8, 4] <- signif(residual_summary$coefficients[8, 4], 3)
lm_table[9, 4] <- signif(residual_summary$coefficients[9, 4], 3)

# mse
lm_table[10, 4] <- signif(mse_summary$coefficients[1, 4], 3)
lm_table[11, 4] <- signif(mse_summary$coefficients[2, 4], 3)
# lm_table[14, 4] <- 
lm_table[12, 4] <- signif(mse_summary$coefficients[3, 4], 3)
lm_table[13, 4] <- signif(mse_summary$coefficients[4, 4], 3)
lm_table[14, 4] <- signif(mse_summary$coefficients[5, 4], 3)
lm_table[15, 4] <- signif(mse_summary$coefficients[6, 4], 3)
# lm_table[19, 4] <- 
lm_table[16, 4] <- signif(mse_summary$coefficients[7, 4], 3)
lm_table[17, 4] <- signif(mse_summary$coefficients[8, 4], 3)
lm_table[18, 4] <- signif(mse_summary$coefficients[9, 4], 3)
lm_table[19, 4] <- signif(mse_summary$coefficients[10, 4], 3)

# sharpness
lm_table[20, 4] <- signif(sharpness_summary$coefficients[1, 4], 3)
lm_table[21, 4] <- signif(sharpness_summary$coefficients[2, 4], 3)
# lm_table[26, 4] <- 
lm_table[22, 4] <- signif(sharpness_summary$coefficients[3, 4], 3)
lm_table[23, 4] <- signif(sharpness_summary$coefficients[4, 4], 3)
lm_table[24, 4] <- signif(sharpness_summary$coefficients[5, 4], 3)
lm_table[25, 4] <- signif(sharpness_summary$coefficients[6, 4], 3)
# lm_table[31, 4] <- 
lm_table[26, 4] <- signif(sharpness_summary$coefficients[7, 4], 3)
lm_table[27, 4] <- signif(sharpness_summary$coefficients[8, 4], 3)
lm_table[28, 4] <- signif(sharpness_summary$coefficients[9, 4], 3)

# bias
lm_table[29, 4] <- signif(bias_summary$coefficients[1, 4], 3)
lm_table[30, 4] <- signif(bias_summary$coefficients[2, 4], 3)
# lm_table[37, 4] <- 
lm_table[31, 4] <- signif(bias_summary$coefficients[3, 4], 3)
lm_table[32, 4] <- signif(bias_summary$coefficients[4, 4], 3)
lm_table[33, 4] <- signif(bias_summary$coefficients[5, 4], 3)
lm_table[34, 4] <- signif(bias_summary$coefficients[6, 4], 3)
# lm_table[42, 4] <- 
lm_table[35, 4] <- signif(bias_summary$coefficients[7, 4], 3)
lm_table[36, 4] <- signif(bias_summary$coefficients[8, 4], 3)
lm_table[37, 4] <- signif(bias_summary$coefficients[9, 4], 3)

# Adjusted R-squared
lm_table[1, 5] <- round(residual_summary$adj.r.squared, 2)
lm_table[10, 5] <- round(mse_summary$adj.r.squared, 2)
lm_table[20, 5] <- round(sharpness_summary$adj.r.squared, 2)
lm_table[29, 5] <- round(bias_summary$adj.r.squared, 2)

colnames(lm_table) <- c("Metric", "Coefficient", "Estimate (95\\% CI)", "p-value", "Adjusted $R^{2}$")

addtorow <- list(pos = list(38), command = NULL)

addtorow$command <- c("\\multicolumn{5}{X}{
\\footnotesize{\\textsuperscript{1} calibration window size: 1-7 mean serial intervals} \\newline 
\\footnotesize{\\textsuperscript{2} disease: Ebola (baseline), influenza (2), or SARS (3)} \\newline
\\footnotesize{\\textsuperscript{3} number of cases in calibration window} \\newline 
\\footnotesize{\\textsuperscript{4} projection window: the distance in mean serial intervals between the end of the calibration window and the end of the projection window} \\newline 
\\footnotesize{\\textsuperscript{5} type of prediction: all trajectories do not predict new cases (baseline), all trajectories predict new cases (1), some trajectories predict new cases (2)} \\newline 
\\footnotesize{\\textsuperscript{6} ratio of calibration window size to projection window number} \\newline 
\\footnotesize{\\textsuperscript{7} additive interaction between calibration window size and projection window}} \\\\\n")

tab <- xtable(lm_table, caption = "The coefficients of the linear regression models for explaining variation in the average residual, mean-square error, sharpness, and bias in forecasts of simulated outbreaks of Ebola, influenza, and Severe Acute Respiratory Syndrome (SARS). CI: confidence interval, MSE: mean-square relative error.", label = "metric_lm_table")

digits(tab) <- 2

align(tab) = c("p{0.1\\textwidth}", "p{0.1\\textwidth}", "p{0.18\\textwidth}", "p{0.34\\textwidth}", "p{0.13\\textwidth}", "p{0.15\\textwidth}")

print(tab, hline.after=c(-1, 0, 9, 19, 28, 37), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "bottom", type = "latex", sanitize.rownames.function = identity, sanitize.colnames.function = identity, sanitize.text.function = identity, add.to.row = addtorow, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@

% \textcolor{red}{When it comes to presenting p-values etc., I've presented the ones you get with summary(). Have I presented everything I need to present when it comes to the regression?}

<<metric_correlations, echo = FALSE, results = "hide", message = FALSE>>=
# Testing for correlations between metrics
# gives you the correlation coefficient
# res_mse_cor <- cor(x = total_table$residual, y = total_table$mse)
res_mse_cor <- cor.test(x = total_table$residual, y = total_table$mse)
res_sha_cor <- cor.test(x = total_table$residual, y = total_table$sharpness)
res_bia_cor <- cor.test(x = total_table$residual, y = total_table$bias)
mse_sha_cor <- cor.test(x = total_table$mse, y = total_table$sharpness)
mse_bia_cor <- cor.test(x = total_table$mse, y = total_table$bias)
sha_bia_cor <- cor.test(x = total_table$sharpness, y = total_table$bias)
@

<<sharpness_calculations, echo = FALSE, eval = TRUE, results = "hide">>=
# Proportion of prediction type by disease

# Ebola
ebola_1 <- sum(ebola_table$pred_type == 1) / nrow(ebola_table)
ebola_2 <- (sum(ebola_table$pred_type == 2) / nrow(ebola_table)) * 100

# SARS
sars_1 <- sum(sars_table$pred_type == 1) / nrow(sars_table)
sars_2 <- (sum(sars_table$pred_type == 2) / nrow(sars_table)) * 100

# Influenza
influenza_1 <- sum(influenza_table$pred_type == 1) / nrow(influenza_table)
influenza_2 <- (sum(influenza_table$pred_type == 2) / nrow(influenza_table)) * 100

@
\FloatBarrier

The most parsimonious linear regression models with an addition of interaction terms and where this lowered the AIC is presented for each of the four forecast performance metricsin Table \ref{metric_lm_table}. A comparison of the AICs of all the linear models can be found in Supplementary Information Table \ref{sim_metric_aic_table}. For the average residual and sharpness, the best model was one which contained all the explanatory variables and an additive interaction term between calibration window size and projection window number. For the MSE, the best model additionally included the ratio of calibration window size to projection window number. For bias, the best model included all explanatory variables excluding the prediction type of the forecast (whether or not the forecast includes the possibility of no incidence) and included the interaction term. Out of the metrics, the change in sharpness due to the explanatory variables is best explained by the linear regression model with an adjusted R-squared of 0.47, while the MSE was least explained with an adjuste R-squared of 0.06. As the interaction term between calibration window size and projection window number is statistically significant, it suggests that the effect of the projection window number is different for each of the calibration window sizes.

The correlations between metrics found that the correlation coefficient of the MSE and average residual was \Sexpr{round(res_mse_cor$estimate, 2)}, which means that MSE tends to decrease with increasing average residual. Bias also decreases with increasing average residual with a correlation coefficient of \Sexpr{round(res_bia_cor$estimate, 2)}. Sharpness, on the other hand, increases as the average residual increases with a correlation coefficient of \Sexpr{round(res_sha_cor$estimate, 2)} and decreases with increasing MSE (\Sexpr{round(mse_sha_cor$estimate, 2)}). Bias is positively correlated with MSE with a correlation coefficient of \Sexpr{round(mse_bia_cor$estimate, 2)}, while bias is negatively correlated with sharpness with a correlation coefficient of \Sexpr{round(sha_bia_cor$estimate, 2)}.

% \textcolor{red}{I'm feeling a bit insecure about how I should talk about the linear regression table. Any advice is appreciated. I was thinking that it's overkill to go through each explanatory and metric separately in a case like this, but when I've done regression analysis before for more traditional epidemiological coursework, we discussed all variables in painstaking detail - this isn't very interesting way of doing things for a paper-style report though.}

The sharpness, described as a measure of how narrow the range of predictions for a given time window are from a scale of 0 (wide) to 1 (narrow), varied by the types of predictions that were given within a single projection of a simulation (SI Fig. \ref{sharpness_type_plot}). There are three possible types of projections: ones where all of the projection's 10,000 projection trajectories include a prediction of a daily incidence of 0 cases, ones where some of the trajectories include a prediction of daily incidence of 0 cases, and ones where none of the trajectories include a prediction of a daily incidence of 0 cases.

For projections of simulations that contained an estimate of a daily incidence of 0, the sharpness varied more wildly than for projections, also dipping to lower sharpnesses than the projections that did not contain a daily incidence of 0, reflecting the possibility of a projection to spiral out of control and have a wide range of predictions for a given projection window (SI Fig. \ref{sharpness_type_plot}). Additionally, the distribution of sharpness scores within the zero-incidence-including groups was split into two for each simulated disease - either sharpness was very higher than that of the group that did not include any zero-incidence, or sharpness was lower than the majority of the sharpness distribution for the prediction type group that did not include zero-incidence (SI Fig. \ref{sharpness_type_plot}). For the Ebola simulations' forecasts, \Sexpr{round(ebola_2, 1)}\% contained zero-incidence, while \Sexpr{round(influenza_2, 1)}\% and \Sexpr{round(sars_2, 1)}\% of influenza and SARS simulations' forecasts contained the prediction of zero-incidence, respectively. The high proportion of forecasts containing zero-incidence suggests that for the calibration windows used in this study, the branching process model could often not be used to decipher whether the outbreak would undergo stochastic extinction or not based on the calibration windows used in this study.

\FloatBarrier
\newpage
\subsection{Empirical outbreaks}

%\subsubsection{Prediction metrics}

Daily incidence for the three empirical outbreaks, Ebola, influenza, and SARS, were forecasted and analysed using the same methods as those used for the simulated outbreaks. The incidence curves of the empirical outbreaks can be found in SI Figure \ref{empirical_plot}. Unlike for the simulated outbreaks, none of the projection trajectories for Ebola or influenza included the possibility of a daily incidence of zero for any of the calibration window and projection window combinations. For SARS, only the projections for the first calibration window included the possibility.

\begin{figure}[h]
<<all_real_metric_plot, echo = FALSE, fig.width = 8, fig.height = 10>>=
# Only want calibration windows 1, 2, and 4
sub_table <- filter(real_total_table, cali_window_size == 1 | cali_window_size == 2 | cali_window_size == 4) 

# Make a new column so that I get all the combinations into one graph
sub_table$cali_proj <- paste(sub_table$cali_window_size, ":", sub_table$proj_window_no, sep = "")

# Residual
residual_sub <- ggplot(sub_table, aes(x = cali_proj, y = residual, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# MSE
mse_sub <-  ggplot(sub_table, aes(x = cali_proj, y = mse, color = cali_window_size)) +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# Sharpness
sharpness_sub <- ggplot(sub_table, aes(x = cali_proj, y = sharpness, color = cali_window_size)) +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    coord_cartesian(ylim = c(0, 1)) +
                    labs(y = "Sharpness", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# Bias
bias_sub <- ggplot(sub_table, aes(x = cali_proj, y = bias, color = cali_window_size)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(~dataset, scales = "free") +
                    coord_cartesian(ylim = c(-1, 1)) +
                    labs(y = "Bias", x = "Calibration window size:projection window number") +
                    theme(legend.position = "none",
                          axis.text.x = element_text(angle = 45),
                          axis.title.x = element_blank())

# Call plot
multiplot(residual_sub, mse_sub, sharpness_sub, bias_sub)
@
\caption{How the four prediction metrics for empirical Ebola, influenza, and SARS outbreaks differ as the projection window's distance from the calibration window increases from 1 to 4 for calibration windows of size 1 (dark blue), 2 (blue), and 4 (light blue).}
\label{all_real_metric_plot}
\end{figure}

As can be seen in Figure \ref{all_real_metric_plot}, the branching process model fared worse at predicting daily incidence on the empirical outbreaks than on the simulated outbreaks according to the prediction metrics as exhibited by the scale of differences between the model's performance for different calibration and projection windows the the average residual and MSE. As with the simulated outbreaks, the full performance metrics for each calibration window size and projection window size combination is presented in the Supplementary Information (section 3.2).

The patterns of performance of the branching process model in forecasting incidence for the SARS outbreak is different than those of the Ebola and influenza outbreaks Figure \ref{all_metric_plot}. The SARS outbreak analysed here is one which had two clusters of cases with a period of few cases in between. The outbreak window observed here only covers the first cluster, and the branching process model did not perform well at predicting future incidence for the time period of lower incidence after the first cluster. The Ebola and influenza outbreaks analysed in this study were more clearly still in the initial phase of the outbreak, where daily incidence may still be increasing. 

As the model was not predicting zero incidence as often for the empirical outbreaks as for the simulated outbreaks, the sharpness metric performs more as one might have expected it to for the simulated outbreaks as well. For Ebola and influenza, sharpness decreases as the projection window moves further from the calibration window, reflecting increasing uncertainty in the bounds of daily incidence. Additionally, sharpness is seen to increase with increasing calibration window size. Sharpness starts off high for SARS as a forecast of no cases was a possibility. The sharpness then decreases from there as the forecast moves further into the future. From here, the sharpness of the forecasts increases as forecasts move to later dates.

Compared to the simulated outbreaks, the empirical Ebola and influenza outbreaks were overestimating the number of cases in the projection window more consistently, as can be deducted from observing the bias metric for Ebola and influenza (Fig. \ref{all_real_metric_plot}). The overestimation increased with as projection windows were further from calibration windows, as was also observed for the simulated outbreaks. SARS, on the other hand, underestimates incidence for the earlier calibration windows but then starts to overestimate incidence when 

%%%%%%%%%%%%%%%%
%% Discussion %%
%%%%%%%%%%%%%%%%

\newpage
\section{Discussion}
% \textcolor{red}{Thibaut and I didn't discuss my Discussion much either, but he would like it to answer questions such as "When does the model work?" "When didn't it work so well" "Are we good at estimating uncertainty?" "Are there any systematic biases?" "I have X amount of data - how far can I project? Can I trust what I have projected for the next serial interval"}

% \subsection{Summary of findings and general discussion}
Overall, the analysis of forecasted incidence based on simulated outbreaks of Ebola-like and influenza-like outbreaks found that a forecast is more trustworthy the longer the calibration window is and the closer projection days of interest are to the calibration window (Fig. \ref{all_metric_plot}). This same trend holds for the empirical Ebola and influenza outbreaks as well, but not for SARS (Fig. \ref{all_real_metric_plot}).   

% Discussion of performance metrics 
While the branching process model was better at forecasting daily incidence for simulated outbreaks according to the residual, MSE, and bias, the sharpness of the forecasts was arguably just as good or better for the empirical outbreaks' forecasts (Fig. \ref{all_metric_plot}, \ref{all_real_metric_plot}). This could be because the simulated outbreaks represented an ideal scenario where every case is reported, even the index case and the few sporadic cases that occurred before the outbreak started to gain momentum, unlike the empirical outbreaks for which the outbreak started at a point where there were already cases every day or nearly so (SI Fig. \ref{empirical_plot}). This meant that the simulated outbreaks had much uncertainty at the beginning of their outbreaks as to whether the outbreak would go extinct or not, resulting in many predictions having a wider scope of possible scenarios that could take place, especially as projection windows were taken further from the calibration windows.

Out of the performance metrics used in this study, the MSE may have been superfluous considering that the same trends could be seen with the average residual, though their scales were slightly different. Additionally, sharpness as a performance metric might be more informative for later stage outbreaks or outbreaks where the forecast suggests that the chances of the outbreak dying off are slim. For the simulated outbreaks, it may have been too early to evaluate sharpness in a coherent matter, as the forecasts were predicting the outbreak to die out.

While the branching process model had a tendency of overestimating the forecasted incidence of simulated outbreaks, this was exasperated for early calibration windows compared to later calibration windows and further projection windows in comparison to ones closer to the calibration window. The extent to which the reduction in overestimating with later calibration window size was due to there being more days' worth of cases or due to the outbreak being further along in its growth is a question that cannot be answered with the current model, as the force of infection calculation used for estimating the number of cases in the future does not work as intended if the contribution of cases from earlier on in the outbreak to the force of infection are ignored.

It is possible that Ebola-like outbreaks were observed to have the greatest overestimations in forecasted incidence due to them also having the largest outbreak sizes (SI Fig. \ref{sim_dist}). The average residual considers the absolute difference between the projection and true data rather than the relative difference. This means that if one model forecasts 20 cases when there were actually 10 and another 2 cases when there was 1, the average residual of the model that forecasted 20 cases will be worse than the one that predicted 2 cases even though both models forecasted two times too many cases.

The empirical SARS forecasts behaved differently from the Ebola and influenza forecasts and were quite far from the true incidence towards the end of the observation window. While retrospectively the reason for why this happened can explained through noting that this particular outbreak consisted of two peaks of cases instead of one, it does provide a reminder of the limitations of the branching process model used for this exercise. The model assumes that that the growth of the outbreak, if it happens, will be exponential. Also, a value for $R$ is drawn only once for the forecast, which means that the quality of the forecast relies on an absence of change in $R$. Applying this model to an outbreak that does not follow these criteria will understandably result in sub par projections. Others have also noted the difficulties faced by branching process modelling methods when encountering a non-exponentially growing outbreak \citep{Nouvellet2017, Viboud2017}.

% , much like applying Farr's law to the HIV epidemic \citep{Artzrouni1990}. The only problem with avoiding the wrong shape of outbreak for the tool in question is that when it comes to real-time forecasting of outbreaks, its shape is not known in advance. 

% The early Ebola and influenza outbreaks were of a more suitable shape to be modelled by the branching process model in this study than that of SARS (SI Fig. \ref{empirical_plot}).

% Limitations
One of the limitations of this study is that the performance of the forecasting method is mostly evaluated under ideal circumstances on simulated outbreaks that have been created using a similar model. This means that abnormalities, such as unexpected growth curves, are not a concern and are not considered. Regardless, as the performance of the forecasting method used in this study has not been evaluated before, it can be considered reasonable to start from evaluating its performance for the ideal scenario.

There is also a loss of information regarding forecast performance when it is considered for a time window rather than a number of days individually.

% Generalised advice
Given a scenario with minimal noise, such as when forecasting daily incidence of simulated outbreaks, the distributions of average residuals and bias scores in my results suggest that the true incidence is included in the 95\% projection interval for forecasts that are up to four mean serial intervals' worth of days in length given that the calibration window's size is at least two mean serial intervals (Fig. \ref{all_metric_plot}, SI Fig. \ref{whole_residual},  \ref{whole_bias}). The extent of incidence overprediction does worsen with increasing forecast length and the risk of a grossly overestimating forecast increases with the length of the forecast (SI Fig. \ref{whole_mse}).

Based on the empirical outbreaks analysed in this study, the aforementioned guidelines for maintaining reasonable forecast performance may not apply for empirical outbreaks but would likely be more conservative. The average residuals of the the more suitably-shaped Ebola and influenza outbreaks' forecasts became overestimated at a greater scale than those of the simulated outbreaks (Fig. \ref{all_metric_plot}, \ref{all_real_metric_plot}, SI Fig. \ref{whole_residual}, \ref{real_whole_residual}). Additionally, the forecasts for empirical outbreaks tended to overestimate incidence occasionally reaching a full bias of 1, which was not seen in the simulated outbreaks' forecasts (SI Fig. \ref{whole_bias}, \ref{real_whole_bias}). 

In future work, the impact of factors such as under-reporting, reporting delays, or super-spreading on the performance of the branching process model could be investigated to identify further possible precautions that should be kept in mind when forecasting. The branching process model's performance on a wider array of empirical outbreaks could be tested and regression analysis could be performed on empirical outbreaks as well. In addition, the forecasts' performance for varying splits of time window, such as weeks rather than mean serial intervals or merely individual days could be investigated.

%Points to discuss:
%\begin{itemize}
%  \item Sharpness gets better with more calibration windows regardless of the forecast being in the wrong place.
%\end{itemize}

% Forecasting further than three serial intervals onward based on one serial interval's worth of data resulted in an overestimation of the forecasted disease incidence for Ebola-like and influenza-like disease, and to a lesser extent, SARS-like disease (Fig. \ref{all_metric_plot}, SI section 1.3).

% \subsection{Limitations}
% 
% 
% \begin{itemize}
%   \item In this study I only look at the performance of the branching process model under ideal circumstances, where the serial interval is known and issues such as reporting delays do not exist. On top of this, the simulations that I am running my predictive branching process model on also come from a branching process model.
%   \item The projections are based on a branching process model assuming an infinite pool of susceptibles. This assumption is alright for early outbreak analysis, where depletion of susceptibles isn't that much of an issue.
%   \item Real outbreaks can miss the first few weeks of there not being that many cases due to surveillance methods not being in place. The issue with using real historical outbreak data (especially old data) is that the outbreaks that get recorded and remembered tend to be exceptional. This means that small outbreaks are hard to come by and that models that work nicely for early outbreak surveillance may not work well for these well-documented freak cases. This idea is tentatively supported by for example by the proportion of projection windows containing forecasts of zero-incidence. Two of the three real outbreaks barely had a time window suggesting the possibility of zero incidence while the majority of the simulated outbreaks did contain a possibility of a daily incidence of zero. 
% \end{itemize}
% 
% \subsection{Future work}

% \textcolor{red}{I feel like I should be comparing my findings to other peoples' work and reference some papers in here but I have not done that yet.}

\subsection{Conclusions}

\textcolor{red}{Conclusions here}

%%%%%%%%%%%%%%%%
%% References %%
%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{unsrtnat}
\begin{singlespace}
\begin{group}
   % \fontsize{11pt}
  \bibliography{/home/evelina/Documents/Mendeley/MRes_forecasting.bib}
\end{group}
\end{singlespace}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary information %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\etocsettagdepth{mtappendix}{subsubsection}
% \etocsettagdepth{mtappendix}{figure}
% \etocsettagdepth{mtappendix}{table}

\renewcommand\thefigure{\thesection.\arabic{figure}} 
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{figure}{0}   
\setcounter{equation}{0}
\renewcommand{\thesection}{\arabic{section}}
\newpage
\section*{Supplementary Information}
% \addcontentsline{toc}{section}{Supplementary Information}

%% Table of Contents
\subsection*{Contents}
\vspace{-4em}
\tableofcontents
% \listoffigures
% \addcontentsline{toc}{subsection}{\listfigurename}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\setcounter{section}{0}
\section{Expanded Methods}
\subsection{Forecasting incidence with a branching process model}

% In order to obtain a forecast of daily incidence, first the force of infection for the observed time window was calculated, which was then used to calculate the likelihoods of varying values of the effective reproduction number, $R$. This $R$ likelihood distribution was then sampled from to obtain plausible values of $R$ used for projection purposes. This in turn is then used to predict force of infection and thus the number of new cases.

\subsubsection{Inferring the force of infection and R}
First the observed daily incidence (cases per day) and known estimates of the serial interval distribution were used to obtain a likelihood distribution for the effective reproduction number, $R$. Here it is assumed that the serial interval follows a discrete Gamma distribution. 

Daily incidence $x_{t}$ can be described as:
\begin{equation}
x_{t} \sim \mathcal{P}(\lambda_{t})
\end{equation}
where $P(\lambda_{t})$ is the Poisson process determined by the total force of infection for a given day, $\lambda_{t}$ \citep{Cori2013, Jombart2017}.

The force of infection for a day $t$ is then calculated as the sum of individual forces of infection for that day:
\begin{equation}
\lambda_{t} = R \times \sum_{k = 1}^{t}x_{k}w(t - k)
\end{equation}
Here $w()$ is the discrete serial interval distribution (probability mass function) for day $k$ and $x$ is the incidence on a given day.

The likelihood of observing incidence $x$ given the value of $R$ in turn incorporates the Poisson probability mass function $F_{\mathcal{P}}$ and is expressed as:
\begin{equation}
\mathcal{L}(x) = p(x|R) = \prod_{t = 1}^{T}F_{\mathcal{P}}(x_{t}, \lambda_{t})
\end{equation}

This can then be used to obtain a maximum-likelihood estimation of $R$ and the created likelihood distribution of $R$ can be sampled from to obtain plausible values of $R$ for forecasting incidence. This method of inferring $R$ is implemented in the $earlyR$ R package \citep{Jombart2017}. 

% \textcolor{red}{sample\_R}

% Though there was no data on who infected whom, this could be inferred by calculating for each pair of cases the relative likelihood that one was infected by the other \citep{Wallinga2004}. Thus for a given case $j$, the effective reproduction number can be estimated as the sum of the likelihoods $p$ that given cases $i$ were infected by case $j$:
% \begin{equation}
% R_{j} = \sum_{i}{p_{ij}}
% \end{equation}
% 
% \textcolor{red}{get\_R}
% 
% Estimating the case reproduction numbers for cases $j$ in the calibration window produces a distribution of values of $R$ \citep{Cori2013}. This estimation of $R$ is then utilised to estimate the incidence for the day directly after the end of the calibration window. 
% The log-likelihood for a range of $R$s based on the computed force of infection can then be calculated:
% sum(stats::dpois(x[-1], lambda = R * x_lambdas, log = TRUE))
% x is daily incidence
% x_lambdas comes from overall_infectivity
% \textcolor{red}{What I want to say: sum(stats::dpois(x[-1], lambda = R * x\_lambdas, log = TRUE))}
% \begin{equation}
% p(R) = \sum_{t = 1} log(Pois(x_{t} | R \times \lambda_{t}))
% \end{equation}
% \textcolor{red}{make sure I converted code into equation correctly}

\subsubsection{Forecasting incidence}
\textcolor{red}{project}

To forecast disease incidence using a branching process model, such as the one used in the \textit{projections}-package, the following steps are taken.

The combination of observed daily incidence, serial interval distribution, and sample of plausible values of $R$ can then be used to forecast daily incidence using a branching process. Here daily incidence follows a Poisson process, determined by the daily infectiousness at time $t$, $\lambda_{t}$. $\lambda_{t}$ can be calculated in a similar manner as the force of infection in a window of observed incidence: 

% The force of infection on a given day is the sum of individual forces of infection. The invididual force of infection is computed as the R0 multiplied by the pmf of the serial interval for the corresponding day:
% lambda_{i,t} = R0 w(t - t_i)
% where 'w' is the PMF of the serial interval and 't_i' is the date of onset of case 'i'.
% R values are constant within simulations, so drawn once for all simulations (fix_within = TRUE)

% projections
\begin{equation}
\lambda_{t} = \sum_{k = 1}^{t-1}I_{k}w(t-k)
\end{equation}
where the product of the incidence at time $s$ and the discretised serial interval distribution (probability mass function, $w$) for the time period between time $t$ and $s$ is summed for all days from day $s$ until day $t - 1$.

This force of infection for forecasting day $t$ in conjunction with an $R$ sampled from the likelihood distribution is then used to simulate a daily incidence, $i$ at a future date $t$ with a Poisson process.

\textcolor{red}{what I want to say: stats::rpois(n\_sim, R * lambda)}
\begin{equation}
i_{t} =
\end{equation}

For the following projection days, the forecasted daily incidence during the previous infection days is taken into account when calculating the force of infection.

In R, this projection method can be implemented with the $project$-function of the $projections$ package \citep{Jombart2018}.

% This function simulates future incidence based on past incidence data, a selection of plausible reproduction numbers (R), and the distribution of the serial interval (time from primary onset to secondary onset).
% Explain how branching process models work. Take the calibration window, estimate the likelihood distribution of the reproduction number of the observed incidence.
% \textcolor{red}{Mention that the one used here is found in the \textit{projections}-package.}

\FloatBarrier
\newpage
\section{Simulated outbreaks}
\subsection{Distribution of total outbreak size}
\begin{figure}[h]
\begin{center}
<<sim_dist_plot, echo = FALSE, fig.width = 8, fig.height = 6.5, out.width=".8\\textwidth">>=
ebola_dist_table <- ebola_table %>% group_by(dataset) %>% summarise(size = median(outbreak_size))
influenza_dist_table <- influenza_table %>% group_by(dataset) %>% summarise(size = median(outbreak_size))
sars_dist_table <- sars_table %>% group_by(dataset) %>% summarise(size = median(outbreak_size))

ebola_sim_dist <- ggplot(ebola_dist_table, aes(x = size)) +
                    geom_histogram(alpha = 0.4, binwidth = 7, position = "identity") + 
                    labs(y = "Frequency", x = "Outbreak size") +
                    # facet_wrap(~disease, scales = "free") +
                    theme(legend.position = "none") +
                    annotate("text", label = "A", x = 600, y = 6.5)

flu_sim_dist <- ggplot(influenza_dist_table, aes(x = size)) +
                    geom_histogram(alpha = 0.4, binwidth = 0.7, position = "identity") + 
                    labs(y = "Frequency", x = "Outbreak size") +
                    # facet_wrap(~disease, scales = "free") +
                    theme(legend.position = "none") +
                    annotate("text", label = "B", x = 48, y = 9.5)

sars_sim_dist <- ggplot(sars_dist_table, aes(x = size)) +
                    geom_histogram(alpha = 0.4, binwidth = 1.5, position = "identity") + 
                    labs(y = "Frequency", x = "Outbreak size") +
                    # facet_wrap(~disease, scales = "free") +
                    theme(legend.position = "none") +
                    annotate("text", label = "C", x = 140, y = 6)

multiplot(ebola_sim_dist, flu_sim_dist, sars_sim_dist)

@
\caption[Outbreak size distribution]{Distribution of outbreak sizes for the 80 simulated outbreaks of Ebola-like (A), influenza-like (B), and severe acute respiratory syndrome (SARS)-like diseases (C). Each simulation was run for 8 times each respective disease's serial intervals rounded to the nearest day: 12, 9, and 3 days for Ebola, influenza, and SARS respectively.}
\label{sim_dist}
\end{center}
\end{figure}

\FloatBarrier
\newpage
\subsection{Full prediction metrics}
\subsubsection{Average residual}
\begin{figure}[h]
\begin{center}
<<whole_residual, echo = FALSE, cache = FALSE, fig.width = 8, fig.height = 6.5, warning = FALSE, out.width=".8\\textwidth">>=
residual_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(residual), std = sd(residual), q1 = quantile(residual, probs = 0.025), q2 = quantile(residual, probs = 0.25), q3 = quantile(residual, probs = 0.75), q4 = quantile(residual, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

residual_massive <- ggplot(residual_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "Residual", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

residual_massive2 <- ggplot(residual_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Residual", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +


# Print the figure
residual_massive2
@
\caption[Average residual]{The medians (horizontal line in box), interquartile ranges (box), 95\% quantile ranges (whiskers) of the average residual for all seven calibration window sizes presented separately for each projection window number and disease, as indicated by the labels on top of each panel of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases. The projections tend to approach the ideal average residual of 0 with increasing calibration window size, though note that the scale of average residuals varies with disease and projection window size as indicated by the individual y-axis labels. Additionally, the average residuals tend to be negative, implying overestimation of daily disease incidence.}
\label{whole_residual}
\end{center}
\end{figure}
\FloatBarrier

\clearpage
\subsubsection{Mean-square relative error}
\begin{figure}[h]
\begin{center}
<<whole_mse, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5, warning = FALSE, out.width=".8\\textwidth">>=
mse_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(mse), std = sd(mse), q1 = quantile(mse, probs = 0.025), q2 = quantile(mse, probs = 0.25), q3 = quantile(mse, probs = 0.75), q4 = quantile(mse, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

mse_massive <- ggplot(mse_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "MSE", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

mse_massive2 <- ggplot(mse_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "MSE", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

# Print the figure
mse_massive2
@
\caption[Mean-square relative error]{The medians (horizontal line in box), interquartile ranges (box), 95\% quantile ranges (whiskers) of the mean-squared relative error (MSE) for all seven calibration window sizes presented separately for each projection window number and disease, as indicated by the labels on top of each panel of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases. The scale MSE varies by disease and size of projection window as indicated by the individual y-axes for each panel, the simulated Ebola outbreaks exhibit the highest MSE, followed by influenza and then SARS. MSE tends to increase and the 95\% quantile range tends to widen as the projection window is increasingly further from the calibration window for all diseases.}
\label{whole_mse}
\end{center}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Sharpness}
\begin{figure}[h]
\begin{center}
<<whole_sharpness, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5, warning = FALSE, out.width=".8\\textwidth">>=
sharpness_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.025), q2 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75), q4 = quantile(sharpness, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

sharpness_massive <- ggplot(sharpness_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "Sharpness", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(0, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

sharpness_massive2 <- ggplot(sharpness_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Sharpness", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(0, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

# Print the figure
sharpness_massive2
@
\caption[Sharpness]{The medians (horizontal line in box), interquartile ranges (box), 95\% quantile ranges (whiskers) of the sharpness for all seven calibration window sizes presented separately for each projection window number and disease, as indicated by the labels on top of each panel of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases. Sharpness is measured on a scale from 0 to 1, where 1 is perfect sharpness. Median sharpness tends to drop with increasing calibration window size and increasing distance of projection window from calibration window.}
\label{whole_sharpness}
\end{center}
\end{figure}
\FloatBarrier

\begin{figure}[h]
<<sharpness_plot_total, echo = FALSE, fig.width = 7, fig.height = 3>>=
library(ggplot2)
text_size <- 5

# RMSE plot with calibration window to projection window ratio
sharpness_table <- total_table %>% group_by(disease, pred_type) %>% summarise(avg = median(sharpness), std = sd(sharpness), q1 = quantile(sharpness, probs = 0.25), q3 = quantile(sharpness, probs = 0.75))
                          
sharpness_type <- ggplot(total_table, aes(factor(pred_type),
                     sharpness, color = factor(disease), fill = factor(disease))) +
                     coord_cartesian(ylim = c(0, 1)) +
                     # scale_y_log10() +
                     facet_wrap(~disease) +
                     # geom_pointrange(position = position_dodge(width = 0.5), size = 0.3) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     geom_boxplot(position = position_dodge(width = 1), width = 0.1, fill = "white", color = "black") +
                     labs(y = "Sharpness", x = "Prediction group type") +
                     theme(legend.position = "none") +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))

# Table that I print
sharpness_type
@
\caption[Sharpness by prediction type]{Sharpness by prediction type for simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and H1N1 influenza. The different prediction types include ones which have trajectories predicting only zero incidence ("only 0"), no trajectories predicting zero incidence ("no 0"), or some trajectories predicting zero incidence ("include 0").}
\label{sharpness_type_plot}
\end{figure}
\FloatBarrier

\clearpage
\subsubsection{Bias}
\begin{figure}[h]
\begin{center}
<<whole_bias, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5, warning = FALSE, out.width=".8\\textwidth">>=
bias_iqr <- total_table %>% group_by(disease, cali_window_size, proj_window_no) %>% summarise(med = median(bias), std = sd(bias), q1 = quantile(bias, probs = 0.025), q2 = quantile(bias, probs = 0.25), q3 = quantile(bias, probs = 0.75), q4 = quantile(bias, probs = 0.975))

facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

facet_labeller <- function(variable,value){
  if (variable == "facet1") {
    return(facet1_names[value])
  } else if (variable == "facet2") {
    return(facet2_names[value])
  } else {
    return(as.character(value))
  }
}

bias_massive <- ggplot(bias_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_pointrange(size = 0.5, shape = 1) +
                    facet_wrap(disease~proj_window_no, scales = "free") +
                    labs(y = "Bias", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(-1, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

bias_massive2 <- ggplot(bias_iqr, aes(cali_window_size, y = med, ymin = q1, ymax = q3, color = disease)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_boxplot(aes(ymin = q1, lower = q2, middle = med, upper = q3, ymax = q4, width = 0.5), stat = "identity") +
                    facet_wrap(disease~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Bias", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(-1, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +

# Print the figure
bias_massive2
@
\caption[Bias]{The medians (horizontal line in box), interquartile ranges (box), 95\% quantile ranges (whiskers) of the bias for all seven calibration window sizes presented separately for each projection window number and disease, as indicated by the labels on top of each panel of Ebola-like (red), influenza-like (green), and SARS-like (blue) diseases. Bias is measured on a scale from -1 to 1, where -1 implies that all stochastic trajectories of a projection are underestimating incidence and 1 implies that all stochastic trajetories of a projection are overestimating incidence. The 95\% quantile range for bias tends to widen with increasing projection window. The median bias tends to reduce with increasing calibration window size for Ebola-like and influenza-like outbreaks.}
\label{whole_bias}
\end{center}
\end{figure}
\FloatBarrier

\newpage
\subsection{Distribution of metrics}
\begin{figure}[h]
<<dist_plot, echo = FALSE, fig.width = 8, fig.height = 7.5>>=
new_mse_table <- total_table %>% mutate(mse_new = ifelse(mse > 30, 30, mse))
new_residual_table <- total_table %>% mutate(residual_new = ifelse(residual < -20, -20, residual))

mse_dist <- ggplot(new_mse_table, aes(x = mse_new, fill = factor(disease, levels = c("influenza", "sars", "ebola")))) +
                    geom_histogram(alpha = 0.4, binwidth = 0.5, position = "identity") + 
                    labs(y = "Frequency", x = "MSE") +
                    theme(legend.position = "none") +
                    guides(fill = guide_legend(reverse = TRUE))

residual_dist <- ggplot(new_residual_table, aes(x = residual_new, fill = disease)) +
                        geom_histogram(alpha = 0.4, binwidth = 0.5, position = "identity") + 
                        labs(y = "Frequency", x = "Average Residual") +
                        theme(legend.position = "none")

sharp_dist <- ggplot(total_table, aes(x = sharpness, fill = disease)) +
                     geom_histogram(alpha = 0.4, binwidth = 0.02, position = "identity") + 
                     labs(y = "Frequency", x = "Sharpness") +
                     theme(legend.position = "none")

bias_dist <- ggplot(total_table, aes(x = bias, fill = disease)) +
                     geom_histogram(alpha = 0.4, binwidth = 0.05, position = "identity") + 
                     labs(y = "Frequency", x = "Bias") +
                     theme(legend.position = "none")

bias_freq <- ggplot(total_table, aes(bias, colour = disease)) +
                    geom_freqpoly(binwidth = 0.05)

multiplot(mse_dist, residual_dist, sharp_dist, bias_dist)

@
\caption[Maybe useless]{The distributions of prediction metrics for Ebola (red), influenza (green), and SARS (blue).}
\label{dist_plot}
\end{figure}

\FloatBarrier
\newpage
\subsection{Zero-incidence}
\begin{figure}[h]
<<pred_case_plot, echo = FALSE, fig.width = 7, fig.height = 3>>=
# Convert days since case to SIs since case
total_table$si[total_table$disease == "ebola"] <- 11.6
total_table$si[total_table$disease == "influenza"] <- 2.6
total_table$si[total_table$disease == "sars"] <- 8.7
total_table$si_since_case <- total_table$days_since_case / total_table$si

pred_case_fig <- ggplot(total_table, aes(factor(pred_type),
                     si_since_case, color = factor(disease), fill = factor(disease))) +
                     facet_wrap(~disease, scales = "free") +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     geom_boxplot(width = 0.2, fill = "white", color = "black", position = position_dodge(width = 1)) +
                     labs(y = "Serial intervals since observed case", x = "Prediction group type") +
                     coord_cartesian(ylim = c(0, 8)) +
                     theme(legend.position = "none") +
                     scale_x_discrete(labels = c("only 0", "no 0", "include 0"))

pred_case_scatter <- ggplot(total_table, aes(si_since_case, prop_zero, color = factor(disease), fill = factor(disease))) +
                     facet_wrap(~disease, scales = "free") +
                     geom_point() +
                     labs(y = "Proportion predicting zero-incidence", x = "Serial intervals since last observed case") +
                     coord_cartesian(xlim = c(0, 8)) +
                     theme(legend.position = "none")

pred_case_scatter
@
\caption[Maybe useless too]{The proportion of trajectories for each projection predicting a daily incidence of 0 for a given projection window by the number of serial intervals that have passed between the last observed case and the end of the calibration window for Ebola (red), influenza (green), and SARS (blue).}
\label{pred_case_plot}
\end{figure}

\begin{figure}[h]
<<zero_pred_plot, echo = FALSE, fig.width = 7, fig.height = 3>>=
zero_pred_scatter <- ggplot(total_table, aes(no_proj_cases, prop_zero, color = factor(disease), fill = factor(disease))) +
                     facet_wrap(~disease, scales = "free") +
                     geom_point() +
                     labs(y = "Proportion predicting zero-incidence", x = "Number of cases in prediction window") +
                     # coord_cartesian(xlim = c(0, 8)) +
                     theme(legend.position = "none")

zero_pred_scatter
@
\caption{The proportion of trajectories for each projection predicting a daily incidence of 0 for a given projection window compared to the true number of cases in that prediction window for Ebola (red), influenza (green), and SARS (blue).}
\label{zero_pred_plot}
\end{figure}

\begin{figure}[h]
<<cali_zero_table, echo = FALSE, fig.width = 8, fig.height = 9>>=
facet1_names <- c(
  "1" = "Calibration 1",
  "2" = "Calibration 2",
  "3" = "Calibration 3",
  "4" = "Calibration 4",
  "5" = "Calibration 5",
  "6" = "Calibration 6",
  "7" = "Calibration 7"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

scaleFUN <- function(x) sprintf("%.0f", x) # Function for having no decimal points in y-axis

cali_zero_plot <- ggplot(total_table, aes(factor(pred_type), no_cali_cases, fill = factor(disease), color = factor(disease))) + # , fill = factor(disease)
                     # coord_cartesian(ylim = c(0, 50)) +
                     facet_grid(cali_window_size~proj_window_no, scale = "free", labeller = labeller(
                      cali_window_size = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                     geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1)) +
                     # geom_boxplot(position = position_dodge(width = 1), width = 0.2, fill = "white", color = "black") +
                     labs(y = "Number of cases in calibration window", x = "Prediction type") +
                     scale_y_continuous(labels = scaleFUN) +
                     scale_x_discrete(breaks = c("0", "1", "2"), labels = c("only 0", "no 0", "includes 0")) +
                     theme(legend.position = "none",
                           axis.text.x = element_text(angle = 25))

cali_zero_plot
@
\caption[Number of cases in calibration window and prediction type]{The number of cases in a calibration window for each prediction type split by calibration window size (top number in headers over panels) and projection window (bottom number in headers over panels) - projections with trajectories that predict only a daily incidence of zero cases are referred to as "0", only non-zero cases as "1", and including trajectories of zero incidece "2" for Ebola (red), influenza (green), and SARS (blue).}
\label{cali_zero_plot}
\end{figure}

\FloatBarrier
\newpage
\subsection{Linear regression: model comparison}
<<sim_metric_aic_table, echo = FALSE, results = "asis">>= 
library(xtable)

aic_table <- array(NA, dim =c(17, 4))

# disease names
aic_table[1, 1] <- "Residual"
aic_table[5, 1] <- "MSE"
aic_table[9, 1] <- "Sharpness"
aic_table[13, 1] <- "Bias"

# model names
aic_table[c(1, 5, 9, 13), 2] <- "Basic \\textsuperscript{1}"
aic_table[c(2, 6, 10, 14), 2] <- "Basic + ratio \\textsuperscript{2}"
aic_table[c(3, 7, 11, 15), 2] <- "Basic + interaction \\textsuperscript{3}"
aic_table[c(4, 8, 12, 16), 2] <- "Basic + ratio + interaction \\textsuperscript{4}"

# degrees of freedom
aic_table[1:4, 3] <- residual_aic$df[1:4]
aic_table[5:8, 3] <- mse_aic$df[1:4]
aic_table[9:12, 3] <- sharpness_aic$df[1:4]
aic_table[13:16, 3] <- bias_aic$df[1:4]

# AIC
aic_table[1:4, 4] <- round(residual_aic$AIC[1:4], digits = 2)
aic_table[5:8, 4] <- round(mse_aic$AIC[1:4], digits = 2)
aic_table[9:12, 4] <- round(sharpness_aic$AIC[1:4], digits = 2)
aic_table[13:16, 4] <- round(bias_aic$AIC[1:4], digits = 2)

colnames(aic_table) <- c("Metric", "Model", "df", "AIC")

addtorow <- list(pos = list(17), command = NULL)

addtorow$command <- c("\\multicolumn{4}{X}{\\textsuperscript{1} A basic model consisting of calibration window size, disease (Ebola, influenza, SARS), number of cases in the calibration window, projection window number, type of predictions in the projection window (all projections show no cases, some projections with no cases, all projections show cases) \\newline
\\textsuperscript{2} The basic model and ratio of calibration window size to projection window number  \\newline 
\\textsuperscript{3} The basic model and additive interaction between calibration window size and projection window number \\newline 
\\textsuperscript{4} The basic model incorporation the ratio between windows and additive interaction \\newline} 
\\\\\n")

tab <- xtable(aic_table, digits = 2, caption = "Comparisons of linear models for explaining variation in the average residuals, mean-square error (MSE), sharpness, and bias of simulated outbreaks of Ebola, Severe Acute Respiratory Syndrome (SARS), and influenza. df: degrees of freedom, AIC: Akaike Information Criterion.", label = "sim_metric_aic_table")

# align(tab) <- "lXX{2cm}XX"
align(tab) = c("p{0.2\\textwidth}", "p{0.2\\textwidth}", "p{0.4\\textwidth}", "p{0.15\\textwidth}", "p{0.2\\textwidth}")

print(tab, hline.after=c(-1, 0, 16), comment = FALSE, math.style.exponents = FALSE, include.rownames = FALSE, caption.placement = "bottom", type = "latex", sanitize.rownames.function = identity, sanitize.colnames.function = identity, sanitize.text.function = identity, add.to.row = addtorow, tabular.environment = "tabularx", width = "\\textwidth", size = "\\small")
@


\FloatBarrier
\newpage
\section{Empirical outbreaks}
\subsection{Daily incidence of outbreaks}
<<real_projections, eval = TRUE, echo = FALSE, message = "hide", warning = FALSE>>=
library(epitrix)
library(distcrete)
library(incidence)
library(earlyR)
library(projections)
library(EpiEstim)
library(outbreaks)

set.seed(1)
# Ebola
setwd("/home/evelina/Development/forecasting/data/")
ebola_sl <- read.csv("sierraleone_ebola_2014_clean.csv")
ebola_sl$date <- as.Date(ebola_sl$date, format = "%d/%m/%Y")
ebola_i <- as.incidence(ebola_sl$new_cases, ebola_sl$date, interval = 1)
setwd("/home/evelina/Development/forecasting/simulations/real_outbreaks/ebola/")
delta <- 12

# Get serial interval and R calculation
# cv = sigma / mean 
ebola_sim_si <- gamma_mucv2shapescale(11.6, (5.6/11.6))
ebola_si <- distcrete("gamma", shape = ebola_sim_si$shape, scale = ebola_sim_si$shape, w = 0, interval = 1)
ebola_R1 <- get_R(ebola_i[1:(delta * 1), ], si = ebola_si, max_R = 10)
ebola_R2 <- get_R(ebola_i[1:(delta * 2), ], si = ebola_si, max_R = 10)
ebola_R3 <- get_R(ebola_i[1:(delta * 3), ], si = ebola_si, max_R = 10)
ebola_R4 <- get_R(ebola_i[1:(delta * 4), ], si = ebola_si, max_R = 10)
ebola_R5 <- get_R(ebola_i[1:(delta * 5), ], si = ebola_si, max_R = 10)
ebola_R6 <- get_R(ebola_i[1:(delta * 6), ], si = ebola_si, max_R = 10)
ebola_R7 <- get_R(ebola_i[1:(delta * 7), ], si = ebola_si, max_R = 10)

# Projections
ebola_proj1 <- project(ebola_i[1:delta, ], R = sample_R(ebola_R1, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj2 <- project(ebola_i[1:(delta*2), ], R = sample_R(ebola_R2, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj3 <- project(ebola_i[1:(delta*3), ], R = sample_R(ebola_R3, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj4 <- project(ebola_i[1:(delta*4), ], R = sample_R(ebola_R4, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
ebola_proj5 <- project(ebola_i[1:(delta*5), ], R = sample_R(ebola_R5, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 3), R_fix_within = TRUE)
ebola_proj6 <- project(ebola_i[1:(delta*6), ], R = sample_R(ebola_R6, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 2), R_fix_within = TRUE)
ebola_proj7 <- project(ebola_i[1:(delta*7), ], R = sample_R(ebola_R7, 1000), si = ebola_si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)

# Influenza
data("Flu1918")
flu_1918 <- Flu1918
flu_i <- as.incidence(flu_1918$incidence, interval = 1)

delta <- 3

# Get serial interval and R calculation
# cv = sigma / mean 
flu_sim_si <- gamma_mucv2shapescale(2.6, (1.5/2.6))
flu_si <- distcrete("gamma", shape = flu_sim_si$shape, scale = flu_sim_si$shape, w = 0, interval = 1)
flu_R1 <- get_R(flu_i[1:(delta * 1), ], si = flu_si, max_R = 10)
flu_R2 <- get_R(flu_i[1:(delta * 2), ], si = flu_si, max_R = 10)
flu_R3 <- get_R(flu_i[1:(delta * 3), ], si = flu_si, max_R = 10)
flu_R4 <- get_R(flu_i[1:(delta * 4), ], si = flu_si, max_R = 10)
flu_R5 <- get_R(flu_i[1:(delta * 5), ], si = flu_si, max_R = 10)
flu_R6 <- get_R(flu_i[1:(delta * 6), ], si = flu_si, max_R = 10)
flu_R7 <- get_R(flu_i[1:(delta * 7), ], si = flu_si, max_R = 10)

# Projections
flu_proj1 <- project(flu_i[1:delta, ], R = sample_R(flu_R1, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj2 <- project(flu_i[1:(delta*2), ], R = sample_R(flu_R2, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj3 <- project(flu_i[1:(delta*3), ], R = sample_R(flu_R3, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj4 <- project(flu_i[1:(delta*4), ], R = sample_R(flu_R4, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
flu_proj5 <- project(flu_i[1:(delta*5), ], R = sample_R(flu_R5, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 3), R_fix_within = TRUE)
flu_proj6 <- project(flu_i[1:(delta*6), ], R = sample_R(flu_R6, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 2), R_fix_within = TRUE)
flu_proj7 <- project(flu_i[1:(delta*7), ], R = sample_R(flu_R7, 1000), si = flu_si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)

# SARS
data("sars_canada_2003")
sars_2003 <- sars_canada_2003
sars_2003$total_cases <- rowSums(sars_2003[ , 2:5])
sars_2003$date <- as.Date(sars_2003$date)
sars_i <- as.incidence(sars_2003$total_cases, sars_2003$date, interval = 1)

delta <- 9

# Get serial interval and R calculation
# cv = sigma / mean 
sars_sim_si <- gamma_mucv2shapescale(8.7, (3.6/8.7))
sars_si <- distcrete("gamma", shape = sars_sim_si$shape, scale = sars_sim_si$shape, w = 0, interval = 1)
sars_R1 <- get_R(sars_i[1:(delta * 1), ], si = sars_si, max_R = 10)
sars_R2 <- get_R(sars_i[1:(delta * 2), ], si = sars_si, max_R = 10)
sars_R3 <- get_R(sars_i[1:(delta * 3), ], si = sars_si, max_R = 10)
sars_R4 <- get_R(sars_i[1:(delta * 4), ], si = sars_si, max_R = 10)
sars_R5 <- get_R(sars_i[1:(delta * 5), ], si = sars_si, max_R = 10)
sars_R6 <- get_R(sars_i[1:(delta * 6), ], si = sars_si, max_R = 10)
sars_R7 <- get_R(sars_i[1:(delta * 7), ], si = sars_si, max_R = 10)

# Projections
sars_proj1 <- project(sars_i[1:delta, ], R = sample_R(sars_R1, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj2 <- project(sars_i[1:(delta*2), ], R = sample_R(sars_R2, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj3 <- project(sars_i[1:(delta*3), ], R = sample_R(sars_R3, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj4 <- project(sars_i[1:(delta*4), ], R = sample_R(sars_R4, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 4), R_fix_within = TRUE)
sars_proj5 <- project(sars_i[1:(delta*5), ], R = sample_R(sars_R5, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 3), R_fix_within = TRUE)
sars_proj6 <- project(sars_i[1:(delta*6), ], R = sample_R(sars_R6, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 2), R_fix_within = TRUE)
sars_proj7 <- project(sars_i[1:(delta*7), ], R = sample_R(sars_R7, 1000), si = sars_si, 
                  n_sim = 10000, n_days = (delta * 1), R_fix_within = TRUE)
@

\begin{figure}[h]
<<empirical_plot, eval = TRUE, echo = FALSE, fig.width = 8, fig.height = 3, fig.align = "center">>=
library(incidence)
library(EpiEstim)
library(outbreaks)
library(ggplot2)

set.seed(1)
# Ebola incidence
# load("proj_window_1.RData")
# ebola_projection_1 <- proj_window
ebola_plot <- plot(ebola_i[1:(8*12), ]) 
# %>% add_projections(ebola_proj1, quantiles = FALSE) %>% add_projections(ebola_proj2, quantiles = FALSE) %>% add_projections(ebola_proj3, quantiles = FALSE) %>% add_projections(ebola_proj4, quantiles = FALSE) %>% add_projections(ebola_proj5, quantiles = FALSE) %>% add_projections(ebola_proj6, quantiles = FALSE) %>% add_projections(ebola_proj7, quantiles = FALSE)

# SARS incidence
sars_plot <- plot(sars_i[1:(8*9), ]) 
# %>% add_projections(sars_proj1, quantiles = FALSE) %>% add_projections(sars_proj1, quantiles = FALSE) %>% add_projections(sars_proj2, quantiles = FALSE) %>% add_projections(sars_proj3, quantiles = FALSE) %>% add_projections(sars_proj4, quantiles = FALSE) %>% add_projections(sars_proj5, quantiles = FALSE) %>% add_projections(sars_proj6, quantiles = FALSE) %>% add_projections(sars_proj7, quantiles = FALSE)

# Influenza incidence
influenza_plot <- plot(flu_i[1:(8*3), ]) 
# %>% add_projections(flu_proj1, quantiles = FALSE) %>% add_projections(flu_proj2, quantiles = FALSE) %>% add_projections(flu_proj3, quantiles = FALSE) %>% add_projections(flu_proj4, quantiles = FALSE) %>% add_projections(flu_proj5, quantiles = FALSE) %>% add_projections(flu_proj6, quantiles = FALSE) %>% add_projections(flu_proj7, quantiles = FALSE)

# Combine plots into one plot
multiplot(ebola_plot, influenza_plot, sars_plot, cols = 3)
@
\caption{The early outbreak daily incidence curves for Ebola (left), H1N1 influenza (middle), and Severe Acute Respiratory Syndrome (SARS) (right) \citep{Leone2014, Cori2018, Campbell2018}.}
\label{empirical_plot}
\end{figure}

\FloatBarrier
\newpage
\subsection{Full prediction metrics}

\subsubsection{Average residual}
\begin{figure}[h]
<<whole_real_residual, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5>>=
facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

residual_massive <- ggplot(real_total_table, aes(cali_window_size, y = residual, color = dataset)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Residual", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
residual_massive
@
\caption{The average residual for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_residual}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Mean-square relative error}
\begin{figure}[h]
<<whole_real_mse, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5>>=
facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

mse_massive <- ggplot(real_total_table, aes(cali_window_size, y = mse, color = dataset)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "MSE", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
mse_massive
@
\caption{The mean-square error for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_mse}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Sharpness}
\begin{figure}[h]
<<whole_real_sharpness, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5>>=
facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

sharpness_massive <- ggplot(real_total_table, aes(cali_window_size, y = sharpness, color = dataset)) +
                    # geom_violin(trim = TRUE, scale = "width", position = position_dodge(width = 1), fill = violin_fill, color = violin_fill) +
                    # geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Sharpness", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(0, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
sharpness_massive
@
\caption{Sharpness for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_sharpness}
\end{figure}
\FloatBarrier

\newpage
\subsubsection{Bias}
\begin{figure}[h]
<<whole_real_bias, echo = FALSE, cache = TRUE, fig.width = 8, fig.height = 6.5>>=
facet1_names <- c(
  "ebola" = "Ebola",
  "influenza" = "Influenza",
  "sars" = "SARS"
)

facet2_names <- c(
  "1" = "Projection 1",
  "2" = "Projection 2",
  "3" = "Projection 3",
  "4" = "Projection 4"
)

bias_massive <- ggplot(real_total_table, aes(cali_window_size, y = bias, color = dataset)) +
                    geom_hline(yintercept = 0, linetype = "dashed") +
                    geom_point(size = 1.5, shape = 16) +
                    facet_wrap(dataset~proj_window_no, scales = "free", labeller = labeller(
                      disease = facet1_names,
                      proj_window_no = facet2_names
                    )) +
                    labs(y = "Bias", x = "Calibration window size") +
                    coord_cartesian(xlim = c(1, 7), ylim = c(-1, 1)) +
                    scale_x_discrete(labels = c("1", "2", "3", "4", "5", "6", "7"), limits =  c("1", "2", "3", "4", "5", "6", "7")) +
                    theme(legend.position = "none") # +
# Print the figure
bias_massive
@
\caption{Bias for all seven calibration window size faceted into panels by disease and projection window number, as specified in the header of each panel for empirical outbreaks of Ebola (red), influenza (green), and SARS (blue) diseases.}
\label{real_whole_bias}
\end{figure}

\end{document}