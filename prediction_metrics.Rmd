---
title: "Prediction Metrics"
author: "Janetta E. Skarp"
date: "13 April 2018"
output: pdf_document
fonsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# The data

Assume that I have daily case data for an outbreak. The outbreak of choice is an H1N1 influenza outbreak in Pennsylvania in 2009 (EpiEstim). There is data for 32 days, but I will pretend that I only have data for the first 2 weeks. I assume that the data is complete and devoid of reporting delays. I am aware that this is an unrealistic expectation for real data but this assumption works for my example.

```{r data}
library(EpiEstim) # need the EpiEstim package for data
data("Flu2009") # Load the flu data
Flu2009 # Look at what the full dataset contains
```

Let's have a look at the incidence curve for the outbreak.

```{r incidence_curve}
library(ggplot2)

# Truncate the dataset and make incidence into its own dataframe
length_obs <- 14 # length of observation period in days
end <- length(Flu2009$Incidence)

# Convert to a dataframe for ggplot
Flu2009_incidence <- data.frame(Flu2009$Incidence[1:length_obs]) 
names(Flu2009_incidence) <- c("Incidence") # Rename the variable

# Add a "day" column to the dataset to help plotting
day_sequence <- seq(1, length_obs, 1)
Flu2009_incidence$Day <- day_sequence

incidence_curve <- ggplot(data = Flu2009_incidence, aes(x = Flu2009_incidence$Day, 
                                                        y = Flu2009_incidence$Incidence)) +
                   geom_bar(stat="identity")
incidence_curve

```

\newpage
# Estimating R

Next, I'll generate a forecast so that I can assess its performance later. 

The EpiEstim package provides an estimate of the serial interval Gamma distribution already so I don't need to calculate it:

```{r serial_interval, echo = FALSE}
# Make the serial interval distribution into a dataframe
Flu2009_si <- data.frame(Flu2009$SI.Distr)
SI_end <- length(Flu2009$SI.Distr)

# Plot the serial interval
serial_interval <- ggplot(data = Flu2009_si, aes(x = seq(1:SI_end), 
                                                 y = Flu2009_si$Flu2009.SI.Distr)) +
                   geom_bar(stat="identity")
serial_interval

```

The mean of the distribution is 2.6 days.

The standard deviation of the distribution is 1.5 days.

The shift of the distribution is 1 day.

I can now estimate R:
```{r R_estimate}
library(earlyR)

# Specifying the input for the estimate
mu <- 2.6 # serial interval mean
sigma <- 1.5 # serial interval SD
i <- c(Flu2009$Incidence[1:length_obs]) # incidence vector
typeof(i)

# The R estimation
R <- get_R(as.integer(i), disease = NULL, si = NULL, si_mean = mu, si_sd = sigma, 
           max_R = 10)
print(R)

# Visualising R
plot(R)
plot(R, "lambdas")
abline(v = max(Flu2009_incidence$Day), lty = 2)

```

The dotted lines in the global force of infection plot show the current date (i.e. last date observed). There is still a possibility of a new transmission event occurring. 

\newpage
# Incidence forecasting

Now that I know what's happening with the effective reproduction number, I can forecast into the future to predict what will happen next.

The *projections* package will not work unless you feed incidence to it as an incidence object (using the package *incidence*), so I'll need to re-make my daily incidence:
```{r redo_incidence}
library(incidence)

Flu2009_incidence # what I'll be converting to an incidence object

# Need to open up the incidence so that there's only 1 individual per row with
# their date of becoming a case (assume date of onset is the case date)
Flu2009_infday <- array(NA, dim = c(sum(Flu2009_incidence$Incidence)))

for (i in 1:nrow(Flu2009_incidence)) {
  if (Flu2009_incidence$Incidence[i] > 0) {
    # create an array for the number of individuals who have the same date of infection
    number_cases <- array(Flu2009_incidence$Day[i], 
                          dim = c(Flu2009_incidence$Incidence[i])) 
    # calculate where in the new array the cases for day i should be added to
    addition_point <- sum(Flu2009_incidence$Incidence[0:(i-1)]) + 1 
    # add the cases for day i to the big array
    Flu2009_infday[(addition_point):(addition_point+length(number_cases)-1)] <- Flu2009_incidence$Day[i] 
  }
}

Flu2009_infday # The current shape of incidence

# Now convert this "line list" into an incidence object
Flu2009_newinc <- incidence(dates = as.numeric(Flu2009_infday), interval = 1)
plot(Flu2009_newinc)
```

```{r forecast}
library(projections)

# number of days that I hid data for
hidden_days <- end - length_obs

# Number of trajectories for the projection
n_traj <- 10000 

# Serial interval distribution already provided with the EpiEstim data
proj <- project(Flu2009_newinc, R = sample_R(R, 1000), si = Flu2009$SI.Distr, 
                n_sim = n_traj, n_days = hidden_days, R_fix_within = TRUE)

# Make a data frame of the hidden data for comparison when plotting
Flu2009_incidence_hidden <- data.frame(Flu2009$Incidence[length_obs + 1:end])
Flu2009_incidence_hidden <- data.frame(Flu2009_incidence_hidden[1:hidden_days, ])
names(Flu2009_incidence_hidden) <- c("Incidence") # Rename the variable
Flu2009_incidence_hidden$Day <- seq((length_obs + 1), end, 1)

# Look at prediction quantiles
apply(proj, 1, summary)

# Look at how the mean develops
apply(proj, 1, function(x) mean(x > 0))

# Plot the projection
plot(proj, c(.1, .9)) +
  # Add a line showing the actual incidence for days 15-32
  geom_line(aes(x = Day, y = Incidence), Flu2009_incidence_hidden)

```

According to the predictions, it looks like the outbreak incidence will be increasing over the next 18 days. The uncertainty around the prediction estimates increases as the forecast moves further into the future. Compared to the true incidence (black line), the forecast stays near the prediction for around 3 days, after which the prediction starts to grossly overestimate incidence.

\newpage
# Assessing prediction performance

## Calibration (Funk et al.)
Calibration relates to the model's ability to correctly identify prediction uncertainty. Calibration needs to be assessed before sharpness.

If the model is perfectly calibrated, each time point's data looks as if it comes from the predictive probability distribution (the distribution of possible unobserved values conditional on the observed values) for that time point:

$$ u_{t} = F_{t}(x_{t}) $$
Here $x_{t}$ is the datapoint for time $t$. $F_{t}$ is the cumulative probability distribution at time $t$. $u_{t}$ is the probability.

In my case the predictions should follow a Poisson-distribution with some lambda, $\lambda$. I then use my hidden data as the true data points, $x_{t}$.

The Anderson-Darling test (a statistical test for seeing if the data is drawn from a given probability distribution) of uniformity to $u_{t}$ is applied. The model is calibrated if p > 0.1.

```{r calibration}
# library("goftest")

# The true datapoints: x_t
x_t <- Flu2009_incidence_hidden$Incidence

# The cumulative probability distribution for time t: F_t
cumulative_poisson <- function(data, pred){
  ppois(data, mean(pred))
}

# Calculating calibration for each prediction day
calibration <- function(data, pred){
  calibration <- array(NA, dim = c(hidden_days))
  for (i in 1:hidden_days){
    calibration[i] <- cumulative_poisson(data[i], pred[i, ])
  }
  return(calibration)
}

# anderson-darling <- function(data, pred){
#   anderson-darling <- array(NA, dim = c(hidden_days))
#   for (i in 1:hidden_days){
#   ad.test()
#   }
#   return(anderson-darling)
# }

model_calibration <- calibration(x_t, proj)

```

```{r plot_calibration, echo = FALSE}
calibration_days <- seq(1, hidden_days, 1)
calibration_data <- data.frame(model_calibration)
calibration_data$days <- calibration_days

plot_calibration <- ggplot(data = calibration_data, aes(x = days, 
                                                        y = model_calibration)) +
                           geom_point(size = 2, shape = 16) 
plot_calibration

```

The model is kind of calibrated for the first 3 days (also intuitively seen in the previous plot), but after that calibration drops.

\newpage
## Sharpness (Funk et al.)
Model's ability to make predictions within a narrow range of possible outcomes. 

Sharpness is calculated as follows:

$$ S_{t}(F_{t}) = 1 - \frac{MADM(y)}{m(y)} $$
where y is the prediction variable. MADM stands for the normalised median absolute deviation around the median (m(y)) of y:

$$ MADM(y) = m(|y - m(y)|) $$
The closer to 1 S is, the sharper the forecast.

```{r sharpness, cache = TRUE}
# Function for calculating forecasted incidence median
forecast_median <- function(x){
  return(median(x))
}

# Function for calculating the standard deviations
# Want it to return MADM(y)
MADM <- function(x){
  traj_diff <- array(NA, dim = c(n_traj)) # array for storing SDs
  # For each trajectory for this day, calculate the SD
  for (i in 1:n_traj){
    absolute_diff <-  abs(x[i] - forecast_median(x))
    # save the absolute difference for each trajectory
    traj_diff[i] <- absolute_diff
  }
  MADM <- median(traj_diff)
  return(MADM)
}

# Function for calculating the normalised median absolute deviation
# Want it to return St(Ft)
sharpness <- function(x){
  # array for storing daily sharpness
  sharpness <- array(NA, dim = c(hidden_days))
  for (i in 1:hidden_days){
    sharpness_day <- 1 - (MADM(x[i, ]) / forecast_median(x[i, ])) 
    sharpness[i] <- sharpness_day
  }
  return(sharpness)
}

model_sharpness <- sharpness(proj)

```

```{r plot_sharpness, echo = FALSE}
sharpness_days <- seq(1, hidden_days, 1)
sharpness_data <- data.frame(model_sharpness)
sharpness_data$days <- sharpness_days

plot_sharpness <- ggplot(data = sharpness_data, aes(x = days, 
                                                    y = model_sharpness)) +
                           geom_point(size = 2, shape = 16) 
plot_sharpness

```

Model sharpness decreases as you forecast further. Remember, S = 1 is perfect sharpness.

\newpage
## Bias (Funk et al.)
Does model systematically over- or underpredict data?

Bias can be calculated as:

$$ B_{t}(F_{t}, x_{t}) = 2(E_{F_{t}}[H(X - x_{t})] - 0.5) $$

Where $E_{F_{t}}$ refers to the expectation with respect to the predictive cumulative probability distribution $F_{t}$, and $X$ are independent realisations of a variable with distribution $F_{t}$

If unbiased, half of forecasts would be above estimate and half would be below the estimate, thus $B_{t}$ would be 0. A completely biased model would either be completely above the prediction line ($B_{t} = 1$) or completely under the prediction line ($B_{t} = -1$).

The Heaviside function takes a value of 0 for a negative number and 1 for a positive number. A Heaviside function with the half-maximum convention is 0.5 for H(0)

```{r bias}
# Calculate the difference between datapoints
# Feed in the true datapoint and predictions for a given time t
difference <- function(data, pred){
    # Calculate difference between data point and sample
  difference <- array(NA, dim = c(n_traj))
  for (i in 1:n_traj){
    difference[i] <- pred[i] - data
  }
  return(difference)
}

# Do the Heaviside function with half-maximum convention
# Feed in the differences between the predictions and true values for a given time t
heaviside <- function(difference){
  heaviside <- array(NA, dim = c(n_traj))
  for (i in 1:n_traj){
    if (difference[i] > 0){
      heaviside[i] <- 1
    } else if (difference[i] < 0){
      heaviside[i] <- 0
    } else{
      heaviside[i] <- 0.5
    }
  }
  return(heaviside)
}

# Calculate the expected prediction value for a given time t
expected <- function(h_values, pred){
  mean_heaviside <- mean(h_values)
  return(mean_heaviside)
}

# Calculate bias
bias <- function(data, pred){
  bias <- array(NA, dim = c(hidden_days))
  for (i in 1:hidden_days){
  bias[i] <- 2 * (expected(heaviside(difference(data[i], pred[i, ])), pred) - 0.5)
  }
  return(bias)
}

# Get single bias score
bias_score <- function(data, pred){
  daily_bias <- bias(data, pred)
  bias_score <- sum(daily_bias) / hidden_days
  return(bias_score)
}

model_bias <- bias(x_t, proj)

print(paste("The bias score is", bias_score(x_t, proj)))

```
```{r plot_bias, echo = FALSE}
bias_days <- seq(1, hidden_days, 1)
bias_data <- data.frame(model_bias)
bias_data$days <- bias_days

plot_bias <- ggplot(data = bias_data, aes(x = days, 
                                          y = model_bias)) +
                           geom_point(size = 2, shape = 16) 
plot_bias

```

The model tends to over-predict the number of cases (positive bias?).

\newpage
## Continuous ranked probability score (Funk et al.)
An evaluation method that combines calibration and sharpness. Assesses whether the predictive distribution and the data-generating distribution are the same. In a perfect scenario, CRPS is 0.

Not sure if this equation for calculating CRPS only applies to deterministic forecasts:

$$ CRPS(F_{t}, x_{t}) = E_{F_{t}} | X - x_{t} | - \frac{1}{2} E_{F_{t}} | X - X' | $$

where $X$ is a sample of the predicted number of cases for a given timepoint from predictive distribution $F_{t}$, $x_{t}$ is the observed number of cases for that timepoint, and $X'$ is also a sample of the predicted number of cases, but independent of $X$. 

There's a package called *scoringRules* for evaluating forecasts?

```{r CRPS, cache = TRUE}
absolute_diff <- function(single_x, vector_x){
  absolute_diff <- array(NA, dim = c(length(vector_x)))
  for (i in 1:length(vector_x)){
  absolute_diff[i] <- abs(single_x - vector_x[i])
  }
  return(absolute_diff)
}

expected_diff <- function(single_x, vector_x){
  difference <- absolute_diff(single_x, vector_x)
  expectation <- mean(difference)
  return(expectation)
}

expected_diff_pred <- function(vector_x){
  # absolute_diff <- abs(apply(combn(vector_x, 2), 2, diff)) # much slower
  absolute_diff <- as.numeric(dist(vector_x))
  expectation <- mean(absolute_diff)
  return(expectation)
}

crps <- function(data, pred){
  crps <- array(NA, dim = c(length(data)))
  for (i in 1:length(data)){
  crps[i] <- expected_diff(data[i], pred[i, ]) - 0.5 * expected_diff_pred(pred[i, ])
  }
  return(crps)
}

model_crps <- crps(x_t, proj)

```

```{r plot_crps, echo = FALSE}
crps_days <- seq(1, hidden_days, 1)
crps_data <- data.frame(model_crps)
crps_data$days <- crps_days

plot_crps <- ggplot(data = crps_data, aes(x = days, 
                                          y = model_crps)) +
                           geom_point(size = 2, shape = 16) 
plot_crps

```

CRPS increases as forecasts are further away from the current date. There's a clear jump after the third forecast day, this coincides with the true data not being within the scope of the confidence intervals of the prediction seen in the prediction plot.

\newpage
## Root mean square error (Viboud et al. 2017)
The sample standard deviation of the differences between predicted and observed values.

```{r RMSE}
# Calculate squared difference between true data and prediction for a given timepoint
squared_diff <- function(data, pred){
  squared_diff <- array(NA, dim = c(length(pred)))
  for (i in 1:length(pred)){
  squared_diff[i] <- (data - pred[i])^2
  }
  return(squared_diff)
}

# Take the mean of the squared differences
mean_squared_diff <- function(data, pred){
  mean_squared_diff <- sum(squared_diff(data, pred)) / length(pred)
  return(mean_squared_diff)
}

# Calculates RMSE for each forecasted timepoint
rmse <- function(data, pred){
  rmse <- array(NA, dim = c(length(data)))
  for (i in 1:length(data)){
    rmse[i] <- sqrt(mean_squared_diff(data[i], pred[i, ]))
  }
  return(rmse)
}

model_rmse <- rmse(x_t, proj)

```

```{r plot_rmse, echo = FALSE}
rmse_days <- seq(1, hidden_days, 1)
rmse_data <- data.frame(model_rmse)
rmse_data$days <- rmse_days

plot_rmse <- ggplot(data = rmse_data, aes(x = days, 
                                          y = model_rmse)) +
                           geom_point(size = 2, shape = 16) 
plot_rmse

```

\newpage
## Mean absolute error (Viboud et al. 2017)
```{r MAE}
# Calculate the absolute difference
absolute_diff <- function(data, pred){
  absolute_diff <- array(NA, dim = c(length(pred)))
  for (i in 1:length(pred)){
  absolute_diff[i] <- abs(data - pred[i])
  }
  return(absolute_diff)
}

# Take the mean of the absolute differences
mean_absolute_diff <- function(data, pred){
  mean_absolute_diff <- sum(absolute_diff(data, pred)) / length(pred)
  return(mean_absolute_diff)
}

# Calculate MAE for each forecasted timepoint
mae <- function(data, pred){
  mae <- array(NA, dim = c(length(data)))
  for (i in 1:length(data)){
    mae[i] <- mean_absolute_diff(data[i], pred[i, ])
  }
  return(mae)
}

model_mae <- mae(x_t, proj)

```

```{r plot_mae, echo = FALSE}
mae_days <- seq(1, hidden_days, 1)
mae_data <- data.frame(model_mae)
mae_data$days <- mae_days

plot_mae <- ggplot(data = mae_data, aes(x = days, 
                                        y = model_mae)) +
                           geom_point(size = 2, shape = 16) 
plot_mae

```

\newpage
## Relative mean square error (Viboud et al. 2017)
Not quite sure what this is and how it differs from the other errors

## R squared (Viboud et al. 2017)
Based on y = ax
Plot predicted vs. observed and see how much variation is explained by linear model.
But should I use an adjusted R squared instead of just the normal one?

## Pearson's correlation between predicted and observed incidence (Viboud et al. 2017)

## Bias (Viboud et al. 2017)
Fitting a linear regression to predicted and observed incidence, y = ax + b
Then again, I already have a method for measuring bias. 